{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**autograd** package defines a *computational graph* during the *forward pass* of the network. **`nodes`** in the graph will be *`Tensors`*, and **`edges`** will be *`functions`* that produce output *`Tensors`* from input *`Tensors`*\n",
    "- If `x` is a `Variable` then `x.data` is a `Tensor`, and `x.grad` is another `Variable` holding the `gradient of x with respect to some scalar value`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run on GPU \n",
    "dtype = torch.cuda.FloatTensor # Normal: dtype = torch.FloatTensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# N: batch size, D_in: input dimension, H: hidden dimension, D_out: output dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create random Tensors to hold input and output and wrap them in Variables\n",
    "# Setting requires_grad=False -> Do not compute gradients wrt these Variables during backward pass\n",
    "x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=False)\n",
    "y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create random Tensors for weights and wrap them in Variables\n",
    "# Setting requires_grad=True -> Compute gradients wrt these Variables during backward pass\n",
    "w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=True)\n",
    "w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 30002020.0\n",
      "50 675475.3125\n",
      "100 614328.4375\n",
      "150 3440276.0\n",
      "200 68483.2890625\n",
      "250 nan\n",
      "300 nan\n",
      "350 nan\n",
      "400 nan\n",
      "450 nan\n"
     ]
    }
   ],
   "source": [
    "# Simple Neural Network\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for i in xrange(500):\n",
    "    # Forward pass: Compute predicted y using operations on Variables\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    \n",
    "    # Compute Total SE loss using operations on Variables\n",
    "    # loss is a Variable of shape (1,); loss.data is a Tensor of shape (1,);\n",
    "    # loss.data[0] is a scalar value\n",
    "    loss = (y_pred - y).pow(2).sum() \n",
    "    if i % 50 == 0:\n",
    "        print i, loss.data[0]\n",
    "        \n",
    "    # Manually mutate the gradients before running backward pass\n",
    "    #w1.grad.data.zero_()\n",
    "    #w2.grad.data.zero_()\n",
    "    \n",
    "    # Use autograd to compute backward pass\n",
    "    # gradient of loss wrt all Variables with requires_grad=True\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the weights using gradient descent\n",
    "    # NOTE: w<k>.data is Tensor, w<k>.grad is Variable, w<k>.grad.data is Tensor\n",
    "    w1.data -= learning_rate * w1.grad.data\n",
    "    w2.data -= learning_rate * w2.grad.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining Custom Autograd Functions**\n",
    "- Each primitive autograd operator is really two functions that operate on Tensors\n",
    "    - **`Forward`** function: Computes output Tensors from input Tensors\n",
    "    - **`Backward`** function: Receives the gradient of the output Tensor with respect to some `scalar value`, and computes the gradient of the input Tensors with respect to that same `scalar value`\n",
    "- Define custom autograd operator by defining a subclass of `torch.autograd.Function` and implementing the `forward` and `backward` functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CustomReLU(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Implementing custom autograd Functions by subclassing \n",
    "    torch.autograd.Function, and implementing forward and \n",
    "    backward passes which operate on Tensors\n",
    "    \n",
    "    ReLU = min(0.5, input)\n",
    "    \"\"\"\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Forward pass receives a Tensor containing the input \n",
    "        and returns a Tensor containing output.\n",
    "        \n",
    "        NOTE: Cache arbitrary Tensors for use in the backward \n",
    "        pass using `save_for_backward` method.\n",
    "        \"\"\"\n",
    "        self.save_for_backward(input)\n",
    "        return input.clamp(min=0.5)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass receives a Tensor containing the gradient \n",
    "        of the loss with respect to the output, and computes and \n",
    "        returns the gradient of the loss with respect to the input.\n",
    "        \"\"\"\n",
    "        input = self.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0.5 # Gradient computation \n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# N: batch size, D_in: input dimension, H: hidden dimension, D_out: output dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and output and wrap them in Variables\n",
    "# Setting requires_grad=False -> Do not compute gradients wrt these Variables during backward pass\n",
    "x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=False)\n",
    "y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create random Tensors for weights and wrap them in Variables\n",
    "# Setting requires_grad=True -> Compute gradients wrt these Variables during backward pass\n",
    "w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=True)\n",
    "w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 37390400.0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "indexing a tensor with an object of type bool. The only supported types are integers, slices, numpy scalars and torch.cuda.LongTensor or torch.cuda.ByteTensor as the only argument.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-0bc444a97a10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Use autograd to compute backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# gradient of loss wrt all Variables with requires_grad=True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# Update the weights using gradient descent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ankoor/.virtualenvs/PyTorch/local/lib/python2.7/site-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_variables)\u001b[0m\n\u001b[1;32m    144\u001b[0m                     'or with gradient w.r.t. the variable')\n\u001b[1;32m    145\u001b[0m             \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_as_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execution_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-5dd99f98a6ce>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad_output)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mgrad_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mgrad_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;31m# Gradient computation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: indexing a tensor with an object of type bool. The only supported types are integers, slices, numpy scalars and torch.cuda.LongTensor or torch.cuda.ByteTensor as the only argument."
     ]
    }
   ],
   "source": [
    "# Simple Neural Network with custom autograd function\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for i in xrange(500):\n",
    "    # Create an instance of CustomReLU class to use in the network\n",
    "    relu = CustomReLU()\n",
    "    \n",
    "    # Forward pass: Compute predicted y using operations on Variables\n",
    "    y_pred = relu(x.mm(w1)).mm(w2)\n",
    "    \n",
    "    # Compute Total SE loss using operations on Variables\n",
    "    # loss is a Variable of shape (1,); loss.data is a Tensor of shape (1,);\n",
    "    # loss.data[0] is a scalar value\n",
    "    loss = (y_pred - y).pow(2).sum() \n",
    "    if i % 50 == 0:\n",
    "        print i, loss.data[0]\n",
    "        \n",
    "    # Manually mutate the gradients before running backward pass\n",
    "    #w1.grad.data.zero_()\n",
    "    #w2.grad.data.zero_()\n",
    "    \n",
    "    # Use autograd to compute backward pass\n",
    "    # gradient of loss wrt all Variables with requires_grad=True\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the weights using gradient descent\n",
    "    # NOTE: w<k>.data is Tensor, w<k>.grad is Variable, w<k>.grad.data is Tensor\n",
    "    w1.data -= learning_rate * w1.grad.data\n",
    "    w2.data -= learning_rate * w2.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
