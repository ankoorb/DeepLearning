{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long-Short Term Memory (LSTM) for Part-of-Speech Tagging\n",
    "- [Bi-LSTM in TF example](https://github.com/monikkinom/ner-lstm)\n",
    "\n",
    "```bash\n",
    "# Install PyTorch : Linux, Pip, Python 2.7, CUDA 8.0\n",
    "!pip install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp27-cp27mu-linux_x86_64.whl \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let input sentence be $w_1, ..., w_M$, where $w_i \\in V$, $V$ is vocabulary\n",
    "- Let $T$ be tag set and $y_i$ the tag of word $w_i$\n",
    "- Prediction of the tag of word $w_i$ is denoted by $\\hat{y_i}$ and output is a sequence $\\hat{y_1}, ..., \\hat{y_M}$, where $\\hat{y_i} \\in T$\n",
    "- To predict, pass an LSTM over the sentence. Hidden state at time step $i$ is denoted by $h_i$. Also assign each tag a unique index. Then prediction rule for $\\hat{y_i}$ is \n",
    "\n",
    "$$\\hat{y_i}=argmax_j (log \\text{ }Softmax(Ah_i+b))_j$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_to_idx:  {'Everybody': 5, 'ate': 2, 'apple': 4, 'that': 7, 'read': 6, 'dog': 1, 'book': 8, 'the': 3, 'The': 0}\n"
     ]
    }
   ],
   "source": [
    "# Prepare data\n",
    "\n",
    "training_data = [\n",
    "    ('The dog ate the apple'.split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    ('Everybody read that book'.split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "]\n",
    "\n",
    "testing_data = [\n",
    "    ('The dog ate the book'.split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "]\n",
    "\n",
    "word_to_idx = {}\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_idx:\n",
    "            word_to_idx[word] = len(word_to_idx)\n",
    "            \n",
    "print('word_to_idx: ', word_to_idx)\n",
    "\n",
    "tag_to_idx = {'DET': 0, 'NN': 1, 'V': 2}\n",
    "\n",
    "def prepare_sequence(seq, to_idx):\n",
    "    indices = [to_idx[w] for w in seq]\n",
    "    tensor = torch.LongTensor(indices)\n",
    "    return autograd.Variable(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyper-parameters\n",
    "\n",
    "# Development so DIM is small\n",
    "EMBEDDING_DIM = 6 \n",
    "HIDDEN_DIM = 6 \n",
    "num_epochs = 25\n",
    "learning_rate = 0.1\n",
    "best_accuracy = torch.FloatTensor([0])\n",
    "start_epoch = 0\n",
    "\n",
    "# Path to saved model weights (as hdf5)\n",
    "resume_weights = './lstm-ner/checkpoint.pth.tar'\n",
    "\n",
    "# CUDA\n",
    "cuda = False # torch.cuda.is_available()\n",
    "\n",
    "# Seed for reproducibility \n",
    "torch.manual_seed(1)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "class LSTM_Tagger(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTM_Tagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # LSTM takes word embeddings as inputs and outputs hidden states with dimensionality hidden dim\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        \n",
    "        # Linear layer that maps from hidden state space to tag space\n",
    "        self.hidden_to_tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        # Initially there is no hidden state\n",
    "        # torch.zeros(num_layers, minibatch_size, hidden_dim)\n",
    "        return (autograd.Variable(torch.zeros(1, 1, self.hidden_dim)),\n",
    "                autograd.Variable(torch.zeros(1, 1, self.hidden_dim)))\n",
    "    \n",
    "    def forward(self, sentence):\n",
    "        embeddings = self.word_embeddings(sentence)\n",
    "        lstm_out, self.hidden = self.lstm(\n",
    "            embeddings.view(len(sentence), 1, -1), self.hidden)\n",
    "        tag_space = self.hidden_to_tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def train(model, optimizer, loss_fn, train_data):\n",
    "    \"\"\"\n",
    "    train_data -- list of tuples, e.g. [(['dog', 'ate'], ['NN', 'V']), (...), ...]\n",
    "    \"\"\"\n",
    "    for sentence, tags in train_data:\n",
    "        \n",
    "        # Prepare inputs\n",
    "        sentence_in = prepare_sequence(sentence, word_to_idx)\n",
    "        targets = prepare_sequence(tags, tag_to_idx)\n",
    "        \n",
    "        if cuda:\n",
    "            sentence_in, targets = sentence_in.cuda(), targets.cuda()\n",
    "                    \n",
    "        # Clear gradients as Pytorch accumulates gradients\n",
    "        model.zero_grad()\n",
    "        \n",
    "         # Clear out the hidden state of LSTM\n",
    "        model.hidden = model.init_hidden()\n",
    "        \n",
    "        # Forward pass\n",
    "        tag_scores = model(sentence_in) # Element i, j is the score for tag j for word i\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        \n",
    "        # Backward + Optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "def evaluate(model, data):\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    for sentence, tags in data:\n",
    "        \n",
    "        # Prepare inputs\n",
    "        sentence_in = prepare_sequence(sentence, word_to_idx)\n",
    "        targets = prepare_sequence(tags, tag_to_idx)\n",
    "        \n",
    "        if cuda:\n",
    "            sentence_in, targets = sentence_in.cuda(), targets.cuda()\n",
    "            \n",
    "        # Forward pass\n",
    "        tag_scores = model(sentence_in)\n",
    "        \n",
    "        # Load output to CPU\n",
    "        if cuda:\n",
    "            tag_scores.cpu()\n",
    "            \n",
    "        # Prediction\n",
    "        _, pred = torch.max(tag_scores, 1) # argmax for axis 1 \n",
    "        \n",
    "        # Compute accuracy\n",
    "        correct += torch.equal(pred, targets)\n",
    "        \n",
    "    return correct/float(len(data))\n",
    "\n",
    "# Keep only a single checkpoint, the best over test accuracy.\n",
    "def save_checkpoint(state, is_best, filename=None):\n",
    "    \"\"\"Save checkpoint if a new best is achieved\"\"\"\n",
    "    if is_best:\n",
    "        print (\"=> Saving a new best\")\n",
    "        torch.save(state, filename)  # save checkpoint\n",
    "    else:\n",
    "        print (\"=> Validation Accuracy did not improve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model and loss function\n",
    "model = LSTM_Tagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_idx), len(tag_to_idx))\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "# If GPU available then load the model on GPU\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "    loss_function.cuda()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint: \"./lstm-ner/checkpoint.pth.tar\" ...\n",
      "Loaded checkpoint \"./lstm-ner/checkpoint.pth.tar\" (trained for 129 epochs)\n"
     ]
    }
   ],
   "source": [
    "# If best model weights exist then load it\n",
    "if os.path.isfile(resume_weights):\n",
    "    print('Loading checkpoint: \"{}\" ...'.format(resume_weights))\n",
    "    \n",
    "    # Load weights\n",
    "    checkpoint = torch.load(resume_weights)\n",
    "    \n",
    "    start_epoch = checkpoint['epoch']\n",
    "    best_accuracy = checkpoint['best_accuracy']\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    print('Loaded checkpoint \"{}\" (trained for {} epochs)'.format(resume_weights, start_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-0 Test Set: Accuracy: 1.00\n",
      "=> Validation Accuracy did not improve\n",
      "Epoch-1 Test Set: Accuracy: 1.00\n",
      "=> Validation Accuracy did not improve\n",
      "Epoch-2 Test Set: Accuracy: 1.00\n",
      "=> Validation Accuracy did not improve\n",
      "Epoch-3 Test Set: Accuracy: 1.00\n",
      "=> Validation Accuracy did not improve\n",
      "Epoch-4 Test Set: Accuracy: 1.00\n",
      "=> Validation Accuracy did not improve\n",
      "Epoch-5 Test Set: Accuracy: 1.00\n",
      "=> Validation Accuracy did not improve\n",
      "Epoch-6 Test Set: Accuracy: 1.00\n",
      "=> Validation Accuracy did not improve\n",
      "Epoch-7 Test Set: Accuracy: 1.00\n",
      "=> Validation Accuracy did not improve\n",
      "Epoch-8 Test Set: Accuracy: 1.00\n",
      "=> Validation Accuracy did not improve\n",
      "Epoch-9 Test Set: Accuracy: 1.00\n",
      "=> Validation Accuracy did not improve\n",
      "Epoch-10 Test Set: Accuracy: 1.00\n",
      "=> Validation Accuracy did not improve\n",
      "Epoch-11 Test Set: Accuracy: 1.00\n",
      "=> Validation Accuracy did not improve\n",
      "Epoch-12 Test Set: Accuracy: 1.00\n",
      "=> Validation Accuracy did not improve\n",
      "Epoch-13 Test Set: Accuracy: 1.00\n",
      "=> Validation Accuracy did not improve\n",
      "Epoch-14 Test Set: Accuracy: 1.00\n",
      "=> Validation Accuracy did not improve\n",
      "Epoch-15 Test Set: Accuracy: 1.00\n",
      "=> Validation Accuracy did not improve\n",
      "Epoch-16 Test Set: Accuracy: 1.00\n",
      "=> Validation Accuracy did not improve\n",
      "Epoch-17 Test Set: Accuracy: 1.00\n",
      "=> Validation Accuracy did not improve\n",
      "Epoch-18 Test Set: Accuracy: 1.00\n",
      "=> Validation Accuracy did not improve\n",
      "Epoch-19 Test Set: Accuracy: 1.00\n",
      "=> Validation Accuracy did not improve\n",
      "Epoch-20 Test Set: Accuracy: 1.00\n",
      "=> Validation Accuracy did not improve\n",
      "Epoch-21 Test Set: Accuracy: 1.00\n",
      "=> Validation Accuracy did not improve\n",
      "Epoch-22 Test Set: Accuracy: 1.00\n",
      "=> Validation Accuracy did not improve\n",
      "Epoch-23 Test Set: Accuracy: 1.00\n",
      "=> Validation Accuracy did not improve\n",
      "Epoch-24 Test Set: Accuracy: 1.00\n",
      "=> Validation Accuracy did not improve\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    train(model, optimizer, loss_function, training_data)\n",
    "    acc = evaluate(model, testing_data)\n",
    "    print('Epoch-{} Test Set: Accuracy: {:.2f}'.format(epoch, acc))\n",
    "    \n",
    "    acc = torch.FloatTensor([acc])\n",
    "    \n",
    "    # Get bool not ByteTensor\n",
    "    is_best = bool(acc.numpy() > best_accuracy.numpy())\n",
    "    \n",
    "    # Get greater tensor to keep track of best_accuracy\n",
    "    best_accuracy = torch.FloatTensor(max(acc.numpy(), best_accuracy.numpy()))\n",
    "    \n",
    "    # Save checkpoint\n",
    "    save_checkpoint({'epoch': start_epoch + epoch + 1,\n",
    "                     'state_dict': model.state_dict(),\n",
    "                     'best_accuracy': best_accuracy,}, is_best, filename=resume_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test:  True\n",
      "Train:  True\n"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "yt = prepare_sequence(testing_data[0][1], tag_to_idx)\n",
    "ts = model(prepare_sequence(testing_data[0][0], word_to_idx))\n",
    "_, yh = torch.max(ts, 1)\n",
    "print('Test: ', torch.equal(yh, yt))\n",
    "\n",
    "yt = prepare_sequence(training_data[0][1], tag_to_idx)\n",
    "ts = model(prepare_sequence(training_data[0][0], word_to_idx))\n",
    "_, yh = torch.max(ts, 1)\n",
    "print('Train: ', torch.equal(yh, yt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
