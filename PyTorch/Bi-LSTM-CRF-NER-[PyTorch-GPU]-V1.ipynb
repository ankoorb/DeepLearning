{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-LSTM Conditional Random Field for NER (`State of the art`)\n",
    "\n",
    "- [PyTorch - Bi-LSTM + CRF](http://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html)\n",
    "- [Michael Collins CRF](http://www.cs.columbia.edu/~mcollins/crf.pdf)\n",
    "- [TensorFlow - Bi-LSTM + CRF with character embeddings for NER](https://guillaumegenthial.github.io/sequence-tagging-with-tensorflow.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let $\\mathbf{x} = (x^1,...,x^M)$ denote a sequence of words and $\\mathbf{y} = (y^1,...,y^M)$ denote the corresponding sequence of part-of-speech tags.\n",
    "- $M$ is the length of a sequence and $\\mathbf{x}$ is encoded using $D$ words. So there are $D^M$ possible length-$M$ sequences of $\\mathbf{x}$\n",
    "- There are $L$ possible labels for each $y^i$ so there are $L^M$ possible length-$M$ sequences of $\\mathbf{y}$\n",
    "- Training set of $N$ training examples: $S = \\{(\\mathbf{x}_i, \\mathbf{y}_i)\\}_{i=1}^N$\n",
    "- Goal is to learn mapping from $\\mathbf{x}$ to $\\mathbf{y}$ by fitting a first order sequential Conditional Random Field (CRF) to $S$. A CRF is a `log-linear` conditional probabilistic model.\n",
    "\n",
    "---\n",
    "*$1^{st}$ Order Hidden Markov Model (*For understanding CRF*)*\n",
    "- Generative model $P(x, y)$, i.e. joint model over $x$ and $y$\n",
    "- Notation\n",
    "    - $P(x^i|y^i)$: Probability of state $y^i$ generating $x^i$\n",
    "    - $P(y^{i+1}|y^i)$: Probability of state $y^i$ transitioning to $y^{i+1}$\n",
    "    - $P(y^1|y^0)$: $y^0$ is defined to be the start state\n",
    "    - $P(END|y^M)$: Prior probability of $y^M$ being the final state (Not always used)\n",
    "\n",
    "$$P(x, y) = P(END|y^M)\\prod_{i=1}^M{P(y^i|y^{i-1})}\\prod_{i=1}^M{P(x^i|y^i})$$\n",
    "\n",
    "- In matrix notation (where $A$ denote `transition` probabilities and $O$ denote `emission` or `observation` probabilities)\n",
    "$$P(x, y) = A_{END, y^M}\\prod_{i=1}^M{A_{y^i, y^{i-1}}O_{y^i, x^i}}$$\n",
    "\n",
    "---\n",
    "Log-Linear $1^{st}$ Order Sequential CRF Model\n",
    "\n",
    "$$P(y|x) = \\frac{1}{Z(x)}\\exp\\left\\{\\sum_{i=1}^M(A_{y^i, y^{i-1}}O_{y^i, x^i})\\right\\} \\equiv \\frac{1}{Z(x)}\\exp\\{F(y,x)\\}$$\n",
    "\n",
    "- $Z(x) = \\sum_{y'}\\exp{F(y', x)}$ is `Partition function` which sums over the scores of all possible $y'$ and acts as a normalizer so that $P(y|x)$ yields a valid probability\n",
    "- $F(y,x) \\equiv \\sum_{i=1}^M(A_{y^i, y^{i-1}}O_{y^i, x^i})$ is `scoring` function or `joint discriminant` function\n",
    "- Example: $x$ = \"Dog Barks\", $y$ = (N, V)\n",
    "\n",
    "$$P(N,V|\\text{\"Dog Barks\"}) = \\frac{1}{Z(x)}\\exp\\left\\{A_{N,Start} + O_{N, Dog} + A_{V,N} + O_{V, Barks}\\right\\}$$\n",
    "\n",
    "---\n",
    "- Training via Gradient Descent and Dynamic Programming\n",
    "    - Gradienet descent to minimize the negative log loss over a training set. The gradient of log partition i.e. $\\text{log }Z(x)$ is equal to the sum of the expected value of each feature under the distribution induced by the current model $P(x, y)$. `Forward-Backward` algorithm is used to compute the expected value in the formulation of gradient of log partition\n",
    "    \n",
    "$$\\underset{\\theta}{argmin}\\sum_{i=1}^N - log(P(y_i|x_i)) = \\underset{\\theta}{argmin}\\sum_{i=1}^N -F(y_i, x_i) + log(Z(x_i))$$\n",
    "\n",
    "---\n",
    "\n",
    "- Prediction is made by using `Viterbi` Algorithm (one line in `TensorFlow`)\n",
    "    - $\\underset{y}{argmax}\\text{ }P(y|x) = \\underset{y}{argmax}\\text{ }log(P(y|x)) =\\underset{y}{argmax}\\text{ }F(y,x)$\n",
    "    - Viterbi algorithm does not compute $P(y|x)$, it just maximizes $F(y,x)$ so need to compute the partition function $Z(x)$\n",
    "    - Viterbi algorithm finds an entire sequence of states such that the sum of transition scores is maximized\n",
    "    \n",
    "---\n",
    "\n",
    "- Computing `Partition Function`\n",
    "    - Notation: Let matrix $G^i(b, a) = \\exp\\{A_{b, a}+O_{b, x^i}\\}$ encode the unnormalized probability of transitioning from $y^{i-1} = a$ to $y^i=b$ in the $i$-th token of the sequence. \n",
    "\n",
    "$$Z(x) = \\sum_{y'}\\prod_{i=1}^MG^i(y^{'i}, y^{'i-1})$$\n",
    "\n",
    "\n",
    "- Use Viterbi Algorithm to compute `Partition Function`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for BiLSTM_CRF\n",
    "def to_scalar(var):\n",
    "    return var.view(-1).data.tolist()[0]\n",
    "\n",
    "def argmax(vec):\n",
    "    _, idx = torch.max(vec, 1)\n",
    "    return to_scalar(idx)\n",
    "\n",
    "# Compute log sum exp in a numerically stable for the forward algorithm\n",
    "def log_sum_exp(vec):\n",
    "    max_score = vec[0, argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
    "\n",
    "# Helper functions for preparing sequences for BiLSTM_CRF\n",
    "def prepare_sequence(seq, to_idx, train=True):\n",
    "    indices = [to_idx[w] for w in seq]\n",
    "    if cuda:\n",
    "        tensor = torch.LongTensor(indices).cuda()\n",
    "    else:\n",
    "        tensor = torch.LongTensor(indices)\n",
    "        \n",
    "    if train:\n",
    "        return autograd.Variable(tensor, requires_grad=False)\n",
    "    else:\n",
    "        return autograd.Variable(tensor, requires_grad=False, volatile=True) # Solves GPU running out of memory issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "\n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=1, bidirectional=True)\n",
    "\n",
    "        # Maps the output of the LSTM into tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
    "\n",
    "        # Matrix of transition parameters. Entry i,j is the score of\n",
    "        # transitioning to i from j\n",
    "        self.transitions = nn.Parameter(torch.randn(self.tagset_size, self.tagset_size))\n",
    "        \n",
    "        # Enforce the constraint: never transition to start tag and never \n",
    "        # transition from stop tag\n",
    "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000.0\n",
    "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000.0\n",
    "\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        if cuda:\n",
    "            return (autograd.Variable(torch.randn(2, 1, self.hidden_dim // 2)).cuda(), \n",
    "                    autograd.Variable(torch.randn(2, 1, self.hidden_dim // 2)).cuda())\n",
    "        else:\n",
    "            return (autograd.Variable(torch.randn(2, 1, self.hidden_dim // 2)), \n",
    "                    autograd.Variable(torch.randn(2, 1, self.hidden_dim // 2)))\n",
    "\n",
    "    def _forward_alg(self, feats):\n",
    "        \"\"\"Forward algorithm for computing the partition function\"\"\"\n",
    "        init_alphas = torch.Tensor(1, self.tagset_size).fill_(-10000.0)\n",
    "                    \n",
    "        # START_TAG has all of the score.\n",
    "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.0\n",
    "\n",
    "        # Wrap in a variable so that we will get automatic backprop\n",
    "        if cuda:\n",
    "            forward_var = autograd.Variable(init_alphas).cuda()\n",
    "        else:\n",
    "            forward_var = autograd.Variable(init_alphas)\n",
    "\n",
    "        # Iterate through the sentence\n",
    "        for feat in feats:\n",
    "            alphas_t = []  # The forward variables at this timestep\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # broadcast the emission score: it is the same regardless of\n",
    "                # the previous tag\n",
    "                emit_score = feat[next_tag].view(1, -1).expand(1, self.tagset_size)\n",
    "                # the ith entry of trans_score is the score of transitioning to\n",
    "                # next_tag from i\n",
    "                trans_score = self.transitions[next_tag].view(1, -1)\n",
    "                # The ith entry of next_tag_var is the value for the\n",
    "                # edge (i -> next_tag) before we do log-sum-exp\n",
    "                next_tag_var = forward_var + trans_score + emit_score\n",
    "                # The forward variable for this tag is log-sum-exp of all the\n",
    "                # scores.\n",
    "                alphas_t.append(log_sum_exp(next_tag_var))\n",
    "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        alpha = log_sum_exp(terminal_var)\n",
    "        return alpha\n",
    "\n",
    "    def _get_lstm_features(self, sentence):\n",
    "        \"\"\"Returns LSTM output for a given sequence\"\"\"\n",
    "        self.hidden = self.init_hidden()\n",
    "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
    "        lstm_feats = self.hidden2tag(lstm_out)\n",
    "        return lstm_feats\n",
    "\n",
    "    def _score_sentence(self, feats, tags):\n",
    "        \"\"\"Returns the score of a provided tag sequence\"\"\"\n",
    "        if cuda:\n",
    "            score = autograd.Variable(torch.Tensor([0])).cuda()\n",
    "        else:\n",
    "            score = autograd.Variable(torch.Tensor([0]))\n",
    "            \n",
    "        tags = torch.cat([torch.LongTensor([self.tag_to_ix[START_TAG]]), tags])\n",
    "        for i, feat in enumerate(feats):\n",
    "            score = score + self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
    "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
    "        return score\n",
    "\n",
    "    def _viterbi_decode(self, feats):\n",
    "        \"\"\"Uses Viterbi algorithm to compute score and best tag sequence\"\"\"\n",
    "        backpointers = []\n",
    "\n",
    "        # Initialize the viterbi variables in log space\n",
    "        init_vvars = torch.Tensor(1, self.tagset_size).fill_(-10000.0)\n",
    "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
    "\n",
    "        # forward_var at step i holds the viterbi variables for step i-1\n",
    "        if cuda:\n",
    "            forward_var = autograd.Variable(init_vvars).cuda()\n",
    "        else:\n",
    "            forward_var = autograd.Variable(init_vvars)\n",
    "        \n",
    "        for feat in feats:\n",
    "            bptrs_t = [] # holds the backpointers for this step\n",
    "            viterbivars_t = [] # holds the viterbi variables for this step\n",
    "\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
    "                # previous step, plus the score of transitioning\n",
    "                # from tag i to next_tag.\n",
    "                # We don't include the emission scores here because the max\n",
    "                # does not depend on them (we add them in below)\n",
    "                next_tag_var = forward_var + self.transitions[next_tag]\n",
    "                best_tag_id = argmax(next_tag_var)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id])\n",
    "            # Now add in the emission scores, and assign forward_var to the set\n",
    "            # of viterbi variables we just computed\n",
    "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "            backpointers.append(bptrs_t)\n",
    "\n",
    "        # Transition to STOP_TAG\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        best_tag_id = argmax(terminal_var)\n",
    "        path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "        # Follow the back pointers to decode the best path.\n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        # Pop off the start tag (we dont want to return that to the caller)\n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        \"\"\"Returns score and tag sequence\"\"\"\n",
    "        # Get the emission scores from the BiLSTM\n",
    "        lstm_feats = self._get_lstm_features(sentence)\n",
    "\n",
    "        # Find the best path, given the features.\n",
    "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
    "        return score, tag_seq\n",
    "    \n",
    "    def neg_log_likelihood(self, sentence, tags):\n",
    "        \"\"\"Negative Log Likelihood\"\"\"\n",
    "        feats = self._get_lstm_features(sentence)\n",
    "        forward_score = self._forward_alg(feats)\n",
    "        gold_score = self._score_sentence(feats, tags)\n",
    "        return forward_score - gold_score\n",
    "      \n",
    "def train(model, optimizer, train_data):\n",
    "    \"\"\"\n",
    "    train_data -- list of tuples, e.g. [(['dog', 'ate'], ['NN', 'V']), (...), ...]\n",
    "    \"\"\"\n",
    "    for sentence, tags in train_data:\n",
    "        \n",
    "        # Step 1: Clear gradients as Pytorch accumulates gradients\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Step 2: Prepare inputs\n",
    "        sentence_in = prepare_sequence(sentence, word_to_idx)\n",
    "        targets = torch.LongTensor([tag_to_idx[tag] for tag in tags])\n",
    "        \n",
    "        # Step 3: Run forward pass and compute loss\n",
    "        neg_log_likelihood = model.neg_log_likelihood(sentence_in, targets)\n",
    "        \n",
    "        # Step 4: Compute gradients, optimize and update the gradients\n",
    "        neg_log_likelihood.backward()\n",
    "        optimizer.step()\n",
    "          \n",
    "def evaluate(model, data):\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    for sentence, tags in data:\n",
    "        \n",
    "        # Step 1: Prepare inputs\n",
    "        sentence_in = prepare_sequence(sentence, word_to_idx, train=False)\n",
    "        targets = torch.LongTensor([tag_to_idx[tag] for tag in tags])\n",
    "        \n",
    "        # Step 2: Forward pass\n",
    "        pred_score, pred_tags = model(sentence_in)\n",
    "                            \n",
    "        # Step 3: Compute accuracy\n",
    "        correct += torch.equal(torch.LongTensor(pred_tags), targets)\n",
    "        \n",
    "    return correct/float(len(data))\n",
    "\n",
    "# Keep only a single checkpoint, the best over test accuracy.\n",
    "def save_checkpoint(state, is_best, filename=None):\n",
    "    \"\"\"Save checkpoint if a new best is achieved\"\"\"\n",
    "    if is_best:\n",
    "        print (\"### Saving New Best Model Weights ###\")\n",
    "        torch.save(state, filename)  # save checkpoint\n",
    "    else:\n",
    "        print (\"Validation Accuracy did not improve!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  18\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters and data preparation\n",
    "START_TAG = '<START>'\n",
    "STOP_TAG = '<STOP>'\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 256\n",
    "num_epochs = 5\n",
    "\n",
    "learning_rate = 0.01\n",
    "best_accuracy = torch.FloatTensor([0])\n",
    "start_epoch = 0\n",
    "\n",
    "# Path to saved model weights (as hdf5)\n",
    "resume_weights = './bi-lstm-crf-ner/checkpoint.pth.tar'\n",
    "\n",
    "# GPU?\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "# Set GPU to use\n",
    "torch.cuda.set_device(2)\n",
    "\n",
    "# Seed for reproducibility\n",
    "torch.manual_seed(1)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(1)\n",
    "\n",
    "# Data\n",
    "training_data = [\n",
    "    ('the wall street journal reported today that apple corporation made money'.split(),\n",
    "     'B I I I O O O B I O O'.split()),\n",
    "    ('georgia tech is a university in georgia'.split(),\n",
    "     'B I O O O O B'.split())\n",
    "]\n",
    "\n",
    "testing_data = [\n",
    "    ('california tech is a university in california'.split(), # 'caltech' actually\n",
    "     'B I O O O O B'.split())\n",
    "]\n",
    "\n",
    "word_to_idx = {}\n",
    "for sentence, tags in training_data:\n",
    "    for word in sentence:\n",
    "        if word not in word_to_idx:\n",
    "            word_to_idx[word] = len(word_to_idx)\n",
    "            \n",
    "for sentence, tags in testing_data:\n",
    "    for word in sentence:\n",
    "        if word not in word_to_idx:\n",
    "            word_to_idx[word] = len(word_to_idx)\n",
    "            \n",
    "tag_to_idx = {'B': 0, 'I': 1, 'O': 2, START_TAG: 3, STOP_TAG: 4}\n",
    "\n",
    "print('Vocab size: ', len(word_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint: \"./bi-lstm-crf-ner/checkpoint.pth.tar\" ...\n",
      "Loaded checkpoint \"./bi-lstm-crf-ner/checkpoint.pth.tar\" (trained for 319 epochs)\n"
     ]
    }
   ],
   "source": [
    "# Model and optimizer\n",
    "model = BiLSTM_CRF(len(word_to_idx), tag_to_idx, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "    \n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "# If best model weights exist then load it\n",
    "if os.path.isfile(resume_weights):\n",
    "    print('Loading checkpoint: \"{}\" ...'.format(resume_weights))\n",
    "    \n",
    "    # Load weights\n",
    "    checkpoint = torch.load(resume_weights)\n",
    "    \n",
    "    start_epoch = checkpoint['epoch']\n",
    "    best_accuracy = checkpoint['best_accuracy']\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    print('Loaded checkpoint \"{}\" (trained for {} epochs)'.format(resume_weights, start_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-0 Test Set: Accuracy: 0.00\n",
      "Epoch-0 Train Set: Accuracy: 1.00\n",
      "Validation Accuracy did not improve!\n",
      "Epoch-1 Test Set: Accuracy: 0.00\n",
      "Epoch-1 Train Set: Accuracy: 1.00\n",
      "Validation Accuracy did not improve!\n",
      "Epoch-2 Test Set: Accuracy: 0.00\n",
      "Epoch-2 Train Set: Accuracy: 1.00\n",
      "Validation Accuracy did not improve!\n",
      "Epoch-3 Test Set: Accuracy: 0.00\n",
      "Epoch-3 Train Set: Accuracy: 1.00\n",
      "Validation Accuracy did not improve!\n",
      "Epoch-4 Test Set: Accuracy: 0.00\n",
      "Epoch-4 Train Set: Accuracy: 1.00\n",
      "Validation Accuracy did not improve!\n",
      "Training Time:  0:00:00.693733\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "start = datetime.now()\n",
    "for epoch in range(num_epochs):\n",
    "    train(model, optimizer, training_data)\n",
    "    acc = evaluate(model, testing_data)\n",
    "    tr_acc = evaluate(model, training_data)\n",
    "    print('Epoch-{} Test Set: Accuracy: {:.2f}'.format(epoch, acc))\n",
    "    print('Epoch-{} Train Set: Accuracy: {:.2f}'.format(epoch, tr_acc))\n",
    "        \n",
    "    acc = torch.FloatTensor([acc])\n",
    "    \n",
    "    # Get bool not ByteTensor\n",
    "    is_best = bool(acc.numpy() > best_accuracy.numpy())\n",
    "    \n",
    "    # Get greater tensor to keep track of best_accuracy\n",
    "    best_accuracy = torch.FloatTensor(max(acc.numpy(), best_accuracy.numpy()))\n",
    "    \n",
    "    # Save checkpoint\n",
    "    save_checkpoint({'epoch': start_epoch + epoch + 1,\n",
    "                     'state_dict': model.state_dict(),\n",
    "                     'optimizer_state_dict': optimizer.state_dict(),\n",
    "                     'best_accuracy': best_accuracy,}, is_best, filename=resume_weights)\n",
    "    \n",
    "end = datetime.now()\n",
    "elapsed = end - start\n",
    "print('Training Time: ', elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0), (1, 1), (2, 2), (2, 2), (2, 2), (2, 2), (1, 0)]"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction\n",
    "test_sent = prepare_sequence(testing_data[0][0], word_to_idx, train=False)\n",
    "test_tags = torch.LongTensor([tag_to_idx[tag] for tag in testing_data[0][1]])\n",
    "zip(model(test_sent)[1], test_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Viterbi Algorithm**\n",
    "\n",
    "**Viterbi Algorithm Explanation**\n",
    "- Dynamic Programming algorithm\n",
    "- Used to find the most likely sequence of hidden states that results in a sequence of observed events\n",
    "- HMM - The state is not directly visible but the observations dependent on the state are visible\n",
    "\n",
    "\n",
    "- **Input**\n",
    "    - State space $S=\\{s_1, s_2, ..., s_N\\}$\n",
    "    - Observation space $O=\\{o_1, o_2, ..., o_K\\}$\n",
    "    - Transition matrix $\\underset{N \\times N}A$ such that $A_{i,j}$ stores the transition probability of transiting from state $s_i$ to $s_j$\n",
    "    - Emission matrix $\\underset{N \\times K}B$ such that $B_{i,j}$ stores the probability of observing $o_j$ from state $s_i$\n",
    "    - An array of initial probabilities $\\pi$ of size $N$ such that $\\pi_i$ stores the probability of state $s_i$ at time $t=1$\n",
    "    - Sequence of observations $y_1, y_2, ..., y_T$\n",
    "- **Output**\n",
    "    - Most likely hidden state sequence $X = \\{x_1, x_2, ..., x_T\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.35  0.02  0.09]\n",
      " [ -inf  -inf  -inf]\n",
      " [ -inf  -inf  -inf]]\n",
      "[[ 0.35   0.02   0.09 ]\n",
      " [ 0.147  0.007  0.021]\n",
      " [  -inf   -inf   -inf]]\n",
      "[[ 0.35     0.02     0.09   ]\n",
      " [ 0.147    0.007    0.021  ]\n",
      " [ 0.00882  0.01764  0.00882]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0, 0, 1], 0.017639999999999999)"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize the model parameters\n",
    "\n",
    "# Transition matrix A(i,j): Probability of transitioning from state i to state j\n",
    "A = np.array([[0.6, 0.2, 0.2],\n",
    "              [0.5, 0.3, 0.2],\n",
    "              [0.4, 0.1, 0.5]])\n",
    "\n",
    "# Array of initial probabilities\n",
    "pi = np.array([0.5, 0.2, 0.3]) \n",
    "\n",
    "# Emission matrix O(i, j): Probability of observing Oj from state Si\n",
    "O = np.array([[0.7, 0.1, 0.2],\n",
    "              [0.1, 0.6, 0.3],\n",
    "              [0.3, 0.3, 0.4]])\n",
    "\n",
    "# State space\n",
    "states = UP, DOWN, UNCHANGED = 0, 1, 2\n",
    "\n",
    "observations = [UP, UP, DOWN]\n",
    "\n",
    "# Viterbi algorithm keeps a store of 2 components: (1) Best score to reach\n",
    "# a state at a given time; (2) Last step of the path to get there\n",
    "\n",
    "alpha = np.zeros((len(observations), len(states))) # time steps x states\n",
    "alpha[:,:] = float('-inf') # Initialized to -inf to denote that values are not set yet\n",
    "backpointers = np.zeros((len(observations), len(states)), dtype='int')\n",
    "\n",
    "# Base case for recursion sets the starting state probabilities based on phi and generating\n",
    "# the observations\n",
    "alpha[0, :] = pi * O[:, UP]\n",
    "print(alpha)\n",
    "\n",
    "# Recursive step: Maximize over incoming transitions reusing the best incoming score computed \n",
    "# above\n",
    "\n",
    "# time step 1\n",
    "for t1 in states:\n",
    "    for t0 in states:\n",
    "        score = alpha[0, t0] * A[t0, t1] * O[t1, UP]\n",
    "        if score > alpha[1, t1]:\n",
    "            alpha[1, t1] = score\n",
    "            backpointers[1, t1] = t0\n",
    "            \n",
    "print(alpha)\n",
    "\n",
    "# time step 2\n",
    "for t2 in states:\n",
    "    for t1 in states:\n",
    "        score = alpha[1, t1] * A[t1, t2] * O[t2, DOWN]\n",
    "        if score > alpha[2, t2]:\n",
    "            alpha[2, t2] = score\n",
    "            backpointers[2, t2] = t1\n",
    "            \n",
    "print(alpha)\n",
    "\n",
    "def viterbi(params, observations):\n",
    "    pi, A, O = params\n",
    "    M = len(observations)\n",
    "    S = pi.shape[0]\n",
    "    \n",
    "    alpha = np.zeros((M, S))\n",
    "    alpha[:,:] = float('-inf') # Initialized to -inf\n",
    "    backpointers = np.zeros((M, S), dtype='int')\n",
    "    \n",
    "    # base case\n",
    "    alpha[0, :] = pi * O[:,observations[0]]\n",
    "    \n",
    "    # Recursive case\n",
    "    for t in range(1, M):\n",
    "        for s2 in range(S):\n",
    "            for s1 in range(S):\n",
    "                score = alpha[t-1, s1] * A[s1, s2] * O[s2, observations[t]]\n",
    "                if score > alpha[t, s2]:\n",
    "                    alpha[t, s2] = score\n",
    "                    backpointers[t, s2] = s1\n",
    "    \n",
    "    # Follow backpointers to resolve the state sequence\n",
    "    ss = []\n",
    "    ss.append(np.argmax(alpha[M-1,:]))\n",
    "    for i in range(M-1, 0, -1):\n",
    "        ss.append(backpointers[i, ss[-1]])\n",
    "        \n",
    "    return list(reversed(ss)), np.max(alpha[M-1,:])\n",
    "\n",
    "viterbi((pi, A, O), [UP, UP, DOWN])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forward-Backward Algorithm**\n",
    "\n",
    "- Find the probability of an observation sequence *under any hidden state sequence*. This allows HMM to function as language models, but also is key to unsupervised training and the central algorithm for training.\n",
    "- The quantity to compute is $p(w) = \\sum_{t} p(t,w)$ where $w$ are the observations (words) and $t$ are the states (tags).\n",
    "\n",
    "$$p(w)  = \\sum_{t_1} \\sum_{t_2} \\cdots \\sum_{t_{N-1}} \\sum_{t_N} p(t,w)$$\n",
    "\n",
    "- Expand the HMM probability \n",
    "\n",
    "$$\n",
    "p(w)  = \\sum_{t_1} \\sum_{t_2} \\cdots \\sum_{t_{N-1}} \\sum_{t_N} p(t_1) p(w_1 | t_1) p(t_2 | t_1) p(w_2| t_2) \\cdots p(t_{N-1} | t_{N-2}) p(w_{N-1}| t_{N-1}) p(t_{N} | t_{N-1}) p(w_{N}| t_{N})\n",
    "$$\n",
    "\n",
    "- Compare the full marginal probability $p(w)$ and the probability up to position $N-1$, finishing with tag $t_{N-1}$\n",
    "\n",
    "$$p(w_1, w_2, \\ldots, w_{N-1}, t_{N-1}) = \\sum_{t_1} \\sum_{t_2} \\cdots \\sum_{t_{N-1}} p(t_1) p(w_1 | t_1) p(t_2 | t_1) p(w_2| t_2) \\cdots p(t_{N-1} | t_{N-2}) p(w_{N-1}| t_{N-1})\n",
    "$$\n",
    "\n",
    "- Express $p(w)$ more simply as\n",
    "\n",
    "$$\n",
    "p(w)  = \\sum_{t_N} p(w_1, w_2, \\ldots, w_{N-1}, t_{N-1}) p(t_{N} | t_{N-1}) p(w_{N}| t_{N})\n",
    "$$\n",
    "\n",
    "- Continue further by defining $p(w_1, w_2, \\ldots, w_{N-1}, t_{N-1})$ in terms of $p(w_1, w_2, \\ldots, w_{N-2}, t_{N-2})$ and so forth. (This is the same process used in the Viterbi algorithm, albeit swapping a max for a sum.)\n",
    "\n",
    "- Store a matrix of partial marginals, $\\alpha$ defined as follows\n",
    "\n",
    "$$\\alpha[i, t_i] = p(w_1, w_2, \\ldots, w_i, t_i)$$\n",
    "\n",
    "computed using the recursion\n",
    "\n",
    "$$\\alpha[i, t_i] = \\sum_{t_{i-1}} \\alpha[i-1, t_i] p(t_i | t_{i-1}) p(w_i| t_i)$$\n",
    "\n",
    "and the base case for $i=1$,\n",
    "\n",
    "$$\\alpha[1, t_1] = p(t_1) p(w_1 | t_1)$$\n",
    "\n",
    "- Iteratively compute the vector of alpha[1] values, then alpha[2] etc until the end of input\n",
    "\n",
    "- Backward Algorithm: The same process but working from left to right rather than right to left. Backward Algorithm is used for unsupervized learning (not 100% sure as I just read on few slides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([[ 0.35    ,  0.02    ,  0.09    ],\n",
       "         [ 0.1792  ,  0.0085  ,  0.0357  ],\n",
       "         [ 0.012605,  0.025176,  0.016617]]), 0.054398000000000002),\n",
       " (array([[ 0.1216,  0.1077,  0.1076],\n",
       "         [ 0.24  ,  0.29  ,  0.25  ],\n",
       "         [ 1.    ,  1.    ,  1.    ]]), 0.054397999999999995))"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def forward(params, observations):\n",
    "    pi, A, O = params\n",
    "    N = len(observations)\n",
    "    S = pi.shape[0]\n",
    "    \n",
    "    alpha = np.zeros((N, S))\n",
    "    \n",
    "    # Base case\n",
    "    alpha[0, :] = pi * O[:,observations[0]]\n",
    "    \n",
    "    # Recursive case\n",
    "    for i in range(1, N):\n",
    "        for s2 in range(S):\n",
    "            for s1 in range(S):\n",
    "                alpha[i, s2] += alpha[i-1, s1] * A[s1, s2] * O[s2, observations[i]]\n",
    "    \n",
    "    return (alpha, np.sum(alpha[N-1,:]))\n",
    "\n",
    "def backward(params, observations):\n",
    "    pi, A, O = params\n",
    "    N = len(observations)\n",
    "    S = pi.shape[0]\n",
    "    \n",
    "    beta = np.zeros((N, S))\n",
    "    \n",
    "    # Base case\n",
    "    beta[N-1, :] = 1\n",
    "    \n",
    "    # Recursive case\n",
    "    for i in range(N-2, -1, -1):\n",
    "        for s1 in range(S):\n",
    "            for s2 in range(S):\n",
    "                beta[i, s1] += beta[i+1, s2] * A[s1, s2] * O[s2, observations[i+1]]\n",
    "    \n",
    "    return (beta, np.sum(pi * O[:, observations[0]] * beta[0,:]))\n",
    "\n",
    "\n",
    "forward((pi, A, O), [UP, UP, DOWN]), backward((pi, A, O), [UP, UP, DOWN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-1000 -1000 -1000 -1000\n",
      "[torch.FloatTensor of size 1x4]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "-1000     0 -1000 -1000\n",
       "[torch.FloatTensor of size 1x4]"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st = '<START>'\n",
    "tag_to_idx = {'<START>': 1, 'NN': 0, 'V': 2, '<STOP>':3}\n",
    "a = torch.Tensor(1, len(tag_to_idx)).fill_(-1000.0)\n",
    "print(a)\n",
    "a[0][tag_to_idx[st]] = 0\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtype = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "# indices = [word_to_idx[w] for w in testing_data[0][0]]\n",
    "# tensor = torch.LongTensor(indices).type(dtype)\n",
    "# print(type(tensor))\n",
    "# E = nn.Embedding(len(word_to_idx), EMBEDDING_DIM).cuda()\n",
    "# E(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.cuda.LongTensor'>\n",
      "<class 'torch.cuda.LongTensor'>\n",
      "<class 'torch.LongTensor'>\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # Create a LongTensor and transfers it to GPU as torch.cuda.LongTensor\n",
    "    a = torch.LongTensor(10).fill_(3).cuda()\n",
    "    print(type(a))\n",
    "    b = a.cpu()\n",
    "    # Transfer it to CPU, back to being a torch.LongTensor\n",
    "    print(type(a))\n",
    "    print(type(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.autograd.variable.Variable'>\n",
      "<class 'torch.autograd.variable.Variable'>\n"
     ]
    }
   ],
   "source": [
    "test_sent = prepare_sequence(testing_data[0][0], word_to_idx)\n",
    "print(type(test_sent))\n",
    "if cuda:\n",
    "    test_sent.cuda()\n",
    "    \n",
    "print(type(test_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       "  0.2793 -1.8519 -1.0544 -2.8030  1.6265\n",
       "  0.8026 -0.3939  0.6067  1.5321  0.0224\n",
       "  0.5590  0.7341  0.9777  1.1262  0.4591\n",
       " -0.4116 -0.6755 -0.2887  0.6900  0.3158\n",
       " -1.4657  1.2674  0.8226 -1.4745  0.5069\n",
       " -0.9811 -2.3167  0.3534  0.0263 -0.2052\n",
       "  0.2793 -1.8519 -1.0544 -2.8030  1.6265\n",
       " [torch.cuda.FloatTensor of size 7x5 (GPU 2)],\n",
       " torch.cuda.LongTensor,\n",
       " torch.autograd.variable.Variable,\n",
       " torch.autograd.variable.Variable)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # TypeError fix\n",
    "# TypeError: torch.index_select received an invalid combination of arguments - \n",
    "# got (torch.cuda.FloatTensor, int, torch.LongTensor), but expected \n",
    "# (torch.cuda.FloatTensor source, int dim, torch.cuda.LongTensor index)\n",
    "\n",
    "indices = [word_to_idx[w] for w in testing_data[0][0]]\n",
    "tensor = torch.LongTensor(indices).cuda()\n",
    "V = autograd.Variable(tensor, requires_grad=False)\n",
    "E = nn.Embedding(len(word_to_idx), EMBEDDING_DIM).cuda()\n",
    "E(V), type(tensor), type(V), type(E(V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence_old(seq, to_idx, train=True):\n",
    "    \"\"\"NOTE: This does not work well!\"\"\"\n",
    "    dtype = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "    indices = [to_idx[w] for w in seq]\n",
    "    tensor = torch.LongTensor(indices)\n",
    "    if train:\n",
    "        return autograd.Variable(tensor.type(dtype), requires_grad=False)\n",
    "    else:\n",
    "        return autograd.Variable(tensor.type(dtype), requires_grad=False, volatile=True) # Solves GPU running out of memory issue\n",
    "    \n",
    "def data_from_df_to_list(df):\n",
    "    tuples = [tuple(x) for x in df.values]\n",
    "    data = [(r, s.split(), l.split()) for r, s, l, _ in tuples]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def neg_log_likelihood(self, sentence, tags):\n",
    "#     feats = self._get_lstm_features(sentence)\n",
    "#     forward_score = self._forward_alg(feats)\n",
    "#     gold_score = self._score_sentence(feats, tags)\n",
    "#     return forward_score - gold_score\n",
    "\n",
    "# neg_log_likelihood(model, test_sent, test_tags) # Works outside!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = model._get_lstm_features(test_sent) # Works now for GPU\n",
    "score = model._score_sentence(features, test_tags) # Works now for GPU\n",
    "alpha = model._forward_alg(features) # Works now for GPU\n",
    "path_score, best_path = model._viterbi_decode(features) # Works now for GPU\n",
    "\n",
    "# model.neg_log_likelihood(features, test_tags) # AssertionError: Embedding doesn't compute the gradient w.r.t. the indices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
