{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ImageNet Classification with Deep Convolutional Neural Networks\n",
    "\n",
    "**Authors: ** A. Krizhevsky, I. Sutskever, and G. Hinton  \n",
    "**Link: **http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf  \n",
    "\n",
    "---\n",
    "\n",
    "- Trained a large deep CNN to classify 1.2 million high-resolution images in the ImageNet contest into 1000 different classes (50,000 validation images, 150,000 testing images). \n",
    "- ImageNet consists of variable-resolution images: Down-sampled images to fixed resolution of 256 x 256. \n",
    "- Preprocessing: Subtracted mean over the training set from each pixel\n",
    "- Neural Network has 60 million parameters, 650,000 neurons, 5 convolutional layers (some followed by max-pooling layers), 3 fully-connected layers, final layer uses softmax.\n",
    "- Used non-saturating neurons (ReLU)\n",
    "- Efficient GPU implementation of convolution operation (5-6 days to train)\n",
    "- Used \"dropout\" regularization method to reduce overfitting. \n",
    "- Architecture: Network is spread across 2 GPUs\n",
    "![](images/alexnet1.png)\n",
    "- ReLU Nonlinearity: \n",
    "    - Saturating non-linearities: $f(x) = tanh(x)$ and $f(x) = (1+e^{-x})^{-1}$: Computationally expensive during training with SGD compared to non-saturating non-linearity $f(x) = max(0, x)$\n",
    "    - ReLUs do not require input normalization to prevent them from saturating\n",
    "- Found that using Local Response Normalization aids generalization\n",
    "- Pooling Layers: A grid of pooling units, each summarizing a neighborhood of size $z \\times z$ centered at the location of the pooling unit. Overlapping pooling reduces error.\n",
    "- Cost: Maximizing average across training cases of the log-probability of the correct label under the prediction distribution\n",
    "- Reducing Overfitting\n",
    "    - Data Augmentation: Artificially augmented dataset using label preserving transformations (reduces overfitting)\n",
    "        - Generated image translations and horizontal reflections (extracted random patches and trained network on extracted patches)\n",
    "        - Altered intensities of RGB channels (PCA)\n",
    "    - Dropout: Set the output of each hidden neuron to zero with probability 0.5\n",
    "        - Reduces complex co-adaptations of neurons (enables a neuron not to rely on the presence of particular other neurons). Neurons are forced to learn more robust features that are useful in conjunction with many different random subsets of other neurons.\n",
    "- Training:\n",
    "    - SGD with batch size = 128 examples, momentum = 0.9, decay = 0.0005\n",
    "    - Weights initialized from a zero mean Gaussian distribution with standard deviation 0.01. Few layers biases initialized with constant 1 (accelerates early stages of learning by providing the ReLUs with positive input) and remaining with constant 0\n",
    "![](images/alexnet2.png)\n",
    "\n",
    "---\n",
    "\n",
    "**Information**\n",
    "- ImageNet: 15 million labeled high-resolution (variable) images in over 22,000 categories. Images collected from the web and labeled using Amazon Mechanical Turk.\n",
    "\n",
    "---\n",
    "\n",
    "- [Code to produce Convolution figures](https://github.com/vdumoulin/conv_arithmetic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
