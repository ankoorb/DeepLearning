{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going Deeper with Convolutions\n",
    "\n",
    "**Authors:** C. Szegedy, W. Liu, Y. Jia, *et al.*  \n",
    "**Link:** https://arxiv.org/pdf/1409.4842.pdf  \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "- Proposed a deep CNN architecture (Inception-\"Network in network\") with increased depth and width of the network while improving utilization of computing resources (Models designed to keep a computational budget of 1.5 billion multiply-adds at inference time)\n",
    "- `Hebbian principle` - Neurons that fire together, wire together: A method of determining how to alter the weights between model neurons. The weight between two neurons increases if the two neurons activate simultaneously, and reduces if they activate separately. Nodes that tend to be either both positive or both negative at the same time have strong positive weights, while those that tend to be opposite have strong negative weights.\n",
    "- GoogLeNet uses 12 times fewer parameters than AlexNet and is 22 layer deep. \n",
    "- Max pooling layers sometimes result in loss of accurate spatial information.\n",
    "    - Advantages of Max pooling\n",
    "        - No parameters\n",
    "        - Often accurate\n",
    "    - Disadvantages of Max pooling\n",
    "        - More computationally expensive\n",
    "        - More hyper-parameters (pooling size and stride)\n",
    "- [1 x 1 Explanation](https://www.youtube.com/watch?v=qVP574skyuM): 1 x 1 convolutional layers are used as dimension reduction modules to remove computational bottlenecks. This allows increase in depth as well as width (number of units at each level) of network. For example, a feature map with size 100 x 100 x C channels on convolution with $k$ 1 x 1 filters would result in a feature map of size 100 x 100 x $k$. \n",
    "- Deep network has drawbacks:\n",
    "    - Large number of parameters make deep network more prone to overfitting\n",
    "    - Training a deep network requires a lot of computational resources.\n",
    "    - Solution: Efficient distribution of computational resources and introduce sparsity and replace fully connected layers by sparse layers\n",
    "- Architecture\n",
    "    - Filter size 1 x 1, 3 x 3, and 5 x 5 are used to avoid patch alignment issues\n",
    "    - [Inception modules](https://www.youtube.com/watch?v=VxhSouuSZDY): Used 9 inception modules with over 100 layers in total\n",
    "        - Naive version:\n",
    "            - Merging of outputs of the pooling layer with outputs of the convolutional layer would increase the number of outputs from stage to stage and this will lead to a computational blow up within a few stages\n",
    "\t- Dimensionality Reduction Inception module (idea based on embeddings): Using 1 x 1 filter size to reduce dimension as well as to increase non-linearity\n",
    "    ![](images/gnet0.png)\n",
    "    - All convolutions use ReLU non-linearity for activations\n",
    "    ![](images/gnet1.png)\n",
    "    - \"#3 x3 reduce\" and \"#5 x 5 reduce\" stands for the number of 1 x 1 filters in the reduction layer used before the 3 x 3 and 5 x 5 convolutions.\n",
    "    ![](images/gnet2.png)\n",
    "    ![](images/gnet3.png)\n",
    "    - Auxiliary classifiers were added to intermediate layers to combat vanishing gradient problem while providing regularization. During training auxiliary classifier loss (with discount weight 0.3) gets added to total loss of the network. \n",
    "    - Used Average pooling layer\n",
    "- Training:\n",
    "    - GoogLeNet networks were trained using the **DistBelief** (Large Scale Distributed Deep Networks) distributed machine learning system\n",
    "    - Asynchronous SGD with momentum = 0.9, learning rate decreased by 4% every 8 epochs. \n",
    "    - ImageNet 2012 dataset (1.3 Million images)\n",
    "- Trained 7 versions of same GoogLeNet model and performed ensemble prediction and obtained a top-5 error of 6.67%\n",
    "\n",
    "---\n",
    "\n",
    "**Object Localization**\n",
    "- Approach similar to R-CNN used but augmented with inception model as the region classifier\n",
    "\n",
    "---\n",
    "![](images/GoogLeNet.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
