{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introspective Classifier Learning: Empower Generatively\n",
    "\n",
    "**Authors:** L. Jin, J. Lazarow, Z. Tu  \n",
    "**Link:** https://arxiv.org/pdf/1704.07816.pdf  \n",
    "\n",
    "---\n",
    "\n",
    "**Contributions**\n",
    "- Proposed `Introspective Classifier Learning` (ICL) Framework - A single model that is simultaneously discriminative and generative\n",
    "- Studied how generative aspect of their a benefits its own discriminative training\n",
    "- Developed an efficient sampling procedure to synthesize new data (from scratch) from a discriminative classifier\n",
    "- Developed *Reclassification-By-Synthesis* algorithm to iteratively augment negative samples and update the classifier.\n",
    "- Proposed a formulation to train a *multi-class classifier* on training set and augmented samples.\n",
    "\n",
    "---\n",
    "\n",
    "**Improving Classifier Performance**\n",
    "- Using more data (hard examples) to train the classifier (a common way)\n",
    "- Bootstrapping, active learning, semi-supervised learning\n",
    "- Data augmentation\n",
    "- *Above approaches use data that is already present in training set or created by humans or separate algorithms* - Utilizes positive samples\n",
    "\n",
    "---\n",
    "\n",
    "**Generative-Discriminative Modeling Concept** (Tu, 2007)\n",
    "- A generative model can be successfully modeled by learning a sequence of discriminative classifiers via self-generated samples called `pseudo-negatives`\n",
    "- `New samples that pass the learned classifier are considered as a new set of pseudo-negatives in the next round`\n",
    "\n",
    "---\n",
    "\n",
    "**ICL Advantages compared to other approaches**\n",
    "- Convolutional - Automatic feature learning (Vs features pre-selected manually)\n",
    "- More efficient learning process\n",
    "- More efficient sampling process (Vs time consuming MCMC simulations)\n",
    "- Simplicity of having a single classifier as opposed having a sequence of boosting classifiers\n",
    "\n",
    "---\n",
    "\n",
    "- `Reference distribution` - Generates first batch of pseudo-negatives\n",
    "- Discriminative classifier is trained to separate the given input data and `pseudo-negatives`\n",
    "- Algorithm repeats until `pseudo-negatives` are no longer distinguishable from the input data\n",
    "\n",
    "- Discriminative classifier computes the probability of $\\mathbf x$ being positive or negative, i.e. compute: $p(y|\\mathbf x)$\n",
    "\n",
    "- Probabilities should sum to 1, i.e. $p(y=+1|\\mathbf x) + p(y=-1|\\mathbf x) = 1$\n",
    "\n",
    "- Generative model models $p(y,\\mathbf x) = p(\\mathbf x|y)p(y)$. This captures the underlying generation process of $\\mathbf x$ for class $y$\n",
    "\t\n",
    "- Binary classification: Positive samples are of primary interest. Using Bayes theorem and assuming equal priors $p(y=+1)=p(y=-1)$: \n",
    "\n",
    "\t$$p(\\mathbf x|y=+1) = \\frac{p(y=+1|\\mathbf x)}{p(y=-1|\\mathbf x)}p(\\mathbf x|y=-1)$$\n",
    "\n",
    "---\n",
    "\n",
    "**Input** - Input data has an underlying distribution we wish to learn\n",
    "\n",
    "![](images/icl-data.png)\n",
    "\n",
    "- Goal is to gradually learn $p_t(\\mathbf x|y=-1)$ such that samples drawn from it become indistinguishable from the given positive samples, i.e. $p_t(\\mathbf x|y=-1)\\overset{t=\\infty}{\\rightarrow}p(\\mathbf x|y=+1)$\n",
    "\n",
    "![](images/icl-goal.png)\n",
    "\n",
    "---\n",
    "\n",
    "**Initialization**\n",
    "\n",
    "![](images/icl-init.png)\n",
    "\n",
    "---\n",
    "\n",
    "**Synthesis-Reclassification Loop**\n",
    "\n",
    "**For $t=0, 1, ..., T$**\n",
    "\n",
    "Update **Generative** Model: \n",
    "\n",
    "$p_t(\\mathbf x|y=-1) = \\frac{1}{Z_t}\\frac{q_t(y=+1|\\mathbf x)}{q_t(y=-1|\\mathbf x)}p_r(\\mathbf x|y=-1)$ \n",
    "\n",
    "- Where \n",
    "$Z_t = \\int \\frac{q_t(y=+1|\\mathbf x)}{q_t(y=-1|\\mathbf x)}p_r(\\mathbf x|y=-1)dx$\n",
    "\n",
    "- $Z_t$ is Partition Function (also known as Normalizing Constant). Unnormalized probability distribution is guaranteed to be non-negative everywhere, however it is not guaranteed to sum or integrate to 1. To obtain a valid probability distribution the unnormalized probability distribution needs to be normalized: $p(x) = \\frac{1}{Z}\\tilde{p(x)}$\n",
    "\n",
    "- $Z$ is integral or sum over all possible joint assignments of the state $x$, i.e. $Z = \\int \\tilde{p(x)} dx$ and it is often intractable to compute (resort to approximation).\n",
    "    \n",
    "- **Synthesis Step**\n",
    "    - Sample $l$ pseudo-negative samples \n",
    "    \n",
    "    ![](images/l-samples.png)\n",
    "    \n",
    "    - $\\mathbf x_i \\sim p_t(\\mathbf x|y=-1), i=n+tl+1, ..., n+tl+l$ from the current model $p_t(\\mathbf x|y=-1)$ using `Variational Sampling Procedure (Stochastic Gradient on Input)`\n",
    "    \n",
    "    - Authors update a random sample $$\\mathbf x$$ drawn from $p_r(\\mathbf x|y=-1)$ by increasing $\\frac{q_t(y=+1|\\mathbf x; W_t)}{q_t(y=-1|\\mathbf x; W_t)}$ using backpropagation.\n",
    "    \n",
    "- **Augment Pseudo-negative Set**\n",
    "\n",
    "\t![](images/pn-samples-t.png) \n",
    "\n",
    "    - $\\mathbf S_{pn}^{t+1} = \\mathbf S_{pn}^{t} \\cup \\{(\\mathbf x_i, -1), i=n+tl+1, ..., n+tl+l\\}$\n",
    "    \n",
    "\t![](images/pn-samples-tt.png)\n",
    "\n",
    "- **Reclassification Step**\n",
    "    - Expand training set: $\\mathbf S_{e}^{t+1} = \\mathbf S \\cup \\mathbf S_{pn}^{t+1}$\n",
    "    \n",
    "    ![](images/e-set.png) \n",
    "\n",
    "    - Update CNN classifier to $\\mathbf C^{t+1}$ on expanded set $\\mathbf S_{e}^{t+1}$ to get $q_{t+1}(y=+1|\\mathbf x)$\n",
    "    \n",
    "    ![](images/c-t.png)\n",
    "    \n",
    "- $t \\leftarrow t+1$ until convergence *(e.g. no improvement on validation set)*\n",
    "\n",
    "---\n",
    "\n",
    "**Synthesis Process**\n",
    "\n",
    "- Goal is to draw fair samples from \n",
    "\n",
    "$p_t(\\mathbf x|y=-1) = \\frac{1}{Z_t}\\frac{q_t(y=+1|\\mathbf x; W_t)}{q_t(y=-1|\\mathbf x; W_t)}p_r(\\mathbf x|y=-1)$\n",
    "\n",
    "- Update a random sample $\\mathbf x$ drawn from $p_t(\\mathbf x|y=-1)$ by increasing $\\frac{q_t(y=+1|\\mathbf x; W_t)}{q_t(y=-1|\\mathbf x|W_t)}$ using backpropagation.\n",
    "\n",
    "- Partition function $Z_t$ is a constant and not dependent on the sample $\\mathbf x$\n",
    "- Let $g_t(\\mathbf x) = \\frac{q_t(y=+1|\\mathbf x; W_t)}{q_t(y=-1|\\mathbf x|W_t)} = \\exp\\{\\mathbf w_t^{(1)T}\\phi(\\mathbf x; \\mathbf w_t^{(0)})\\}$, which is essentially odds ratio. Taking *natural log* of both sides: $ln(g_t(\\mathbf x)) = \\mathbf w_t^{(1)T}.\\phi(\\mathbf x; \\mathbf w_t^{(0)})$\n",
    "\n",
    "- Starting from $x$ drawn from $p_r(\\mathbf x|y=-1)$, authors directly increase  $\\mathbf w_t^{(1)T}.\\phi(\\mathbf x; \\mathbf w_t^{(0)})$ using stochastic gradient **ascent** on $\\mathbf x$ via backpropagation, which allows them to obtain fair samples subject to $p_t(\\mathbf x|y=-1) = \\frac{1}{Z_t}\\frac{q_t(y=+1|\\mathbf x; W_t)}{q_t(y=-1|\\mathbf x; W_t)}p_r(\\mathbf x|y=-1)$\n",
    "\n",
    "---\n",
    "\n",
    "**Synthesis Process Example**\n",
    "\n",
    "- Sample **3** samples $\\{x_1, x_2, x_3\\}$ from $p_t(\\mathbf x|y=-1)$\n",
    "\n",
    "  ![](images/samples-3.png)\n",
    "\n",
    "- Update sample $x_i$ in **3** samples by increasing $\\mathbf w_t^{(1)T}.\\phi(\\mathbf x_i; \\mathbf w_t^{(0)})$ using `stochastic gradient ascent` on $\\mathbf x_i$ via backpropagation as follows:\n",
    "\n",
    "  ![](images/synth-input.png)\n",
    "  \n",
    "  ![](images/synth-grad.png)\n",
    "  \n",
    "  ![](images/synth-update.png)\n",
    "  \n",
    "---\n",
    "\n",
    "NOTE: Check paper for `Multi-class Classification Loss Function` derivation and details\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
