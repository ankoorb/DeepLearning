{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Phase-I: Assemble Graph**\n",
    "1. Import data (`tf.data` or define `placeholders` for input and output)\n",
    "2. Define the `weights`\n",
    "3. Define the inference `model` (i.e. Forward path of the model)\n",
    "4. Define `cost` function\n",
    "5. Define `optimizer`\n",
    "\n",
    "**Phase-II: Execute Computation (i.e. Train Model)**\n",
    "1. Initialize all model variables for the first time\n",
    "2. Feed in the training data. Might involve randomizing the order of data samples\n",
    "3. Execute the inference `model` on the training data\n",
    "4. Compute the `loss`\n",
    "5. Adjust the model `weights` to minimize/maximize `loss` depending on the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Word Embedding`**\n",
    "- Captures the semantic relationships between words\n",
    "\n",
    "**Word2Vec: skip-gram**\n",
    "- Softmax is computationally expensive (because of the size of word vocabulary)\n",
    "- Negative sampling (a simplified version of `Noise Contrastive Estimation (NCE)`). NCE guarantes approximation to softmax, where as Negative sampling does not approximate to softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # Adjust verbosity to suppress information logs\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "\n",
    "tfe.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Pre-Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "import zipfile\n",
    "import random\n",
    "\n",
    "from six.moves import urllib\n",
    "from collections import Counter\n",
    "\n",
    "# Utility functions\n",
    "def make_dir(path):\n",
    "    \"\"\"Create a directory if directory does not exist.\"\"\"\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError:\n",
    "        pass\n",
    "    \n",
    "def download_one_file(download_url, local_dest, expected_byte=None, unzip_and_remove=False):\n",
    "    \"\"\" \n",
    "    Download the file from download_url into local_dest, if the file doesn't already exists.\n",
    "    If expected_byte is provided, check if the downloaded file has the same number of bytes.\n",
    "    If unzip_and_remove is True, unzip the file and remove the zip file\n",
    "    \"\"\"\n",
    "    if os.path.exists(local_dest) or os.path.exists(local_dest[:-3]):\n",
    "        print('%s already exists' % local_dest)\n",
    "    else:\n",
    "        print('Downloading %s' % download_url)\n",
    "        local_file, _ = urllib.request.urlretrieve(download_url, local_dest)\n",
    "        file_stat = os.stat(local_dest)\n",
    "        if expected_byte:\n",
    "            if file_stat.st_size == expected_byte:\n",
    "                print('Successfully downloaded %s' % local_dest)\n",
    "                if unzip_and_remove:\n",
    "                    with gzip.open(local_dest, 'rb') as f_in, open(local_dest[:-3],'wb') as f_out:\n",
    "                        shutil.copyfileobj(f_in, f_out)\n",
    "                    os.remove(local_dest)\n",
    "            else:\n",
    "                print('The downloaded file has unexpected number of bytes')\n",
    "\n",
    "def read_data(file_path):\n",
    "    \"\"\"Read data into a list of tokens. There should be 17,005,207 tokens.\"\"\"\n",
    "    with zipfile.ZipFile(file_path) as f:\n",
    "        # Convert input to string using Python 2 vs 3 compatibility as_str()\n",
    "        tokens = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    return tokens\n",
    "\n",
    "def build_vocab(tokens, vocab_size, visual_fld):\n",
    "    \"\"\"Build vocabulary of VOCAB_SIZE most frequent tokens and write\n",
    "    it to visualization/vocab.tsv\n",
    "    \"\"\"\n",
    "    # Create vocabulary\n",
    "    dictionary = dict()\n",
    "    count = [('UNK', -1)] # For Unknown words\n",
    "    count.extend(Counter(tokens).most_common(vocab_size - 1)) # Extend the list (ensures that UNK is 0th)\n",
    "    index = 0\n",
    "    \n",
    "    # Create directory to store vocab.tsv\n",
    "    make_dir(visual_fld)\n",
    "    with open(os.path.join(visual_fld, 'vocab.tsv'), \"w\") as f:\n",
    "        for token, _ in count:\n",
    "            dictionary[token] = index\n",
    "            index += 1\n",
    "            f.write(token + '\\n')\n",
    "            \n",
    "    index_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return dictionary, index_dictionary\n",
    "\n",
    "def convert_tokens_to_index(tokens, dictionary):\n",
    "    \"\"\"Replace each token in the dataset with its index in the dictionary.\"\"\"\n",
    "    return [dictionary[token] if token in dictionary else 0 for token in tokens]\n",
    "\n",
    "def generate_sample(index_tokens, context_window_size):\n",
    "    \"\"\"Form training pairs according to the skip-gram model.\"\"\"\n",
    "    for index, center in enumerate(index_tokens):\n",
    "        context = random.randint(1, context_window_size)\n",
    "        # Get a random target token before the center token\n",
    "        for target in index_tokens[max(0, index - context): index]:\n",
    "            yield center, target\n",
    "        # Get a random target token after the center token\n",
    "        for target in index_tokens[index + 1: index + context + 1]:\n",
    "            yield center, target\n",
    "            \n",
    "def most_common_tokens(visual_fld, num_visualize):\n",
    "    \"\"\"Create a list of num_visualize most frequent words to visualize\n",
    "    on Tenserboard. The list is saved to visualization/vocab_[num_visualize].tsv\"\"\"\n",
    "    vocab = open(os.path.join(visual_fld, 'vocab.tsv'), 'r').readlines()[:num_visualize]\n",
    "    tokens = [token for token in vocab]\n",
    "    path = os.path.join(visual_fld, 'vocab_' + str(num_visualize) + '.tsv')\n",
    "    with open(path, 'w') as f:\n",
    "        for token in tokens:\n",
    "            f.write(token)\n",
    "\n",
    "def batch_gen(download_url, expected_byte, vocab_size, batch_size, skip_window, visual_fld):\n",
    "    local_dest = './data/text8.zip'\n",
    "    download_one_file(download_url, local_dest, expected_byte)\n",
    "    tokens = read_data(local_dest)\n",
    "    dictionary, _ = build_vocab(tokens, vocab_size, visual_fld)\n",
    "    index_tokens = convert_tokens_to_index(tokens, dictionary)\n",
    "    del tokens  # To save memory\n",
    "    single_gen = generate_sample(index_tokens, skip_window)\n",
    "    \n",
    "    while True:\n",
    "        center_batch = np.zeros(batch_size, dtype=np.int32)\n",
    "        target_batch = np.zeros([batch_size, 1])\n",
    "        for index in range(batch_size):\n",
    "            center_batch[index], target_batch[index] = next(single_gen)\n",
    "        yield center_batch, target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for downloading data\n",
    "DOWNLOAD_URL = 'http://mattmahoney.net/dc/text8.zip'\n",
    "EXPECTED_BYTES = 31344016\n",
    "\n",
    "# Model Hyper-parameters\n",
    "VOCAB_SIZE = 50000\n",
    "BATCH_SIZE = 64\n",
    "EMBED_SIZE = 300  # Dimension of the word embedding vectors\n",
    "SKIP_WINDOW = 1  # Context window size\n",
    "NUM_SAMPLED = 25  # Number of negative examples to sample\n",
    "LEARNING_RATE = 1.0\n",
    "NUM_TRAIN_STEPS = 20000\n",
    "SKIP_STEP = 2000  # Steps to skip before reporting the loss\n",
    "VISUAL_FLD = 'visualization'  # Directory name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(object):\n",
    "    def __init__(self, vocab_size, embed_size, num_sampled=NUM_SAMPLED):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.num_sampled = num_sampled\n",
    "        \n",
    "        # Create the variables: embedding matrix, nce weight, nce bias\n",
    "        # ============================================================\n",
    "        # If one word is represented with a vector of size EMBED_SIZE, then the embedding matrix will \n",
    "        # have shape [VOCAB_SIZE, EMBED_SIZE]\n",
    "        self.embed_matrix = tfe.Variable(initial_value=tf.random_uniform([self.vocab_size, self.embed_size], \n",
    "                                                                        minval=-1, maxval=1), name='embeddings')\n",
    "        \n",
    "        # Shape: [VOCAB_SIZE, EMBED_SIZE]\n",
    "        self.nce_weight = tfe.Variable(initial_value=tf.truncated_normal([self.vocab_size, self.embed_size], \n",
    "                                                                        stddev=1.0/(self.embed_size**0.5)), name='nce_weight')\n",
    "        \n",
    "        # Shape: VOCAB_SIZE\n",
    "        self.nce_bias = tfe.Variable(tf.zeros([self.vocab_size]), name='nce_bias')\n",
    "        \n",
    "        \n",
    "    def compute_loss(self, center_words, target_words):\n",
    "        \"\"\"Computes the forward pass of Word2Vec with NCE loss.\"\"\"\n",
    "        # Look up the embeddings for the center word\n",
    "        embed = tf.nn.embedding_lookup(self.embed_matrix, center_words, name='embed')\n",
    "        \n",
    "        \n",
    "        # Compute loss using: tf.reduce_mean and tf.nn.nce_loss\n",
    "        loss = tf.nn.nce_loss(weights=self.nce_weight, biases=self.nce_bias, \n",
    "                              labels=target_words, inputs=embed, num_sampled=self.num_sampled, \n",
    "                              num_classes=self.vocab_size)\n",
    "        \n",
    "        return tf.reduce_mean(loss)\n",
    "    \n",
    "\n",
    "\n",
    "def generate():\n",
    "    yield from batch_gen(DOWNLOAD_URL, EXPECTED_BYTES, VOCAB_SIZE, \n",
    "                         BATCH_SIZE, SKIP_WINDOW, VISUAL_FLD)\n",
    "    \n",
    "\n",
    "def train():\n",
    "    # Dataset\n",
    "    dataset = tf.data.Dataset.from_generator(generate, output_types=(tf.int32, tf.int32), \n",
    "                                             output_shapes=(tf.TensorShape([BATCH_SIZE]), \n",
    "                                                            tf.TensorShape([BATCH_SIZE, 1])))\n",
    "\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer([LEARNING_RATE])\n",
    "    \n",
    "    # Create the model\n",
    "    model = Word2Vec(vocab_size=VOCAB_SIZE, embed_size=EMBED_SIZE)\n",
    "    \n",
    "    # Create the gradients function (using tfe.implicit_value_and_gradients)\n",
    "    grad_fn = tfe.implicit_value_and_gradients(model.compute_loss)\n",
    "    \n",
    "    \n",
    "    total_loss = 0.0  # For average loss in the last SKIP_STEP steps\n",
    "    num_train_steps = 0\n",
    "    while num_train_steps < NUM_TRAIN_STEPS:\n",
    "        for center_words, target_words in tfe.Iterator(dataset):\n",
    "            if num_train_steps >= NUM_TRAIN_STEPS:\n",
    "                break\n",
    "                \n",
    "            # Compute the loss and gradients, and take an optimizaton step\n",
    "            loss_batch, grads = grad_fn(center_words, target_words)\n",
    "            total_loss += loss_batch\n",
    "            optimizer.apply_gradients(grads)\n",
    "            \n",
    "            if (num_train_steps + 1) % SKIP_STEP == 0:\n",
    "                print('Average loss at step {}: {:5.1f}'.format(num_train_steps,\n",
    "                                                                total_loss/SKIP_STEP))\n",
    "                total_loss = 0.0\n",
    "            num_train_steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/text8.zip already exists\n",
      "Average loss at step 1999:  68.2\n",
      "Average loss at step 3999:  42.7\n",
      "Average loss at step 5999:  32.3\n",
      "Average loss at step 7999:  26.5\n",
      "Average loss at step 9999:  22.2\n",
      "Average loss at step 11999:  19.2\n",
      "Average loss at step 13999:  16.8\n",
      "Average loss at step 15999:  15.2\n",
      "Average loss at step 17999:  13.2\n",
      "Average loss at step 19999:  12.0\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Step\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(skg.global_step.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'a', 'b', 'c', 'd', 'e', 'a', 'b', 'c']\n",
    "print(Counter(tokens))\n",
    "print(Counter(tokens).most_common(3))\n",
    "count = [('u', -1)]\n",
    "count.extend(Counter(tokens).most_common(3))\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "text = \"\"\"Randomness is the lack of pattern or predictability in events.\n",
    "A random sequence of events, symbols or steps has no order and does not \n",
    "follow an intelligible pattern or combination. Individual random events \n",
    "are by definition unpredictable, but in many cases the frequency of \n",
    "different outcomes over a large number of events (or \"trials\") is \n",
    "predictable. For example, when throwing two dice, the outcome of any \n",
    "particular roll is unpredictable, but a sum of 7 will occur twice as \n",
    "often as 4. In this view, randomness is a measure of uncertainty of an \n",
    "outcome, rather than haphazardness, and applies to concepts of chance, \n",
    "probability, and information entropy.\"\"\"\n",
    "\n",
    "tokens = text.split()\n",
    "count = [('UNK', -1)]\n",
    "count.extend(Counter(tokens).most_common(50))\n",
    "\n",
    "# Create dictionary and index dictionary\n",
    "dictionary = dict()\n",
    "index = 0\n",
    "for token, _ in count:\n",
    "    dictionary[token] = index\n",
    "    index += 1\n",
    "    \n",
    "index_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "\n",
    "# Replace each token in the dataset with its index in the dictionary\n",
    "index_tokens = [dictionary[token] if token in dictionary else 0 for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_tokens[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "context_window_size = 5\n",
    "for index, center in enumerate(index_tokens):\n",
    "    context = random.randint(1, context_window_size)\n",
    "    print(index-context, index)\n",
    "    for target in index_tokens[max(0, index - context): index]:\n",
    "        print(index_dictionary[center], index_dictionary[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
