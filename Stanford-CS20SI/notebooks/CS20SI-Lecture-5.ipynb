{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # Adjust verbosity to suppress information logs\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector # For visualizing embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradients**\n",
    "\n",
    "- Automatic differentiation\n",
    "- `tf.gradienets(y, [xs])` - Take derivative of `y` with respect to each tensor in the list `[xs]`\n",
    "\n",
    "---\n",
    "\n",
    "Example:\n",
    "\n",
    "$y = 2x^3$ and $z = 3 + y^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[768.0, 32.0]\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(initial_value=2.0)\n",
    "y = 2.0 * (x**3)\n",
    "z = 3.0 + y**2\n",
    "\n",
    "grad_z = tf.gradients(z, [x, y])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(x.initializer)\n",
    "    print(sess.run(grad_z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Structure Model**\n",
    "- Need models to be reusable - Object Oriented Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Managing Experiments**\n",
    "\n",
    "- **`tf.train.Saver()`**\n",
    "    - Saves the graph's variables in binary files\n",
    "    - Step at which graph variables are saved is called a checkpoint\n",
    "    - Periodically save the model's parameters after a certain number of steps and allows to restore/retrain model from last saved checkpoint\n",
    "    - `tf.train.Saver().save()` stores all variables of the graph by default\n",
    "    - `tf.train.Saver().save()` only save variables not the entire graph so need to create the graph and then load in variables\n",
    "\n",
    "`tf.train.Saver.save(sess, save_path, global_step, latest_filename=None, meta_graph_suffix='meta', write_meta_graph=True, write_state=True)`\n",
    "\n",
    "```python\n",
    "# Define model\n",
    "\n",
    "# Create a saver object\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Launch a session to compute ethe graph\n",
    "with tf.Session() as sess:\n",
    "    # Training loop\n",
    "    for step in range(training_steps):\n",
    "        sess.run([optimizer])\n",
    "        \n",
    "        if (step + 1) % 1000 == 0:\n",
    "            saver.save(sess, 'checkpoint_directory/model_name',\n",
    "                       global_step=model.global_step)\n",
    "\n",
    "```\n",
    "\n",
    "- **`Global Step`**\n",
    "    - During training many checkpoints will be created, so it is helpful to append number of training steps the model has gone through in a **variable** called `global_step`\n",
    "    -  After creating **variable** `global_step`, it is initialized to 0 and **set as to be not trainable**\n",
    "    - Pass `global_step` as a parameter to the optimizer so that optimizer increments `global_step` by 1 after each training step\n",
    "\n",
    "```python\n",
    "# Create global_step variable\n",
    "self.global_step  =  tf.Variable(0, dtype=tf.int32 ,trainable=False, name='global_step')\n",
    "\n",
    "# Pass global_step as a parameter to the optimizer\n",
    "self.optimizer = tf.train.GradientDescentOptimizer(self.lr).minimize(self.loss, global_step=self.global_step)\n",
    "\n",
    "# Save the session's variables in the directory `checkpoints` with name `model-name-global-step` (e.g. *skip-gram-1000*)\n",
    "saver.save(sess, 'checkpoint_directory/model_name', global_step=model.global_step)\n",
    "\n",
    "# Restore the session at 1000th step\n",
    "saver.restore(sess, 'checkpoints/skip-gram-1000')\n",
    "\n",
    "# # Passing what variables to store by passing them in a list or a dict\n",
    "v1 = tf.Variable(..., name='v1')\n",
    "v2 = tf.Variable(..., name='v2')\n",
    "\n",
    "# Pass the variables as a dict\n",
    "saver = tf.train.Saver({'v1': v1, 'v2': v2})\n",
    "\n",
    "# Pass the variables as a list\n",
    "saver = tf.train.Saver([v1, v2])\n",
    "\n",
    "# Passing a list is equivalent to passing a dict with the variable op names as keys\n",
    "saver = tf.train.Saver({v.op.name: v for v in [v1, v2]})\n",
    "```\n",
    "     \n",
    "- **`tf.train.get_checkpoint_state('checkpoint_directory_path')`**\n",
    "    - Allows to get checkpoint from a directory\n",
    "    \n",
    "```python\n",
    "# The file checkpoint automatically updates the path to the latest checkpoint\n",
    "checkpoint = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint'))\n",
    "if checkpoint and checkpoint.model_checkpoint_path:\n",
    "    saver.restore(sess, checkpoint.model_checkpoint_path)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`tf.summary`**\n",
    "    - Visualize summary statistics during training (via TensorBoard)\n",
    "    - Create a new `tf.name_scope(...)` to hold all the summary operations\n",
    "    - Because summary is an operation - Need to execute it with `sess.run()` to obtain it\n",
    "    - After obtaining summary - Need to write summary to file\n",
    "    \n",
    "```python\n",
    "def _create_summaries(self):\n",
    "    with tf.name_scope('summaries'):\n",
    "        tf.summary.scalar('loss', self.loss)\n",
    "        tf.summary.scalar('accuracy', self.accuracy)\n",
    "        tf.summary.histogram('histogram_loss', self.loss)\n",
    "        \n",
    "        # Merge several summaries into one operation (op) to make it easier to manage\n",
    "        self.summary_op = tf.summary.merge_all()\n",
    "        \n",
    "\n",
    "# To write `Summary` protocol buffers to event files\n",
    "writer = tf.summary.FileWriter('skip-gram-graph/lr' + str(self.learning_rate), sess.graph)\n",
    "            \n",
    "with tf.Session() as sess:\n",
    "    batch_loss, _, summary = sess.run([model.loss, model.optimizer, model.summary_op], feed_dict=feed_dict)\n",
    "    \n",
    "    # Write summary to file\n",
    "    writer.add_summary(summary, global_step=global_step)\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Control Randomization**\n",
    "- Set random seed at operation level. Each operation keeps its own seed\n",
    "- Set random seed at graph level with `tf.set_random_seed(seed)` (to be able to replicate result on another graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.57493\n",
      "-5.97319\n",
      "-0.02878\n",
      "\n",
      "3.57493\n",
      "3.57493\n",
      "\n",
      "3.57493\n",
      "9.13163\n"
     ]
    }
   ],
   "source": [
    "# Set random seed at operation level\n",
    "c = tf.random_uniform([], minval=-10, maxval=10, seed=2)\n",
    "d = tf.random_uniform([], minval=-10, maxval=10, seed=4)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(c))\n",
    "    print(sess.run(c))\n",
    "    print(sess.run(c))\n",
    "    print()\n",
    "    \n",
    "# Each new session restarts the random state \n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(c))\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(c))\n",
    "    \n",
    "print()\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(c))\n",
    "    print(sess.run(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.7782\n",
      "2.03784\n"
     ]
    }
   ],
   "source": [
    "# Set random seed at graph level\n",
    "tf.set_random_seed(3)\n",
    "\n",
    "c = tf.random_uniform([], minval=-10, maxval=10)\n",
    "d = tf.random_uniform([], minval=-10, maxval=10)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(c))\n",
    "    print(sess.run(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading Data in TensorFlow**\n",
    "- There are 2 main ways to load data into a TensorFlow graph\n",
    "    - Using `feed_dict` - It sends data from storage system to the client and then from clienet to the worker process. This will cause the data to slow down if the client is on a different machine from the worker process\n",
    "    - Using TensorFlow readers - Allows to load data directley into the worker process\n",
    "    \n",
    "- `tf.TextLineReader` - Outputs the lines of a file delimieted by new lines, e.g. text files, CSV files\n",
    "- `tf.FixedLenFeature` - Outputs the entire file when all files have same fixed lengts, e.g. each MNIST file has 28 x 28 pixels\n",
    "- `tf.WholeFileReader` - Outputs the entire file content\n",
    "- `tf.TFRecordReader` - Read samples from TensorFlow's own binary format (`TFRecord`)\n",
    "- `tf.ReaderBase` - Allows you to create your own readers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autodiff\n",
    "\n",
    "- TensorFlow uses *reverse mode automatic differentiation*: It allows one to take derivative of a function at roughly the same cost as computing the original function.\n",
    "- `tf.gradients(y, [x1, x2, ...])` - Take derivative of `y` with respect to each tensor in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[768.0, 32.0]\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(2.0)\n",
    "y = 2.0 * x**3\n",
    "z = 3.0 + y**2\n",
    "\n",
    "grad_z = tf.gradients(z, [x, y])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(x.initializer)\n",
    "    print(sess.run(grad_z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name Scope\n",
    "- If names are given tensors and ops: In TensorBoard the nodes are scattered all over the place and this makes graph difficult to read. TensorBoard does not know which nodes are similar to which nodes and should be grouped together.\n",
    "\n",
    "- Name scope - Creates namespace and **groups all ops related to a name together**\n",
    "\n",
    "### Variable Sharing\n",
    "\n",
    "- Variable scope - Creates namespace and **facilitate variable sharing**. It consists of 2 main functions:\n",
    "    - `tf.get_variable(<name>, <shape>, <initializer>)` - Creates or returns a variable with a given name\n",
    "    - `tf.variable_scope(<scope_name>)` - Manages namespaces for names passed to `tf.get_variable(...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.14\n",
      "1.618\n",
      "constant/pi\n",
      "constant/golden_ratio\n"
     ]
    }
   ],
   "source": [
    "# Name scope\n",
    "with tf.name_scope('constant'):\n",
    "    pi = tf.constant(3.14, name='pi')\n",
    "    golden_ratio = tf.constant(1.618, name='golden_ratio')\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(pi))\n",
    "    print(sess.run(golden_ratio))\n",
    "\n",
    "print(pi.op.name)\n",
    "print(golden_ratio.op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add_2:0\", shape=(200, 10), dtype=float32)\n",
      "Tensor(\"add_4:0\", shape=(200, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Need for Variable scope\n",
    "\n",
    "# We want to create a NN with 2 hidden layers. \n",
    "def two_hidden_layer_net(x):\n",
    "    assert x.shape.as_list() == [200, 100]\n",
    "    w1 = tf.Variable(tf.random_normal([100, 50]), name='h1_weights')\n",
    "    b1 = tf.Variable(tf.zeros([50]), name='h1_biases')\n",
    "    h1 = tf.matmul(x, w1) + b1\n",
    "    assert h1.shape.as_list() == [200, 50]\n",
    "    w2 = tf.Variable(tf.random_normal([50, 10]), name='h2_weights')\n",
    "    b2 = tf.Variable(tf.zeros([10]), name='h2_biases')\n",
    "    logits = tf.matmul(h1, w2) + b2\n",
    "    return logits\n",
    "\n",
    "# 2 different input x1 and x2\n",
    "x1 = tf.truncated_normal([200, 100], name='x1')\n",
    "x2 = tf.truncated_normal([200, 100], name='x2')\n",
    "\n",
    "# Call network on different inputs\n",
    "logits1 = two_hidden_layer_net(x1)\n",
    "logits2 = two_hidden_layer_net(x2)\n",
    "\n",
    "print(logits1)  # Variables: h1_weights, h1_biases, etc.\n",
    "print(logits2)  # Variables: h1_weights_1, h1_biases_1, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each time the network is called, TensorFlow creates a different set of variables. However, the network should share the same variables for all inputs. To share variables use `tf.get_variable()`\n",
    "\n",
    "- `tf.get_variable()` - First checks whether that variable exists. If it exists then reuse it, if not then create a new one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to create a NN with 2 hidden layers. \n",
    "def two_hidden_layer_net(x):\n",
    "    assert x.shape.as_list() == [200, 100]\n",
    "    w1 = tf.get_variable(name='h1_weights', shape=[100, 50], \n",
    "                         initializer=tf.random_normal_initializer())\n",
    "    b1 = tf.get_variable(name='h1_biases', shape=[50], \n",
    "                         initializer=tf.constant_initializer(0.0))\n",
    "    h1 = tf.matmul(x, w1) + b1\n",
    "    assert h1.shape.as_list() == [200, 50]\n",
    "    w2 = tf.get_variable(name='h2_weights', shape=[50, 10], \n",
    "                         initializer=tf.random_normal_initializer())\n",
    "    b2 = tf.get_variable(name='h2_biases', shape=[10], \n",
    "                         initializer=tf.constant_initializer(0.0))\n",
    "    logits = tf.matmul(h1, w2) + b2\n",
    "    return logits\n",
    "\n",
    "# 2 different input x1 and x2\n",
    "x1 = tf.truncated_normal([200, 100], name='x1')\n",
    "x2 = tf.truncated_normal([200, 100], name='x2')\n",
    "\n",
    "# Call network on different inputs\n",
    "#logits1 = two_hidden_layer_net(x1)\n",
    "#print(logits1)  \n",
    "\n",
    "#logits2 = two_hidden_layer_net(x2)\n",
    "#print(logits2)  \n",
    "\n",
    "# ValueError: Variable already exists\n",
    "# To avoid the above error, put all variables in a Variable Scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"two_layers/add_1:0\", shape=(200, 10), dtype=float32)\n",
      "Tensor(\"two_layers/add_3:0\", shape=(200, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Variable Scope\n",
    "\n",
    "with tf.variable_scope('two_layers', reuse=tf.AUTO_REUSE) as scope:\n",
    "    logits1 = two_hidden_layer_net(x1)\n",
    "    print(logits1)\n",
    "    # scope.reuse_variables() # Redundant if using reuse?\n",
    "    logits2 = two_hidden_layer_net(x2)\n",
    "    print(logits2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits1:  (200, 10)\n",
      "logits2:  (200, 10)\n"
     ]
    }
   ],
   "source": [
    "def fully_connected(x, output_dim, scope):\n",
    "    with tf.variable_scope(scope) as scope:\n",
    "        w = tf.get_variable('weights', shape=[x.shape[1], output_dim], \n",
    "                            initializer=tf.random_normal_initializer())\n",
    "        b = tf.get_variable('biases', shape=[output_dim], \n",
    "                            initializer=tf.constant_initializer(0.0))\n",
    "        return tf.matmul(x, w) + b\n",
    "    \n",
    "def two_hidden_layers_net(x):\n",
    "    h1 = fully_connected(x, output_dim=50, scope='h1')\n",
    "    h2 = fully_connected(h1, output_dim=10, scope='h2')\n",
    "    return h2\n",
    "\n",
    "# 2 different input x1 and x2\n",
    "x1 = tf.truncated_normal([200, 100], name='x1')\n",
    "x2 = tf.truncated_normal([200, 100], name='x2')\n",
    "\n",
    "## ValueError\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     logits1 = sess.run(two_hidden_layer_net(x1))\n",
    "#     logits2 = sess.run(two_hidden_layer_net(x2))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    with tf.variable_scope('two_layers', reuse=tf.AUTO_REUSE) as scope:\n",
    "        logits1 = sess.run(two_hidden_layer_net(x1))\n",
    "        print('logits1: ', logits1.shape)\n",
    "        logits2 = sess.run(two_hidden_layer_net(x2))\n",
    "        print('logits2: ', logits2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'foo/v:0' shape=(1,) dtype=float32_ref>\n",
      "<tf.Variable 'foo/v:0' shape=(1,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "def foo():\n",
    "    with tf.variable_scope(\"foo\", reuse=tf.AUTO_REUSE):\n",
    "        v = tf.get_variable(\"v\", [1])\n",
    "    return v\n",
    "\n",
    "v1 = foo()  # Creates v.\n",
    "v2 = foo()  # Gets the same, existing v.\n",
    "assert v1 == v2\n",
    "print(v1)\n",
    "print(v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Collections\n",
    "\n",
    "- `tf.get_collection` - Lets one access a certain collection of variables, with `key` being the name of the collection and `scope` is the scope of the variables. By default all variables are placed in `tf.GraphKeys.GLOBAL_VARIABLES`. *It can be used to freeze weights during transfer learning*\n",
    "- `tf.add_to_collection(name, value)` - Create a collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'graph/weights:0' shape=(10, 3) dtype=float32_ref>, <tf.Variable 'graph/biases:0' shape=(3,) dtype=float32_ref>]\n",
      "\n",
      "[<tf.Variable 'graph/weights:0' shape=(10, 3) dtype=float32_ref>, <tf.Variable 'graph/biases:0' shape=(3,) dtype=float32_ref>]\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('graph', reuse=tf.AUTO_REUSE) as scope:\n",
    "    w = tf.get_variable('weights', shape=[10, 3], \n",
    "                        initializer=tf.random_normal_initializer())\n",
    "    b = tf.get_variable('biases', shape=[3], \n",
    "                        initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "c = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='graph')\n",
    "print(c)\n",
    "print()\n",
    "\n",
    "c = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='graph')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing Experiments\n",
    "\n",
    "**Tools**\n",
    "\n",
    "- **`tf.train.Saver()`** - Saves the graph's variables in binary files. It Allows to periodically save the model's parameters after certain numbers of steps or epochs, and to restore/retrain models from some step. The step at which graph variables is called a `checkpoint`\n",
    "    - By default `Saver` stores all variables of the graph (recommended), however chosen variables can be stored by passing them in as a list or a dict when creating saver object, e.g. `saver = tf.train.Saver([w, b])`\n",
    "\n",
    "- **`global_step`** - During training many checkpoints are created, so it is helpful to append the number of training steps the model has gone through, it is done by creating a variable called `global_step`, initializing it to 0, and setting it to be not trainable. \n",
    "    - NOTE: Need to pass `global_step` as a parameter to the optimizer so it knows to increment `global_step` by 1 with each training step.\n",
    "    \n",
    "- **`tf.summary.FileWriter(directory, graph)`** - Provides a mechanism to create an event file in a given directory and add summaries and events to it.\n",
    "    \n",
    "- **`tf.summary`** - Collects summary statistics during training. Because it is an op, it needs to be executed with `sess.run()`. After obtaining summary, write the summary to file using `FileWriter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.7003365   0.63909185]\n",
      " [-1.662971    0.07174451]]\n"
     ]
    }
   ],
   "source": [
    "# Create a saver object\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Global step\n",
    "global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "with tf.variable_scope('weights', reuse=tf.AUTO_REUSE) as scope:\n",
    "    w = tf.get_variable('weights', shape=[2, 2], \n",
    "                        initializer=tf.random_normal_initializer())\n",
    "\n",
    "# Launch a session to execute computation and save variables\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    out = sess.run(w)\n",
    "    print(out)\n",
    "    \n",
    "    # Save the variable\n",
    "    saver.save(sess, 'checkpoints/saver', global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Control Randomization\n",
    "\n",
    "- Op level random seed\n",
    "    - Each new session restarts the random state\n",
    "    - Each Op keeps it own seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.2614121\n",
      "3.2668896\n",
      "\n",
      "-2.2614121\n"
     ]
    }
   ],
   "source": [
    "c = tf.random_uniform([], minval=-10, maxval=10, seed=3)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(c))\n",
    "    print(sess.run(c))\n",
    "    \n",
    "print()\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:  -2.2614121\n",
      "d:  -2.2614121\n"
     ]
    }
   ],
   "source": [
    "c = tf.random_uniform([], -10, 10, seed=3)\n",
    "d = tf.random_uniform([], -10, 10, seed=3)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # c and d will be same\n",
    "    print('c: ', sess.run(c)) \n",
    "    print('d: ', sess.run(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Graph level random seed\n",
    "    - Result is different from op-level seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:  -5.4093623\n",
      "d:  -9.190879\n"
     ]
    }
   ],
   "source": [
    "tf.set_random_seed(3)\n",
    "\n",
    "c = tf.random_uniform([], -10, 10)\n",
    "d = tf.random_uniform([], -10, 10)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # c and d will be different\n",
    "    print('c: ', sess.run(c))\n",
    "    print('d: ', sess.run(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
