{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structuring TensorFlow Models\n",
    "\n",
    "**Phase-I: Assemble Graph**\n",
    "1. Import data (`tf.data` or define `placeholders` for input and output)\n",
    "2. Define the `weights`\n",
    "3. Define the inference `model` (i.e. Forward path of the model)\n",
    "4. Define `cost` function\n",
    "5. Define `optimizer`\n",
    "\n",
    "**Phase-II: Execute Computation (i.e. Train Model)**\n",
    "1. Initialize all model variables for the first time\n",
    "2. Feed in the training data. Might involve randomizing the order of data samples\n",
    "3. Execute the inference `model` on the training data\n",
    "4. Compute the `loss`\n",
    "5. Adjust the model `weights` to minimize/maximize `loss` depending on the model\n",
    "\n",
    "### Structuring TensorFlow Models For Reuse\n",
    "\n",
    "**Reusable Models**: Using Object Oriented Programming to make models reusable\n",
    "\n",
    "- Define a `class` for the model\n",
    "- Set up model in a collection (e.g. map)\n",
    "\n",
    "> Reusing a model without rebuilding it: Big models that take a long time to build - save the `graph_def` in a file and then load it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Word Embedding`**\n",
    "- Captures the semantic relationships between words\n",
    "\n",
    "**Word2Vec: skip-gram**\n",
    "- Softmax is computationally expensive (because of the size of word vocabulary)\n",
    "- Negative sampling (a simplified version of `Noise Contrastive Estimation (NCE)`). NCE guarantes approximation to softmax, where as Negative sampling does not approximate to softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # Adjust verbosity to suppress information logs\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector # For visualizing embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Pre-Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "import zipfile\n",
    "import random\n",
    "\n",
    "from six.moves import urllib\n",
    "from collections import Counter\n",
    "\n",
    "# Utility functions\n",
    "def make_dir(path):\n",
    "    \"\"\"Create a directory if directory does not exist.\"\"\"\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError:\n",
    "        pass\n",
    "    \n",
    "def download_one_file(download_url, local_dest, expected_byte=None, unzip_and_remove=False):\n",
    "    \"\"\" \n",
    "    Download the file from download_url into local_dest, if the file doesn't already exists.\n",
    "    If expected_byte is provided, check if the downloaded file has the same number of bytes.\n",
    "    If unzip_and_remove is True, unzip the file and remove the zip file\n",
    "    \"\"\"\n",
    "    if os.path.exists(local_dest) or os.path.exists(local_dest[:-3]):\n",
    "        print('%s already exists' % local_dest)\n",
    "    else:\n",
    "        print('Downloading %s' % download_url)\n",
    "        local_file, _ = urllib.request.urlretrieve(download_url, local_dest)\n",
    "        file_stat = os.stat(local_dest)\n",
    "        if expected_byte:\n",
    "            if file_stat.st_size == expected_byte:\n",
    "                print('Successfully downloaded %s' % local_dest)\n",
    "                if unzip_and_remove:\n",
    "                    with gzip.open(local_dest, 'rb') as f_in, open(local_dest[:-3],'wb') as f_out:\n",
    "                        shutil.copyfileobj(f_in, f_out)\n",
    "                    os.remove(local_dest)\n",
    "            else:\n",
    "                print('The downloaded file has unexpected number of bytes')\n",
    "\n",
    "def read_data(file_path):\n",
    "    \"\"\"Read data into a list of tokens. There should be 17,005,207 tokens.\"\"\"\n",
    "    with zipfile.ZipFile(file_path) as f:\n",
    "        # Convert input to string using Python 2 vs 3 compatibility as_str()\n",
    "        tokens = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    return tokens\n",
    "\n",
    "def build_vocab(tokens, vocab_size, visual_fld):\n",
    "    \"\"\"Build vocabulary of VOCAB_SIZE most frequent tokens and write\n",
    "    it to visualization/vocab.tsv\n",
    "    \"\"\"\n",
    "    # Create vocabulary\n",
    "    dictionary = dict()\n",
    "    count = [('UNK', -1)] # For Unknown words\n",
    "    count.extend(Counter(tokens).most_common(vocab_size - 1)) # Extend the list (ensures that UNK is 0th)\n",
    "    index = 0\n",
    "    \n",
    "    # Create directory to store vocab.tsv\n",
    "    make_dir(visual_fld)\n",
    "    with open(os.path.join(visual_fld, 'vocab.tsv'), \"w\") as f:\n",
    "        for token, _ in count:\n",
    "            dictionary[token] = index\n",
    "            index += 1\n",
    "            f.write(token + '\\n')\n",
    "            \n",
    "    index_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return dictionary, index_dictionary\n",
    "\n",
    "def convert_tokens_to_index(tokens, dictionary):\n",
    "    \"\"\"Replace each token in the dataset with its index in the dictionary.\"\"\"\n",
    "    return [dictionary[token] if token in dictionary else 0 for token in tokens]\n",
    "\n",
    "def generate_sample(index_tokens, context_window_size):\n",
    "    \"\"\"Form training pairs according to the skip-gram model.\"\"\"\n",
    "    for index, center in enumerate(index_tokens):\n",
    "        context = random.randint(1, context_window_size)\n",
    "        # Get a random target token before the center token\n",
    "        for target in index_tokens[max(0, index - context): index]:\n",
    "            yield center, target\n",
    "        # Get a random target token after the center token\n",
    "        for target in index_tokens[index + 1: index + context + 1]:\n",
    "            yield center, target\n",
    "            \n",
    "def most_common_tokens(visual_fld, num_visualize):\n",
    "    \"\"\"Create a list of num_visualize most frequent words to visualize\n",
    "    on Tenserboard. The list is saved to visualization/vocab_[num_visualize].tsv\"\"\"\n",
    "    vocab = open(os.path.join(visual_fld, 'vocab.tsv'), 'r').readlines()[:num_visualize]\n",
    "    tokens = [token for token in vocab]\n",
    "    path = os.path.join(visual_fld, 'vocab_' + str(num_visualize) + '.tsv')\n",
    "    with open(path, 'w') as f:\n",
    "        for token in tokens:\n",
    "            f.write(token)\n",
    "\n",
    "def batch_gen(download_url, expected_byte, vocab_size, batch_size, skip_window, visual_fld):\n",
    "    local_dest = './data/text8.zip'\n",
    "    download_one_file(download_url, local_dest, expected_byte)\n",
    "    tokens = read_data(local_dest)\n",
    "    dictionary, _ = build_vocab(tokens, vocab_size, visual_fld)\n",
    "    index_tokens = convert_tokens_to_index(tokens, dictionary)\n",
    "    del tokens  # To save memory\n",
    "    single_gen = generate_sample(index_tokens, skip_window)\n",
    "    \n",
    "    while True:\n",
    "        center_batch = np.zeros(batch_size, dtype=np.int32)\n",
    "        target_batch = np.zeros([batch_size, 1])\n",
    "        for index in range(batch_size):\n",
    "            center_batch[index], target_batch[index] = next(single_gen)\n",
    "        yield center_batch, target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for downloading data\n",
    "DOWNLOAD_URL = 'http://mattmahoney.net/dc/text8.zip'\n",
    "EXPECTED_BYTES = 31344016\n",
    "\n",
    "# Model Hyper-parameters\n",
    "VOCAB_SIZE = 50000\n",
    "BATCH_SIZE = 64\n",
    "EMBED_SIZE = 300  # Dimension of the word embedding vectors\n",
    "SKIP_WINDOW = 1  # Context window size\n",
    "NUM_SAMPLED = 25  # Number of negative examples to sample\n",
    "LEARNING_RATE = 1.0\n",
    "NUM_TRAIN_STEPS = 20000\n",
    "SKIP_STEP = 2000  # Steps to skip before reporting the loss\n",
    "VISUAL_FLD = 'visualization'  # Directory name\n",
    "NUM_VISUALIZE = 1000  # number of tokens to visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Skip-Gram (Reusable)** - Build model as a `class`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "class SkipGramModel:\n",
    "    \"\"\"\n",
    "    Build the graph for Word2Vec (skip-gram) model.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, vocab_size, embed_size, batch_size, num_sampled, learning_rate):\n",
    "        self.dataset = dataset\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_sampled = num_sampled\n",
    "        self.learning_rate = learning_rate\n",
    "        self.skip_step = SKIP_STEP\n",
    "        self.global_step = tf.get_variable('global_step', initializer=tf.constant(0), \n",
    "                                           trainable=False)\n",
    "    \n",
    "    def _import_data(self):\n",
    "        \"\"\"\n",
    "        Step 1: Import data\n",
    "        \"\"\"\n",
    "        self.iterator = self.dataset.make_initializable_iterator()\n",
    "        self.center_words, self.target_words = self.iterator.get_next()\n",
    "    \n",
    "    def _create_embedding(self):\n",
    "        \"\"\"\n",
    "        Step 2: Embeddings (i.e. weights)\n",
    "        \"\"\"\n",
    "        with tf.name_scope('embeddings'):\n",
    "            self.embed_matrix = tf.get_variable('embed_matrix', \n",
    "                                                shape=[self.vocab_size, self.embed_size],\n",
    "                                                initializer=tf.random_uniform_initializer())\n",
    "            \n",
    "            self.embed = tf.nn.embedding_lookup(self.embed_matrix, self.center_words, \n",
    "                                                name='embedding')\n",
    "    \n",
    "    def _create_loss(self):\n",
    "        \"\"\"\n",
    "        Step 3 + 4: Define the inference + the loss function\n",
    "        \"\"\"\n",
    "        with tf.name_scope('loss'):\n",
    "            # Create variables for NCE loss\n",
    "            nce_weight = tf.get_variable(name='nce_weight', \n",
    "                                         shape=[self.vocab_size, self.embed_size],\n",
    "                                         initializer=tf.truncated_normal_initializer(\n",
    "                                             stddev=1.0/self.embed_size ** 0.5))\n",
    "            \n",
    "            nce_bias = tf.get_variable(name='nce_bias', initializer=tf.zeros([self.vocab_size]))\n",
    "            \n",
    "            self.loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weight, \n",
    "                                                      biases=nce_bias, \n",
    "                                                      labels=self.target_words, \n",
    "                                                      inputs=self.embed, \n",
    "                                                      num_sampled=self.num_sampled, \n",
    "                                                      num_classes=self.vocab_size), name='loss')\n",
    "            \n",
    "    \n",
    "    def _create_optimizer(self):\n",
    "        \"\"\"\n",
    "        Step 5: Define optimizer\n",
    "        \"\"\"\n",
    "        self.optimozer = tf.train.GradientDescentOptimizer(\n",
    "            self.learning_rate).minimize(self.loss, global_step=self.global_step)\n",
    "    \n",
    "    def _create_summaries(self):\n",
    "        \"\"\"\n",
    "        For visualization\n",
    "        \"\"\"\n",
    "        with tf.name_scope(name='summaries'):\n",
    "            tf.summary.scalar(name='loss', tensor=self.loss)\n",
    "            tf.summary.histogram(name='histogram_loss', values=self.loss)\n",
    "            \n",
    "            # Because there are several summaries: Merge them all into one\n",
    "            # op to make it easier to manage\n",
    "            self.summary_op = tf.summary.merge_all()\n",
    "        \n",
    "    def build_graph(self):\n",
    "        \"\"\"\n",
    "        Build graph for the skip-gram model\n",
    "        \"\"\"\n",
    "        self._import_data()\n",
    "        self._create_embedding()\n",
    "        self._create_loss()\n",
    "        self._create_optimizer()\n",
    "        self._create_summaries()\n",
    "    \n",
    "    def train(self, num_train_steps):\n",
    "        \"\"\"\n",
    "        Training loop\n",
    "        \"\"\"\n",
    "        saver = tf.train.Saver()  # Defaults to saving all variables: embed_matrix, nce_weight, nce_bias\n",
    "        \n",
    "        make_dir('checkpoints')\n",
    "        \n",
    "        with tf.Session() as sess:  # How would this work in dmed case?\n",
    "            sess.run(self.iterator.initializer)  # Why?\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint'))\n",
    "            \n",
    "            # If checkpoint exists then restore from checkpoint\n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "                saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "                \n",
    "            total_loss = 0.0  # For calculating average los in last SKIP_STEP steps\n",
    "            writer = tf.summary.FileWriter('graphs/word2vec/lr' + str(self.learning_rate), sess.graph)\n",
    "            initial_step = self.global_step.eval()  # What is this for?\n",
    "            \n",
    "            for index in range(initial_step, initial_step + num_train_steps):\n",
    "                try:\n",
    "                    batch_loss, _, summary = sess.run([self.loss, self.optimizer, self.summary_op])\n",
    "                    writer.add_summary(summary, global_step=index)\n",
    "                    total_loss += batch_loss\n",
    "                    if (index + 1) % self.skip_step == 0:\n",
    "                        print('Average loss at step {}:{:5.1f}'.format(index,\n",
    "                                                                       total_loss/self.skip_step))\n",
    "                        \n",
    "                        total_loss = 0.0\n",
    "                        saver.save(sess, 'checkpoints/skip-gram', index)\n",
    "                        \n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    sess.run(self.iterator.initializer)\n",
    "            writer.close()\n",
    "            \n",
    "    def visualize(self):\n",
    "        \"\"\"\n",
    "        Run `tensorboard --logdir='visualization'` to see the embeddings\n",
    "        \"\"\"\n",
    "        # Create a list of most common num_visualize words to visualize\n",
    "        most_common_tokens(visual_fld, num_visualize)\n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint'))\n",
    "            \n",
    "            # If checkpoint exists then restore from checkpoint\n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "                saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "                \n",
    "            final_embed_matrix = sess.run(self.embed_matrix)\n",
    "            \n",
    "            # Need to store embeddings in a new variable\n",
    "            embedding_var = tf.Variable(final_embed_matrix[:num_visualize], name='embedding')\n",
    "            sess.run(embedding_var.initializer)\n",
    "            \n",
    "            config = projector.ProjectorConfig()\n",
    "            summary_writer = tf.summary.FileWriter(visual_fld)\n",
    "            \n",
    "            # Add embedding to the config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "VOCAB_SIZE = 50000\n",
    "BATCH_SIZE = 64\n",
    "EMBED_SIZE = 300  # Dimension of the word embedding vectors\n",
    "SKIP_WINDOW = 1  # Context window size\n",
    "NUM_SAMPLED = 25  # Number of negative examples to sample\n",
    "LEARNING_RATE = 1.0\n",
    "NUM_TRAIN_STEPS = 20000\n",
    "SKIP_STEP = 2000  # Steps to skip before reporting the loss\n",
    "\n",
    "# Word2Vec Graph for Skip-Gram\n",
    "def word2vec(batch_gen):\n",
    "    \"\"\"Build the graph for word2vec model and train it.\"\"\"\n",
    "    # Step 1: define the placeholders for input and output\n",
    "    # center_words have to be int to work on embedding lookup\n",
    "\n",
    "    # TODO\n",
    "    # Instead of using one-hot vectors, input the index of those words directly. \n",
    "    # For example, if the center word is the 1001th word in the vocabulary, input the number 1001.\n",
    "    center_words = tf.placeholder(tf.int32, shape=[BATCH_SIZE])\n",
    "    target_words = tf.placeholder(tf.int32, shape=[BATCH_SIZE, 1]) # [BATCH_SIZE] -> ValueError: Shape must be rank 2 but is rank 1\n",
    "\n",
    "    # Step 2: define weights. In word2vec, it's actually the weights that we care about\n",
    "    # vocab size x embed size\n",
    "    # initialized to random uniform -1 to 1\n",
    "\n",
    "    # TODO\n",
    "    # If one word is represented with a vector of size EMBED_SIZE, then the embedding matrix will \n",
    "    # have shape [VOCAB_SIZE, EMBED_SIZE]\n",
    "    embed_matrix = tf.Variable(initial_value=tf.random_uniform([VOCAB_SIZE, EMBED_SIZE], minval=-1, maxval=1))\n",
    "\n",
    "\n",
    "    # Step 3: define the inference\n",
    "    # get the embed of input words using tf.nn.embedding_lookup\n",
    "    # embed = tf.nn.embedding_lookup(embed_matrix, center_words, name='embed')\n",
    "\n",
    "    # TODO\n",
    "    # embed_matrix has dimension VOCAB_SIZE x EMBED_SIZE, with each row of the embedding matrix \n",
    "    # corresponds to the vector representation of the word at that index\n",
    "    embed = tf.nn.embedding_lookup(embed_matrix, center_words, name='embed')\n",
    "\n",
    "\n",
    "    # Step 4: construct variables for NCE loss\n",
    "    # tf.nn.nce_loss(weights, biases, labels, inputs, num_sampled, num_classes, ...)\n",
    "    # nce_weight (vocab size x embed size), intialized to truncated_normal stddev=1.0 / (EMBED_SIZE ** 0.5)\n",
    "    # bias: vocab size, initialized to 0\n",
    "\n",
    "    # TODO\n",
    "    nce_weight = tf.Variable(tf.truncated_normal([VOCAB_SIZE, EMBED_SIZE], stddev=1.0/(EMBED_SIZE**0.5)), \n",
    "                             name='nce_weight')\n",
    "    nce_bias = tf.Variable(tf.zeros([VOCAB_SIZE]), name='nce_bias')\n",
    "\n",
    "    # define loss function to be NCE loss function\n",
    "    # tf.nn.nce_loss(weights, biases, labels, inputs, num_sampled, num_classes, ...)\n",
    "    # need to get the mean accross the batch\n",
    "    # note: you should use embedding of center words for inputs, not center words themselves\n",
    "\n",
    "    # TODO\n",
    "    nce_loss = tf.nn.nce_loss(weights=nce_weight, biases=nce_bias, labels=target_words, inputs=embed, \n",
    "                              num_sampled=NUM_SAMPLED, num_classes=VOCAB_SIZE)\n",
    "    # Mean accross the batch\n",
    "    loss = tf.reduce_mean(nce_loss)\n",
    "\n",
    "        \n",
    "    # Step 5: define optimizer\n",
    "    \n",
    "    # TODO\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=LEARNING_RATE).minimize(loss)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # TODO: initialize variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        total_loss = 0.0 # To calculate the average loss in the last SKIP_STEP steps\n",
    "        writer = tf.summary.FileWriter('./graphs/skip-gram/', sess.graph)\n",
    "        \n",
    "        for index in range(NUM_TRAIN_STEPS):\n",
    "            \n",
    "            # Get batch data\n",
    "            centers, targets = batch_gen.next()\n",
    "            \n",
    "            # TO DO: create feed_dict, run optimizer, fetch loss_batch\n",
    "            _, loss_batch = sess.run([optimizer, loss], feed_dict={center_words: centers, target_words: targets})\n",
    "\n",
    "            total_loss += loss_batch\n",
    "            if (index + 1) % SKIP_STEP == 0:\n",
    "                print('Average loss at step {}: {:5.1f}'.format(index, total_loss/SKIP_STEP))\n",
    "                total_loss = 0.0\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset is ready.\n",
      "Average loss at step 1999: 140.7\n",
      "Average loss at step 3999:  53.4\n",
      "Average loss at step 5999:  31.1\n",
      "Average loss at step 7999:  21.8\n",
      "Average loss at step 9999:  15.2\n",
      "Average loss at step 11999:  14.4\n",
      "Average loss at step 13999:  11.6\n",
      "Average loss at step 15999:  10.5\n",
      "Average loss at step 17999:  10.3\n",
      "Average loss at step 19999:   9.0\n"
     ]
    }
   ],
   "source": [
    "# Get text data and pre-process it\n",
    "batch_gen = process_data(VOCAB_SIZE, BATCH_SIZE, SKIP_WINDOW)\n",
    "\n",
    "# Train\n",
    "word2vec(batch_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name Scope\n",
    "- For grouping nodes together\n",
    "```python\n",
    "with tf.name_scope('name_of_scope'):\n",
    "    # Declare Operation-1\n",
    "    # Declare Operation-2\n",
    "    # ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Skip Gram Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Course examples\n",
    "def train_model(model, batch_gen, num_train_steps, weights_fld):\n",
    "    saver = tf.train.Saver() # defaults to saving all variables - in this case embed_matrix, nce_weight, nce_bias\n",
    "\n",
    "    initial_step = 0\n",
    "    make_dir('checkpoints')\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Get CheckpointState proto from the \"checkpoint\" file.\n",
    "        ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint'))\n",
    "        \n",
    "        # if that checkpoint exists, restore from checkpoint\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "        total_loss = 0.0 # we use this to calculate late average loss in the last SKIP_STEP steps\n",
    "        writer = tf.summary.FileWriter('improved_graph/lr' + str(LEARNING_RATE), sess.graph)\n",
    "        initial_step = model.global_step.eval()\n",
    "        for index in range(initial_step, initial_step + num_train_steps):\n",
    "            centers, targets = batch_gen.next()\n",
    "            feed_dict={model.center_words: centers, model.target_words: targets}\n",
    "            loss_batch, _, summary = sess.run([model.loss, model.optimizer, model.summary_op], \n",
    "                                              feed_dict=feed_dict)\n",
    "            writer.add_summary(summary, global_step=index)\n",
    "            total_loss += loss_batch\n",
    "            if (index + 1) % SKIP_STEP == 0:\n",
    "                print('Average loss at step {}: {:5.1f}'.format(index, total_loss / SKIP_STEP))\n",
    "                total_loss = 0.0\n",
    "                saver.save(sess, 'checkpoints/skip-gram', index)\n",
    "        \n",
    "        ####################\n",
    "        # code to visualize the embeddings. uncomment the below to visualize embeddings\n",
    "        # run \"'tensorboard --logdir='processed'\" to see the embeddings\n",
    "        # final_embed_matrix = sess.run(model.embed_matrix)\n",
    "        \n",
    "        # # it has to variable. constants don't work here. you can't reuse model.embed_matrix\n",
    "        # embedding_var = tf.Variable(final_embed_matrix[:1000], name='embedding')\n",
    "        # sess.run(embedding_var.initializer)\n",
    "\n",
    "        # config = projector.ProjectorConfig()\n",
    "        # summary_writer = tf.summary.FileWriter('processed')\n",
    "\n",
    "        # # add embedding to the config file\n",
    "        # embedding = config.embeddings.add()\n",
    "        # embedding.tensor_name = embedding_var.name\n",
    "        \n",
    "        # # link this tensor to its metadata file, in this case the first 500 words of vocab\n",
    "        # embedding.metadata_path = 'processed/vocab_1000.tsv'\n",
    "\n",
    "        # # saves a configuration file that TensorBoard will read during startup.\n",
    "        # projector.visualize_embeddings(summary_writer, config)\n",
    "        # saver_embed = tf.train.Saver([embedding_var])\n",
    "        # saver_embed.save(sess, 'processed/model3.ckpt', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train using function\n",
    "tf.reset_default_graph()\n",
    "skg = SkipGramModel(VOCAB_SIZE, BATCH_SIZE, EMBED_SIZE, NUM_SAMPLED, LEARNING_RATE)\n",
    "skg.build_graph()\n",
    "train_model(skg, batch_gen, NUM_TRAIN_STEPS, WEIGHTS_FLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModel():\n",
    "    \"\"\"\n",
    "    Build the graph for Word2Vec model.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, batch_size, embed_size, num_sampled, learning_rate):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.batch_size = batch_size\n",
    "        self.embed_size = embed_size\n",
    "        self.num_sampled = num_sampled\n",
    "        self.learning_rate = learning_rate\n",
    "        self.global_step = tf.Variable(initial_value=0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "        \n",
    "    \n",
    "    def _create_placeholder(self):\n",
    "        \"\"\"Step 1: define placeholders for input and output\"\"\"\n",
    "        with tf.name_scope('data'):\n",
    "            self.center_words = tf.placeholder(tf.int32, shape=[self.batch_size])\n",
    "            self.target_words = tf.placeholder(tf.int32, shape=[self.batch_size, 1])\n",
    "    \n",
    "    def _create_embeddings(self):\n",
    "        \"\"\"Step 2: define weights, (vocab size x embed size)\"\"\"\n",
    "        with tf.device('/cpu:0'):\n",
    "            with tf.name_scope('embeddings'):\n",
    "                self.embed_matrix = tf.Variable(initial_value=tf.random_uniform([self.vocab_size, self.embed_size], \n",
    "                                                                                minval=-1, maxval=1), name='embed_matrix')\n",
    "    \n",
    "    def _create_loss(self):\n",
    "        \"\"\"Step 3 + 4: define the inference + the loss function\"\"\"\n",
    "        with tf.name_scope('loss'):\n",
    "            # Step 3: define the inference\n",
    "            self.embed = tf.nn.embedding_lookup(self.embed_matrix, self.center_words, name='embed')\n",
    "            \n",
    "            # Step 4: construct variables for NCE loss\n",
    "            self.nce_weight = tf.Variable(tf.truncated_normal([self.vocab_size, self.embed_size], \n",
    "                                                              stddev=1.0/(self.embed_size**0.5)), name='nce_weight')\n",
    "        \n",
    "            self.nce_bias = tf.Variable(tf.zeros([self.vocab_size]), name='nce_bias')\n",
    "            \n",
    "            self.nce_loss = tf.nn.nce_loss(weights=self.nce_weight, biases=self.nce_bias, labels=self.target_words, \n",
    "                                           inputs=self.embed, num_sampled=self.num_sampled, num_classes=self.vocab_size)\n",
    "            # Mean accross the batch\n",
    "            self.loss = tf.reduce_mean(self.nce_loss)\n",
    "            \n",
    "    \n",
    "    def _create_optimizer(self):\n",
    "        \"\"\"Step 5: define optimizer\"\"\"\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate).minimize(self.loss, \n",
    "                                                                                                      global_step=self.global_step)\n",
    "        \n",
    "    def _create_summaries(self):\n",
    "        with tf.name_scope(\"summaries\"):\n",
    "            tf.summary.scalar(\"loss\", self.loss)\n",
    "            tf.summary.histogram(\"histogram_loss\", self.loss)\n",
    "            \n",
    "            # Because there are 2 summaries - merge summaries into one op to make it easier to manage\n",
    "            self.summary_op = tf.summary.merge_all()\n",
    "        \n",
    "    def build_graph(self):\n",
    "        self._create_placeholder()\n",
    "        self._create_embeddings()\n",
    "        self._create_loss()\n",
    "        self._create_optimizer()\n",
    "        self._create_summaries()\n",
    "    \n",
    "#     def train(self, batch_gen, num_train_steps, skip_step, weights_fld):\n",
    "        \n",
    "#         # Instantiate class\n",
    "#         sgm = self.__class__(self.vocab_size, self.batch_size, self.embed_size, self.num_sampled, \n",
    "#                              self.learning_rate)\n",
    "        \n",
    "#         # Build graph\n",
    "#         sgm.build_graph()\n",
    "        \n",
    "#         # Object to save and restore variables\n",
    "#         saver = tf.train.Saver() # defaults to saving all variables - embed_matrix, nce_weight, nce_bias\n",
    "        \n",
    "#         # Create directory for checkpoints\n",
    "#         make_dir('checkpoints')\n",
    "        \n",
    "#         with tf.Session() as sess:\n",
    "            \n",
    "#             # Initialize variables\n",
    "#             sess.run(tf.global_variables_initializer())\n",
    "                        \n",
    "#             # Get CheckpointState proto from the \"checkpoint\" file.\n",
    "#             ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint'))\n",
    "            \n",
    "#             # If checkpoint exists then restroe from checkpoint\n",
    "#             if ckpt and ckpt.model_checkpoint_path:\n",
    "#                 saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "                \n",
    "#             total_loss = 0.0 # To calculate late average loss in the last SKIP_STEP steps\n",
    "            \n",
    "#             # To write `Summary` protocol buffers to event files\n",
    "#             writer = tf.summary.FileWriter('improved_graph/lr' + str(self.learning_rate), sess.graph)\n",
    "            \n",
    "#             initial_step = sgm.global_step.eval() # Why?\n",
    "            \n",
    "#             for index in range(initial_step, initial_step + num_train_steps):\n",
    "#                 centers, targets = batch_gen.next()\n",
    "#                 feed_dict = {sgm.center_words: centers, sgm.target_words: targets}\n",
    "#                 batch_loss, _, summary = sess.run([sgm.loss, sgm.optimizer, sgm.summary_op],\n",
    "#                                                   feed_dict=feed_dict)\n",
    "                \n",
    "#                 writer.add_summary(summary, global_step=index)\n",
    "#                 total_loss += batch_loss\n",
    "#                 if (index + 1) % skip_step == 0:\n",
    "#                     print('Average loss at step {}: {:5.1f}'.format(index, total_loss / skip_step))\n",
    "#                 total_loss = 0.0\n",
    "#                 saver.save(sess, 'checkpoints/skip-gram', index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train using SkipGramModel method\n",
    "tf.reset_default_graph()\n",
    "skg = SkipGramModel(VOCAB_SIZE, BATCH_SIZE, EMBED_SIZE, NUM_SAMPLED, LEARNING_RATE)\n",
    "skg.train(batch_gen, NUM_TRAIN_STEPS, SKIP_STEP, WEIGHTS_FLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Step\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(skg.global_step.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'a', 'b', 'c', 'd', 'e', 'a', 'b', 'c']\n",
    "print(Counter(tokens))\n",
    "print(Counter(tokens).most_common(3))\n",
    "count = [('u', -1)]\n",
    "count.extend(Counter(tokens).most_common(3))\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "text = \"\"\"Randomness is the lack of pattern or predictability in events.\n",
    "A random sequence of events, symbols or steps has no order and does not \n",
    "follow an intelligible pattern or combination. Individual random events \n",
    "are by definition unpredictable, but in many cases the frequency of \n",
    "different outcomes over a large number of events (or \"trials\") is \n",
    "predictable. For example, when throwing two dice, the outcome of any \n",
    "particular roll is unpredictable, but a sum of 7 will occur twice as \n",
    "often as 4. In this view, randomness is a measure of uncertainty of an \n",
    "outcome, rather than haphazardness, and applies to concepts of chance, \n",
    "probability, and information entropy.\"\"\"\n",
    "\n",
    "tokens = text.split()\n",
    "count = [('UNK', -1)]\n",
    "count.extend(Counter(tokens).most_common(50))\n",
    "\n",
    "# Create dictionary and index dictionary\n",
    "dictionary = dict()\n",
    "index = 0\n",
    "for token, _ in count:\n",
    "    dictionary[token] = index\n",
    "    index += 1\n",
    "    \n",
    "index_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "\n",
    "# Replace each token in the dataset with its index in the dictionary\n",
    "index_tokens = [dictionary[token] if token in dictionary else 0 for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_tokens[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "context_window_size = 5\n",
    "for index, center in enumerate(index_tokens):\n",
    "    context = random.randint(1, context_window_size)\n",
    "    print(index-context, index)\n",
    "    for target in index_tokens[max(0, index - context): index]:\n",
    "        print(index_dictionary[center], index_dictionary[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
