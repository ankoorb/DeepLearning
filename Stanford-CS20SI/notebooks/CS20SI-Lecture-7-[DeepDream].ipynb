{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Ascent\n",
    "\n",
    "Generate a synthetic image that maximally activates a neuron.\n",
    "\n",
    "$$\\mathbf{I^{*}} = arg max_{I} f(I) + R(I)$$\n",
    "\n",
    "Where, $f(I)$ is neuron value and $R(I)$ is natural image regularizer\n",
    "\n",
    "## Deep Dream\n",
    "\n",
    "Making the \"dream\" images is very simple. Essentially it is just a gradient ascent process that tries to maximize the L2 norm of activations of a particular CNN layer. The optimization resembles Backpropagation, however instead of adjusting the network weights, the weights are held fixed and the input is adjusted. Idea: Use gradient ascent to optimize an image so it maximizes the mean value of the given layer tensor.Here are a few simple tricks that were found useful for getting good images:\n",
    "\n",
    "- Offset image by a random jitter\n",
    "- Normalize the magnitude of gradient ascent steps\n",
    "- Apply ascent across multiple scales (octaves)\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "Calculate the gradient of a given layer of the network with respect to input image. The gradient is then added to the input image so the mean value of the layer tensor is increased. This process is repeated a number of times and amplifies whatever patterns the Inception model sees in the input image. \n",
    "\n",
    "Google Implementation: Implemented Gradient Ascent through different scales, these scales were called as \"octaves\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import PIL.Image\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.app.flags` - Google uses this setting global data for parsing arguments from the commandline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "tf.app.flags.DEFINE_string('inception', './data/inception', \n",
    "                           help='Directory for storing Inception Network.')\n",
    "\n",
    "tf.app.flags.DEFINE_string('jpeg', 'deep-dream.jpg', \n",
    "                           help='Where to save the resulting JPEG.')\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#   tf.app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer(layer):\n",
    "    \"\"\"\n",
    "    Helper for getting layer output (Tensor) in model Graph.\n",
    "    \n",
    "    Args:\n",
    "        layer: str, layer name\n",
    "    \n",
    "    Returns:\n",
    "        Tensor for the given layer name\n",
    "    \"\"\"\n",
    "    graph = tf.get_default_graph()\n",
    "    return graph.get_tensor_by_name('import/%s:0' % layer)\n",
    "\n",
    "def download_network(dir_path):\n",
    "    \"\"\"\n",
    "    Maybe download pretrained Inception Network.\n",
    "    \n",
    "    Args:\n",
    "      dir_path: str, directory path to save data.\n",
    "    \"\"\"\n",
    "    url = 'https://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip'\n",
    "    basename = 'inception5h.zip'\n",
    "    local_file = tf.contrib.learn.datasets.base.maybe_download(basename, dir_path, url)\n",
    "    \n",
    "    # Uncompress the pretrained Inception Network\n",
    "    print('Extracting: Inception Network')\n",
    "    zip_ref = zipfile.ZipFile(local_file, 'r')\n",
    "    zip_ref.extractall(dir_path)\n",
    "    zip_ref.close()\n",
    "    \n",
    "def normalize_image(image):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      image: numpy array\n",
    "    \"\"\"\n",
    "    # Clip to [0, 1] and then convert to uint8\n",
    "    image = np.clip(image, 0, 1)\n",
    "    image = np.uint8(image * 255)\n",
    "    return image\n",
    "\n",
    "def save_jpeg(jpeg_file, image):\n",
    "    pil_image = PIL.Image.fromarray(image)\n",
    "    pil_image.save(jpeg_file)\n",
    "    print('Saved to file: ', jpeg_file)\n",
    "    \n",
    "def show_image(a):\n",
    "    a = np.uint8(np.clip(a, 0, 255))\n",
    "    PIL.Image.fromarray(a).save(f, fmt)\n",
    "    display(Image(data=f.getvalue()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-39280c993062>:23: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "Extracting: Inception Network\n"
     ]
    }
   ],
   "source": [
    "# Download inception\n",
    "download_network('./data/inception')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Conv layers:  59\n",
      "Number of feature channels:  7548\n",
      "Number of channels in \"mixed4d_3x3_bottleneck_pre_relu\": 144\n",
      "At step 0: objective value: -12.84211540222168\n",
      "At step 1: objective value: -29.949438095092773\n",
      "At step 2: objective value: 11.467972755432129\n",
      "At step 3: objective value: 74.13787841796875\n",
      "At step 4: objective value: 137.16217041015625\n",
      "At step 5: objective value: 192.8675537109375\n",
      "At step 6: objective value: 243.3765869140625\n",
      "At step 7: objective value: 294.82867431640625\n",
      "At step 8: objective value: 350.43646240234375\n",
      "At step 9: objective value: 390.0959167480469\n",
      "At step 10: objective value: 439.7633361816406\n",
      "At step 11: objective value: 477.1222229003906\n",
      "At step 12: objective value: 522.21142578125\n",
      "At step 13: objective value: 555.4069213867188\n",
      "At step 14: objective value: 588.0800170898438\n",
      "At step 15: objective value: 616.86865234375\n",
      "At step 16: objective value: 644.4393310546875\n",
      "At step 17: objective value: 672.7307739257812\n",
      "At step 18: objective value: 696.4898071289062\n",
      "At step 19: objective value: 718.5736083984375\n",
      "At step 20: objective value: 740.7435302734375\n",
      "At step 21: objective value: 757.2225952148438\n",
      "At step 22: objective value: 778.73779296875\n",
      "At step 23: objective value: 793.6539916992188\n",
      "At step 24: objective value: 813.5885009765625\n",
      "Saved to file:  deep-dream.jpg\n"
     ]
    }
   ],
   "source": [
    "# Load the pretrained Inception model as a GraphDef\n",
    "model_fn = os.path.join('./data/inception', 'tensorflow_inception_graph.pb')\n",
    "\n",
    "# Open Inception graph using FastGFile\n",
    "with tf.gfile.FastGFile(model_fn, mode='rb') as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())  # Reading graph\n",
    "    \n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    # Define input for the network\n",
    "    input_image = tf.placeholder(np.float32, name='input')\n",
    "    imagenet_mean = 117.0\n",
    "    input_preprocessed = tf.expand_dims(input_image - imagenet_mean, 0)\n",
    "    \n",
    "    # Load initialized graph definition\n",
    "    tf.import_graph_def(graph_def, {'input': input_preprocessed})\n",
    "    \n",
    "    ## Get a list of Tensor names that are the output of convolutions\n",
    "    # Get a list of Convolution Op's \n",
    "    graph = tf.get_default_graph()\n",
    "    layers = [op.name for op in graph.get_operations() if op.type == 'Conv2D' \n",
    "              and 'import/' in op.name]  # List of op_name\n",
    "    \n",
    "    # Tensor names are of the form \"<op_name>:<output_index>\".\n",
    "    feature_nums = [int(graph.get_tensor_by_name(name + ':0').get_shape()[-1]) for \n",
    "                    name in layers]\n",
    "    print('Number of Conv layers: ', len(layers))\n",
    "    print('Number of feature channels: ', sum(feature_nums))  # Sum of n_channels\n",
    "    \n",
    "    # Pick an internal layer and node to visualize. NOTE: Use outputs before applying\n",
    "    # the ReLU non-linearity to have non-zero gradients for features with negative activation\n",
    "    layer = 'mixed4d_3x3_bottleneck_pre_relu'\n",
    "    layer_n_channels = graph.get_tensor_by_name('import/' + layer + ':0').get_shape()[-1]\n",
    "    print('Number of channels in \"{}\": {}'.format(layer, layer_n_channels))\n",
    "    channel = 139\n",
    "    layer_channel = get_layer(layer)[:, :, :, channel]\n",
    "    \n",
    "    # Define the optimization: Maximize L2 norm of activation\n",
    "    objective = tf.reduce_mean(layer_channel)  # Maximize mean of layer channel activations\n",
    "    \n",
    "    # Gradients with respect to input image using Autodiff\n",
    "    input_gradient = tf.gradients(objective, input_image)[0]\n",
    "    \n",
    "    # Use random noise as an image\n",
    "    noise_image = np.random.uniform(size=(224*2, 224*3, 3)) + 100.0\n",
    "    image = noise_image.copy()\n",
    "    \n",
    "    # Deep Dream\n",
    "    step_scale = 1.0\n",
    "    n_iter = 25\n",
    "    with tf.Session() as sess:\n",
    "        for i in range(n_iter):\n",
    "            image_gradient, obj_value = sess.run([input_gradient, objective], \n",
    "                                                 {input_image: image})\n",
    "            \n",
    "            # Normalize the gradient, so the same step size should work\n",
    "            image_gradient /= image_gradient.std() + 1e-8\n",
    "            image += image_gradient * step_scale\n",
    "            print('At step {}: objective value: {}'.format(i, obj_value))\n",
    "            \n",
    "    # Save the image\n",
    "    std_dev = 0.1\n",
    "    image = (image - image.mean()) / max(image.std(), 1e-4) * std_dev + 0.5\n",
    "    image = normalize_image(image)\n",
    "    save_jpeg('deep-dream.jpg', image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import/conv2d0_pre_relu/conv, import/conv2d1_pre_relu/conv, import/conv2d2_pre_relu/conv, import/mixed3a_pool_reduce_pre_relu/conv, import/mixed3a_5x5_bottleneck_pre_relu/conv, import/mixed3a_5x5_pre_relu/conv, import/mixed3a_3x3_bottleneck_pre_relu/conv, import/mixed3a_3x3_pre_relu/conv, import/mixed3a_1x1_pre_relu/conv, import/mixed3b_pool_reduce_pre_relu/conv, import/mixed3b_5x5_bottleneck_pre_relu/conv, import/mixed3b_5x5_pre_relu/conv, import/mixed3b_3x3_bottleneck_pre_relu/conv, import/mixed3b_3x3_pre_relu/conv, import/mixed3b_1x1_pre_relu/conv, import/mixed4a_pool_reduce_pre_relu/conv, import/mixed4a_5x5_bottleneck_pre_relu/conv, import/mixed4a_5x5_pre_relu/conv, import/mixed4a_3x3_bottleneck_pre_relu/conv, import/mixed4a_3x3_pre_relu/conv, import/mixed4a_1x1_pre_relu/conv, import/head0_bottleneck_pre_relu/conv, import/mixed4b_pool_reduce_pre_relu/conv, import/mixed4b_5x5_bottleneck_pre_relu/conv, import/mixed4b_5x5_pre_relu/conv, import/mixed4b_3x3_bottleneck_pre_relu/conv, import/mixed4b_3x3_pre_relu/conv, import/mixed4b_1x1_pre_relu/conv, import/mixed4c_pool_reduce_pre_relu/conv, import/mixed4c_5x5_bottleneck_pre_relu/conv, import/mixed4c_5x5_pre_relu/conv, import/mixed4c_3x3_bottleneck_pre_relu/conv, import/mixed4c_3x3_pre_relu/conv, import/mixed4c_1x1_pre_relu/conv, import/mixed4d_pool_reduce_pre_relu/conv, import/mixed4d_5x5_bottleneck_pre_relu/conv, import/mixed4d_5x5_pre_relu/conv, import/mixed4d_3x3_bottleneck_pre_relu/conv, import/mixed4d_3x3_pre_relu/conv, import/mixed4d_1x1_pre_relu/conv, import/head1_bottleneck_pre_relu/conv, import/mixed4e_pool_reduce_pre_relu/conv, import/mixed4e_5x5_bottleneck_pre_relu/conv, import/mixed4e_5x5_pre_relu/conv, import/mixed4e_3x3_bottleneck_pre_relu/conv, import/mixed4e_3x3_pre_relu/conv, import/mixed4e_1x1_pre_relu/conv, import/mixed5a_pool_reduce_pre_relu/conv, import/mixed5a_5x5_bottleneck_pre_relu/conv, import/mixed5a_5x5_pre_relu/conv, import/mixed5a_3x3_bottleneck_pre_relu/conv, import/mixed5a_3x3_pre_relu/conv, import/mixed5a_1x1_pre_relu/conv, import/mixed5b_pool_reduce_pre_relu/conv, import/mixed5b_5x5_bottleneck_pre_relu/conv, import/mixed5b_5x5_pre_relu/conv, import/mixed5b_3x3_bottleneck_pre_relu/conv, import/mixed5b_3x3_pre_relu/conv, import/mixed5b_1x1_pre_relu/conv\n"
     ]
    }
   ],
   "source": [
    "print(', '.join(layers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image detail generation method described above tends to produce some patterns more often the others. One easy way to improve the generated image diversity is to tweak the optimization objective. Use one more input \"guide\" image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "guide = np.float32(PIL.Image.open('flowers.jpg'))  # TODO: Follow Google's Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Hvass Lab: Visual Analysis](https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/13_Visual_Analysis.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
