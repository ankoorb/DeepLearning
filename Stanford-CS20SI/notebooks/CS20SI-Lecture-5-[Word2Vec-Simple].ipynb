{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Phase-I: Assemble Graph**\n",
    "1. Import data (`tf.data` or define `placeholders` for input and output)\n",
    "2. Define the `weights`\n",
    "3. Define the inference `model` (i.e. Forward path of the model)\n",
    "4. Define `cost` function\n",
    "5. Define `optimizer`\n",
    "\n",
    "**Phase-II: Execute Computation (i.e. Train Model)**\n",
    "1. Initialize all model variables for the first time\n",
    "2. Feed in the training data. Might involve randomizing the order of data samples\n",
    "3. Execute the inference `model` on the training data\n",
    "4. Compute the `loss`\n",
    "5. Adjust the model `weights` to minimize/maximize `loss` depending on the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Word Embedding`**\n",
    "- Captures the semantic relationships between words\n",
    "\n",
    "**Word2Vec: skip-gram**\n",
    "- Softmax is computationally expensive (because of the size of word vocabulary)\n",
    "- Negative sampling (a simplified version of `Noise Contrastive Estimation (NCE)`). NCE guarantes approximation to softmax, where as Negative sampling does not approximate to softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # Adjust verbosity to suppress information logs\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector # For visualizing embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Pre-Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys # To append to paty: sys.path.append('..')\n",
    "import random\n",
    "import zipfile\n",
    "from six.moves import urllib\n",
    "from collections import Counter\n",
    "\n",
    "# Parameters for downloading data\n",
    "DOWNLOAD_URL = 'http://mattmahoney.net/dc/'\n",
    "EXPECTED_BYTES = 31344016\n",
    "DATA_DIR = 'data/'\n",
    "FILE_NAME = 'text8.zip'\n",
    "\n",
    "def make_dir(path):\n",
    "    \"\"\"Create a directory\"\"\"\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "def download(file_name, expected_bytes):\n",
    "    \"\"\"Download the dataset text8 if it has not already been downloaded.\"\"\"\n",
    "    file_path = DATA_DIR + file_name\n",
    "    if os.path.exists(file_path):\n",
    "        print(\"Dataset is ready.\")\n",
    "        return file_path\n",
    "    file_name, _ = urllib.request.urlretrieve(DOWNLOAD_URL + file_name, file_path)\n",
    "    file_stat = os.stat(file_path)\n",
    "    if file_stat.st_size == expected_bytes:\n",
    "        print('Successfully downloaded the file: ', file_name)\n",
    "    else:\n",
    "        raise Exception('File \"' + file_name + '\" might be corrupted. Downloading it using browser.')\n",
    "    return file_path\n",
    "\n",
    "def read_data(file_path):\n",
    "    \"\"\"Read data into a list of tokens. There should be 17,005,207 tokens.\"\"\"\n",
    "    with zipfile.ZipFile(file_path) as f:\n",
    "        # Convert input to string using Python 2 vs 3 compatibility as_str()\n",
    "        tokens = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def build_vocab(tokens, vocab_size):\n",
    "    \"\"\"Build vocabulary of VOCAB_SIZE most frequent tokens.\"\"\"\n",
    "    dictionary = dict()\n",
    "    count = [('UNK', -1)] # For Unknown words\n",
    "    count.extend(Counter(tokens).most_common(vocab_size - 1)) # Extend the list (ensures that UNK is 0th)\n",
    "    index = 0\n",
    "    make_dir('processed')\n",
    "    with open('processed/vocab_1000.tsv', \"w\") as f:\n",
    "        for token, _ in count:\n",
    "            dictionary[token] = index\n",
    "            if index < 1000:\n",
    "                f.write(token + \"\\n\")\n",
    "            index += 1\n",
    "    index_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return dictionary, index_dictionary\n",
    "\n",
    "def convert_tokens_to_index(tokens, dictionary):\n",
    "    \"\"\"Replace each token in the dataset with its index in the dictionary.\"\"\"\n",
    "    return [dictionary[token] if token in dictionary else 0 for token in tokens]\n",
    "\n",
    "def generate_sample(index_tokens, context_window_size):\n",
    "    \"\"\"Form training pairs according to the skip-gram model.\"\"\"\n",
    "    for index, center in enumerate(index_tokens):\n",
    "        context = random.randint(1, context_window_size)\n",
    "        # Get a random target token before the center token\n",
    "        for target in index_tokens[max(0, index - context): index]:\n",
    "            yield center, target\n",
    "        # Get a random target token after the center token\n",
    "        for target in index_tokens[index + 1: index + context + 1]:\n",
    "            yield center, target\n",
    "            \n",
    "def get_batch(iterator, batch_size):\n",
    "    \"\"\"Group a numerical stream into batches and yield them as Numpy arrays.\"\"\"\n",
    "    while True:\n",
    "        center_batch = np.zeros(batch_size, dtype=np.int32)\n",
    "        target_batch = np.zeros([batch_size, 1])\n",
    "        for index in range(batch_size):\n",
    "            center_batch[index], target_batch[index] = next(iterator)\n",
    "        yield center_batch, target_batch\n",
    "        \n",
    "def process_data(vocab_size, batch_size, skip_window):\n",
    "    file_path = download(FILE_NAME, EXPECTED_BYTES)\n",
    "    tokens = read_data(file_path)\n",
    "    dictionary, _ = build_vocab(tokens, vocab_size)\n",
    "    index_tokens = convert_tokens_to_index(tokens, dictionary)\n",
    "    del tokens # To save memory\n",
    "    single_gen = generate_sample(index_tokens, skip_window)\n",
    "    return get_batch(single_gen, batch_size)\n",
    "\n",
    "def get_index_vocab(vocab_size):\n",
    "    file_path = download(FILE_NAME, EXPECTED_BYTES)\n",
    "    tokens = read_data(file_path)\n",
    "    return build_vocab(tokens, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Skip-Gram (simple)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "VOCAB_SIZE = 50000\n",
    "BATCH_SIZE = 64\n",
    "EMBED_SIZE = 300  # Dimension of the word embedding vectors\n",
    "SKIP_WINDOW = 1  # Context window size\n",
    "NUM_SAMPLED = 25  # Number of negative examples to sample\n",
    "LEARNING_RATE = 1.0\n",
    "NUM_TRAIN_STEPS = 20000\n",
    "SKIP_STEP = 2000  # Steps to skip before reporting the loss\n",
    "\n",
    "# Word2Vec Graph for Skip-Gram\n",
    "def word2vec(batch_gen):\n",
    "    \"\"\"Build the graph for word2vec model and train it.\"\"\"\n",
    "    # Step 1: define the placeholders for input and output\n",
    "    # center_words have to be int to work on embedding lookup\n",
    "\n",
    "    # TODO\n",
    "    # Instead of using one-hot vectors, input the index of those words directly. \n",
    "    # For example, if the center word is the 1001th word in the vocabulary, input the number 1001.\n",
    "    center_words = tf.placeholder(tf.int32, shape=[BATCH_SIZE])\n",
    "    target_words = tf.placeholder(tf.int32, shape=[BATCH_SIZE, 1]) # [BATCH_SIZE] -> ValueError: Shape must be rank 2 but is rank 1\n",
    "\n",
    "    # Step 2: define weights. In word2vec, it's actually the weights that we care about\n",
    "    # vocab size x embed size\n",
    "    # initialized to random uniform -1 to 1\n",
    "\n",
    "    # TODO\n",
    "    # If one word is represented with a vector of size EMBED_SIZE, then the embedding matrix will \n",
    "    # have shape [VOCAB_SIZE, EMBED_SIZE]\n",
    "    embed_matrix = tf.Variable(initial_value=tf.random_uniform([VOCAB_SIZE, EMBED_SIZE], minval=-1, maxval=1))\n",
    "\n",
    "\n",
    "    # Step 3: define the inference\n",
    "    # get the embed of input words using tf.nn.embedding_lookup\n",
    "    # embed = tf.nn.embedding_lookup(embed_matrix, center_words, name='embed')\n",
    "\n",
    "    # TODO\n",
    "    # embed_matrix has dimension VOCAB_SIZE x EMBED_SIZE, with each row of the embedding matrix \n",
    "    # corresponds to the vector representation of the word at that index\n",
    "    embed = tf.nn.embedding_lookup(embed_matrix, center_words, name='embed')\n",
    "\n",
    "\n",
    "    # Step 4: construct variables for NCE loss\n",
    "    # tf.nn.nce_loss(weights, biases, labels, inputs, num_sampled, num_classes, ...)\n",
    "    # nce_weight (vocab size x embed size), intialized to truncated_normal stddev=1.0 / (EMBED_SIZE ** 0.5)\n",
    "    # bias: vocab size, initialized to 0\n",
    "\n",
    "    # TODO\n",
    "    nce_weight = tf.Variable(tf.truncated_normal([VOCAB_SIZE, EMBED_SIZE], stddev=1.0/(EMBED_SIZE**0.5)), \n",
    "                             name='nce_weight')\n",
    "    nce_bias = tf.Variable(tf.zeros([VOCAB_SIZE]), name='nce_bias')\n",
    "\n",
    "    # define loss function to be NCE loss function\n",
    "    # tf.nn.nce_loss(weights, biases, labels, inputs, num_sampled, num_classes, ...)\n",
    "    # need to get the mean accross the batch\n",
    "    # note: you should use embedding of center words for inputs, not center words themselves\n",
    "\n",
    "    # TODO\n",
    "    nce_loss = tf.nn.nce_loss(weights=nce_weight, biases=nce_bias, labels=target_words, inputs=embed, \n",
    "                              num_sampled=NUM_SAMPLED, num_classes=VOCAB_SIZE)\n",
    "    # Mean accross the batch\n",
    "    loss = tf.reduce_mean(nce_loss)\n",
    "\n",
    "        \n",
    "    # Step 5: define optimizer\n",
    "    \n",
    "    # TODO\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=LEARNING_RATE).minimize(loss)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # TODO: initialize variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        total_loss = 0.0 # To calculate the average loss in the last SKIP_STEP steps\n",
    "        writer = tf.summary.FileWriter('./graphs/skip-gram/', sess.graph)\n",
    "        \n",
    "        for index in range(NUM_TRAIN_STEPS):\n",
    "            \n",
    "            # Get batch data\n",
    "            centers, targets = batch_gen.next()\n",
    "            \n",
    "            # TO DO: create feed_dict, run optimizer, fetch loss_batch\n",
    "            _, loss_batch = sess.run([optimizer, loss], feed_dict={center_words: centers, target_words: targets})\n",
    "\n",
    "            total_loss += loss_batch\n",
    "            if (index + 1) % SKIP_STEP == 0:\n",
    "                print('Average loss at step {}: {:5.1f}'.format(index, total_loss/SKIP_STEP))\n",
    "                total_loss = 0.0\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset is ready.\n",
      "Average loss at step 1999: 140.7\n",
      "Average loss at step 3999:  53.4\n",
      "Average loss at step 5999:  31.1\n",
      "Average loss at step 7999:  21.8\n",
      "Average loss at step 9999:  15.2\n",
      "Average loss at step 11999:  14.4\n",
      "Average loss at step 13999:  11.6\n",
      "Average loss at step 15999:  10.5\n",
      "Average loss at step 17999:  10.3\n",
      "Average loss at step 19999:   9.0\n"
     ]
    }
   ],
   "source": [
    "# Get text data and pre-process it\n",
    "batch_gen = process_data(VOCAB_SIZE, BATCH_SIZE, SKIP_WINDOW)\n",
    "\n",
    "# Train\n",
    "word2vec(batch_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name Scope\n",
    "- For grouping nodes together\n",
    "```python\n",
    "with tf.name_scope('name_of_scope'):\n",
    "    # Declare Operation-1\n",
    "    # Declare Operation-2\n",
    "    # ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "VOCAB_SIZE = 50000\n",
    "BATCH_SIZE = 64\n",
    "EMBED_SIZE = 300 # Dimension of the word embedding vectors\n",
    "SKIP_WINDOW = 1 # Context window size\n",
    "NUM_SAMPLED = 100 # Number of negative examples to sample\n",
    "LEARNING_RATE = 1.0\n",
    "NUM_TRAIN_STEPS = 20000\n",
    "SKIP_STEP = 2000 # Steps to skip before reporting the loss\n",
    "WEIGHTS_FLD = 'processed/'\n",
    "\n",
    "# Word2Vec Graph for Skip-Gram\n",
    "def word2vec_ns(batch_gen):\n",
    "    \"\"\"Build the graph for word2vec model and train it.\"\"\"\n",
    "    # Step 1: define the placeholders for input and output\n",
    "    # center_words have to be int to work on embedding lookup\n",
    "\n",
    "    # TODO\n",
    "    # Instead of using one-hot vectors, input the index of those words directly. \n",
    "    # For example, if the center word is the 1001th word in the vocabulary, input the number 1001.\n",
    "    with tf.name_scope('data'):\n",
    "        center_words = tf.placeholder(tf.int32, shape=[BATCH_SIZE])\n",
    "        target_words = tf.placeholder(tf.int32, shape=[BATCH_SIZE, 1]) # [BATCH_SIZE] -> ValueError: Shape must be rank 2 but is rank 1\n",
    "\n",
    "    # Step 2: define weights. In word2vec, it's actually the weights that we care about\n",
    "    # vocab size x embed size\n",
    "    # initialized to random uniform -1 to 1\n",
    "\n",
    "    # TODO\n",
    "    # If one word is represented with a vector of size EMBED_SIZE, then the embedding matrix will \n",
    "    # have shape [VOCAB_SIZE, EMBED_SIZE]\n",
    "    \n",
    "    # It seems like the namescope ‘embeddings’ has only one node and therefore it is useless to put it in a separate namescope. \n",
    "    # It, in fact, has two nodes: one for the tf.Variable and one for tf.random_uniform.\n",
    "    with tf.name_scope('embeddings'):\n",
    "        embed_matrix = tf.Variable(initial_value=tf.random_uniform([VOCAB_SIZE, EMBED_SIZE], minval=-1, maxval=1), name='embed_matrix')\n",
    "\n",
    "\n",
    "    # Step 3: define the inference\n",
    "    # get the embed of input words using tf.nn.embedding_lookup\n",
    "    # embed = tf.nn.embedding_lookup(embed_matrix, center_words, name='embed')\n",
    "\n",
    "    # TODO\n",
    "    # embed_matrix has dimension VOCAB_SIZE x EMBED_SIZE, with each row of the embedding matrix \n",
    "    # corresponds to the vector representation of the word at that index\n",
    "    with tf.name_scope('loss'):\n",
    "        embed = tf.nn.embedding_lookup(embed_matrix, center_words, name='embed')\n",
    "\n",
    "\n",
    "    # Step 4: construct variables for NCE loss\n",
    "    # tf.nn.nce_loss(weights, biases, labels, inputs, num_sampled, num_classes, ...)\n",
    "    # nce_weight (vocab size x embed size), intialized to truncated_normal stddev=1.0 / (EMBED_SIZE ** 0.5)\n",
    "    # bias: vocab size, initialized to 0\n",
    "\n",
    "    # TODO\n",
    "        nce_weight = tf.Variable(tf.truncated_normal([VOCAB_SIZE, EMBED_SIZE], stddev=1.0/(EMBED_SIZE**0.5)), \n",
    "                             name='nce_weight')\n",
    "        nce_bias = tf.Variable(tf.zeros([VOCAB_SIZE]), name='nce_bias')\n",
    "\n",
    "    # define loss function to be NCE loss function\n",
    "    # tf.nn.nce_loss(weights, biases, labels, inputs, num_sampled, num_classes, ...)\n",
    "    # need to get the mean accross the batch\n",
    "    # note: you should use embedding of center words for inputs, not center words themselves\n",
    "\n",
    "    # TODO\n",
    "        nce_loss = tf.nn.nce_loss(weights=nce_weight, biases=nce_bias, labels=target_words, inputs=embed, \n",
    "                              num_sampled=NUM_SAMPLED, num_classes=VOCAB_SIZE)\n",
    "    # Mean accross the batch\n",
    "        loss = tf.reduce_mean(nce_loss)\n",
    "\n",
    "        \n",
    "    # Step 5: define optimizer\n",
    "    \n",
    "    # TODO\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=LEARNING_RATE).minimize(loss)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # TODO: initialize variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        total_loss = 0.0 # To calculate the average loss in the last SKIP_STEP steps\n",
    "        writer = tf.summary.FileWriter('./graphs/no_frills/', sess.graph)\n",
    "        \n",
    "        for index in range(NUM_TRAIN_STEPS):\n",
    "            \n",
    "            # Get batch data\n",
    "            centers, targets = batch_gen.next()\n",
    "            \n",
    "            # TO DO: create feed_dict, run optimizer, fetch loss_batch\n",
    "            _, loss_batch = sess.run([optimizer, loss], feed_dict={center_words: centers, target_words: targets})\n",
    "\n",
    "            total_loss += loss_batch\n",
    "            if (index + 1) % SKIP_STEP == 0:\n",
    "                print('Average loss at step {}: {:5.1f}'.format(index, total_loss/SKIP_STEP))\n",
    "                total_loss = 0.0\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get text data and pre-process it\n",
    "batch_gen = process_data(VOCAB_SIZE, BATCH_SIZE, SKIP_WINDOW)\n",
    "\n",
    "# Train\n",
    "word2vec_ns(batch_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_namescope(batch_gen):\n",
    "    \"\"\"Build the graph for word2vec model and train it.\"\"\"\n",
    "    \n",
    "    # Step 1: define the placeholders for input and output\n",
    "    with tf.name_scope('data'):\n",
    "        center_words = tf.placeholder(tf.int32, shape=[BATCH_SIZE])\n",
    "        target_words = tf.placeholder(tf.int32, shape=[BATCH_SIZE, 1])\n",
    "        \n",
    "    # Assemble this part of graph on the GPU or CPU\n",
    "    with tf.device('/cpu:0'):\n",
    "        \n",
    "        # Step 2: define weights (vocab size x embed size)\n",
    "        with tf.name_scope('embeddings'):\n",
    "            embed_matrix = tf.Variable(initial_value=tf.random_uniform([VOCAB_SIZE, EMBED_SIZE], minval=-1, maxval=1), \n",
    "                                       name='embed_matrix')\n",
    "\n",
    "    # Step 3 + 4: define the inference + the loss function\n",
    "    with tf.name_scope('loss'):\n",
    "        \n",
    "        # Step 3: define the inference\n",
    "        embed = tf.nn.embedding_lookup(embed_matrix, center_words, name='embed')\n",
    "        \n",
    "        # Step 4: construct variables for NCE loss\n",
    "        nce_weight = tf.Variable(tf.truncated_normal([VOCAB_SIZE, EMBED_SIZE], stddev=1.0/(EMBED_SIZE**0.5)), \n",
    "                                 name='nce_weight')\n",
    "        \n",
    "        nce_bias = tf.Variable(tf.zeros([VOCAB_SIZE]), name='nce_bias')\n",
    "        \n",
    "        # define loss function to be NCE loss function\n",
    "        nce_loss = tf.nn.nce_loss(weights=nce_weight, biases=nce_bias, labels=target_words, inputs=embed, \n",
    "                                  num_sampled=NUM_SAMPLED, num_classes=VOCAB_SIZE)\n",
    "        \n",
    "        # Mean accross the batch\n",
    "        loss = tf.reduce_mean(nce_loss)\n",
    "        \n",
    "    # Step 5: define optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=LEARNING_RATE).minimize(loss)\n",
    "    \n",
    "    \n",
    "    # Train\n",
    "    with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "        \n",
    "        # initialize variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        total_loss = 0.0 # To calculate the average loss in the last SKIP_STEP steps\n",
    "        writer = tf.summary.FileWriter('./graphs/no_frills/', sess.graph)\n",
    "        \n",
    "        for index in range(NUM_TRAIN_STEPS):\n",
    "            \n",
    "            # Get batch data\n",
    "            centers, targets = batch_gen.next()\n",
    "            \n",
    "            # create feed_dict, run optimizer, fetch loss_batch\n",
    "            _, loss_batch = sess.run([optimizer, loss], feed_dict={center_words: centers, target_words: targets})\n",
    "\n",
    "            total_loss += loss_batch\n",
    "            if (index + 1) % SKIP_STEP == 0:\n",
    "                print('Average loss at step {}: {:5.1f}'.format(index, total_loss/SKIP_STEP))\n",
    "                total_loss = 0.0\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "tf.reset_default_graph()\n",
    "word2vec_namescope(batch_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "VOCAB_SIZE = 50000\n",
    "BATCH_SIZE = 64\n",
    "EMBED_SIZE = 300 # Dimension of the word embedding vectors\n",
    "SKIP_WINDOW = 1 # Context window size\n",
    "NUM_SAMPLED = 100 # Number of negative examples to sample\n",
    "LEARNING_RATE = 1.0\n",
    "NUM_TRAIN_STEPS = 20000\n",
    "SKIP_STEP = 2000 # Steps to skip before reporting the loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Skip Gram Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Course examples\n",
    "def train_model(model, batch_gen, num_train_steps, weights_fld):\n",
    "    saver = tf.train.Saver() # defaults to saving all variables - in this case embed_matrix, nce_weight, nce_bias\n",
    "\n",
    "    initial_step = 0\n",
    "    make_dir('checkpoints')\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Get CheckpointState proto from the \"checkpoint\" file.\n",
    "        ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint'))\n",
    "        \n",
    "        # if that checkpoint exists, restore from checkpoint\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "        total_loss = 0.0 # we use this to calculate late average loss in the last SKIP_STEP steps\n",
    "        writer = tf.summary.FileWriter('improved_graph/lr' + str(LEARNING_RATE), sess.graph)\n",
    "        initial_step = model.global_step.eval()\n",
    "        for index in range(initial_step, initial_step + num_train_steps):\n",
    "            centers, targets = batch_gen.next()\n",
    "            feed_dict={model.center_words: centers, model.target_words: targets}\n",
    "            loss_batch, _, summary = sess.run([model.loss, model.optimizer, model.summary_op], \n",
    "                                              feed_dict=feed_dict)\n",
    "            writer.add_summary(summary, global_step=index)\n",
    "            total_loss += loss_batch\n",
    "            if (index + 1) % SKIP_STEP == 0:\n",
    "                print('Average loss at step {}: {:5.1f}'.format(index, total_loss / SKIP_STEP))\n",
    "                total_loss = 0.0\n",
    "                saver.save(sess, 'checkpoints/skip-gram', index)\n",
    "        \n",
    "        ####################\n",
    "        # code to visualize the embeddings. uncomment the below to visualize embeddings\n",
    "        # run \"'tensorboard --logdir='processed'\" to see the embeddings\n",
    "        # final_embed_matrix = sess.run(model.embed_matrix)\n",
    "        \n",
    "        # # it has to variable. constants don't work here. you can't reuse model.embed_matrix\n",
    "        # embedding_var = tf.Variable(final_embed_matrix[:1000], name='embedding')\n",
    "        # sess.run(embedding_var.initializer)\n",
    "\n",
    "        # config = projector.ProjectorConfig()\n",
    "        # summary_writer = tf.summary.FileWriter('processed')\n",
    "\n",
    "        # # add embedding to the config file\n",
    "        # embedding = config.embeddings.add()\n",
    "        # embedding.tensor_name = embedding_var.name\n",
    "        \n",
    "        # # link this tensor to its metadata file, in this case the first 500 words of vocab\n",
    "        # embedding.metadata_path = 'processed/vocab_1000.tsv'\n",
    "\n",
    "        # # saves a configuration file that TensorBoard will read during startup.\n",
    "        # projector.visualize_embeddings(summary_writer, config)\n",
    "        # saver_embed = tf.train.Saver([embedding_var])\n",
    "        # saver_embed.save(sess, 'processed/model3.ckpt', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train using function\n",
    "tf.reset_default_graph()\n",
    "skg = SkipGramModel(VOCAB_SIZE, BATCH_SIZE, EMBED_SIZE, NUM_SAMPLED, LEARNING_RATE)\n",
    "skg.build_graph()\n",
    "train_model(skg, batch_gen, NUM_TRAIN_STEPS, WEIGHTS_FLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModel():\n",
    "    \"\"\"\n",
    "    Build the graph for Word2Vec model.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, batch_size, embed_size, num_sampled, learning_rate):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.batch_size = batch_size\n",
    "        self.embed_size = embed_size\n",
    "        self.num_sampled = num_sampled\n",
    "        self.learning_rate = learning_rate\n",
    "        self.global_step = tf.Variable(initial_value=0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "        \n",
    "    \n",
    "    def _create_placeholder(self):\n",
    "        \"\"\"Step 1: define placeholders for input and output\"\"\"\n",
    "        with tf.name_scope('data'):\n",
    "            self.center_words = tf.placeholder(tf.int32, shape=[self.batch_size])\n",
    "            self.target_words = tf.placeholder(tf.int32, shape=[self.batch_size, 1])\n",
    "    \n",
    "    def _create_embeddings(self):\n",
    "        \"\"\"Step 2: define weights, (vocab size x embed size)\"\"\"\n",
    "        with tf.device('/cpu:0'):\n",
    "            with tf.name_scope('embeddings'):\n",
    "                self.embed_matrix = tf.Variable(initial_value=tf.random_uniform([self.vocab_size, self.embed_size], \n",
    "                                                                                minval=-1, maxval=1), name='embed_matrix')\n",
    "    \n",
    "    def _create_loss(self):\n",
    "        \"\"\"Step 3 + 4: define the inference + the loss function\"\"\"\n",
    "        with tf.name_scope('loss'):\n",
    "            # Step 3: define the inference\n",
    "            self.embed = tf.nn.embedding_lookup(self.embed_matrix, self.center_words, name='embed')\n",
    "            \n",
    "            # Step 4: construct variables for NCE loss\n",
    "            self.nce_weight = tf.Variable(tf.truncated_normal([self.vocab_size, self.embed_size], \n",
    "                                                              stddev=1.0/(self.embed_size**0.5)), name='nce_weight')\n",
    "        \n",
    "            self.nce_bias = tf.Variable(tf.zeros([self.vocab_size]), name='nce_bias')\n",
    "            \n",
    "            self.nce_loss = tf.nn.nce_loss(weights=self.nce_weight, biases=self.nce_bias, labels=self.target_words, \n",
    "                                           inputs=self.embed, num_sampled=self.num_sampled, num_classes=self.vocab_size)\n",
    "            # Mean accross the batch\n",
    "            self.loss = tf.reduce_mean(self.nce_loss)\n",
    "            \n",
    "    \n",
    "    def _create_optimizer(self):\n",
    "        \"\"\"Step 5: define optimizer\"\"\"\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate).minimize(self.loss, \n",
    "                                                                                                      global_step=self.global_step)\n",
    "        \n",
    "    def _create_summaries(self):\n",
    "        with tf.name_scope(\"summaries\"):\n",
    "            tf.summary.scalar(\"loss\", self.loss)\n",
    "            tf.summary.histogram(\"histogram_loss\", self.loss)\n",
    "            \n",
    "            # Because there are 2 summaries - merge summaries into one op to make it easier to manage\n",
    "            self.summary_op = tf.summary.merge_all()\n",
    "        \n",
    "    def build_graph(self):\n",
    "        self._create_placeholder()\n",
    "        self._create_embeddings()\n",
    "        self._create_loss()\n",
    "        self._create_optimizer()\n",
    "        self._create_summaries()\n",
    "    \n",
    "#     def train(self, batch_gen, num_train_steps, skip_step, weights_fld):\n",
    "        \n",
    "#         # Instantiate class\n",
    "#         sgm = self.__class__(self.vocab_size, self.batch_size, self.embed_size, self.num_sampled, \n",
    "#                              self.learning_rate)\n",
    "        \n",
    "#         # Build graph\n",
    "#         sgm.build_graph()\n",
    "        \n",
    "#         # Object to save and restore variables\n",
    "#         saver = tf.train.Saver() # defaults to saving all variables - embed_matrix, nce_weight, nce_bias\n",
    "        \n",
    "#         # Create directory for checkpoints\n",
    "#         make_dir('checkpoints')\n",
    "        \n",
    "#         with tf.Session() as sess:\n",
    "            \n",
    "#             # Initialize variables\n",
    "#             sess.run(tf.global_variables_initializer())\n",
    "                        \n",
    "#             # Get CheckpointState proto from the \"checkpoint\" file.\n",
    "#             ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint'))\n",
    "            \n",
    "#             # If checkpoint exists then restroe from checkpoint\n",
    "#             if ckpt and ckpt.model_checkpoint_path:\n",
    "#                 saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "                \n",
    "#             total_loss = 0.0 # To calculate late average loss in the last SKIP_STEP steps\n",
    "            \n",
    "#             # To write `Summary` protocol buffers to event files\n",
    "#             writer = tf.summary.FileWriter('improved_graph/lr' + str(self.learning_rate), sess.graph)\n",
    "            \n",
    "#             initial_step = sgm.global_step.eval() # Why?\n",
    "            \n",
    "#             for index in range(initial_step, initial_step + num_train_steps):\n",
    "#                 centers, targets = batch_gen.next()\n",
    "#                 feed_dict = {sgm.center_words: centers, sgm.target_words: targets}\n",
    "#                 batch_loss, _, summary = sess.run([sgm.loss, sgm.optimizer, sgm.summary_op],\n",
    "#                                                   feed_dict=feed_dict)\n",
    "                \n",
    "#                 writer.add_summary(summary, global_step=index)\n",
    "#                 total_loss += batch_loss\n",
    "#                 if (index + 1) % skip_step == 0:\n",
    "#                     print('Average loss at step {}: {:5.1f}'.format(index, total_loss / skip_step))\n",
    "#                 total_loss = 0.0\n",
    "#                 saver.save(sess, 'checkpoints/skip-gram', index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train using SkipGramModel method\n",
    "tf.reset_default_graph()\n",
    "skg = SkipGramModel(VOCAB_SIZE, BATCH_SIZE, EMBED_SIZE, NUM_SAMPLED, LEARNING_RATE)\n",
    "skg.train(batch_gen, NUM_TRAIN_STEPS, SKIP_STEP, WEIGHTS_FLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Step\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(skg.global_step.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'a', 'b', 'c', 'd', 'e', 'a', 'b', 'c']\n",
    "print(Counter(tokens))\n",
    "print(Counter(tokens).most_common(3))\n",
    "count = [('u', -1)]\n",
    "count.extend(Counter(tokens).most_common(3))\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "text = \"\"\"Randomness is the lack of pattern or predictability in events.\n",
    "A random sequence of events, symbols or steps has no order and does not \n",
    "follow an intelligible pattern or combination. Individual random events \n",
    "are by definition unpredictable, but in many cases the frequency of \n",
    "different outcomes over a large number of events (or \"trials\") is \n",
    "predictable. For example, when throwing two dice, the outcome of any \n",
    "particular roll is unpredictable, but a sum of 7 will occur twice as \n",
    "often as 4. In this view, randomness is a measure of uncertainty of an \n",
    "outcome, rather than haphazardness, and applies to concepts of chance, \n",
    "probability, and information entropy.\"\"\"\n",
    "\n",
    "tokens = text.split()\n",
    "count = [('UNK', -1)]\n",
    "count.extend(Counter(tokens).most_common(50))\n",
    "\n",
    "# Create dictionary and index dictionary\n",
    "dictionary = dict()\n",
    "index = 0\n",
    "for token, _ in count:\n",
    "    dictionary[token] = index\n",
    "    index += 1\n",
    "    \n",
    "index_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "\n",
    "# Replace each token in the dataset with its index in the dictionary\n",
    "index_tokens = [dictionary[token] if token in dictionary else 0 for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_tokens[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "context_window_size = 5\n",
    "for index, center in enumerate(index_tokens):\n",
    "    context = random.randint(1, context_window_size)\n",
    "    print(index-context, index)\n",
    "    for target in index_tokens[max(0, index - context): index]:\n",
    "        print(index_dictionary[center], index_dictionary[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
