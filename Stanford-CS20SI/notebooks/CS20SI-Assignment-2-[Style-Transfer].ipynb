{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Adjust verbosity to suppress information logs\n",
    "\n",
    "from six.moves import urllib\n",
    "from scipy.misc import imsave\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import scipy.io  # For loading pre-trained VGG's *.mat files using loadmat\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dir(path):\n",
    "    \"\"\"Create a directory if directory does not exist.\"\"\"\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError:\n",
    "        pass\n",
    "    \n",
    "def save_image(path, image):\n",
    "    \"\"\"Save image to a specified path.\"\"\"\n",
    "    image = image[0]\n",
    "    image = np.clip(image, a_min=0, a_max=255).astype('uint8')\n",
    "    imsave(path, image)\n",
    "    \n",
    "def generate_noise_image(content_image, width, height, noise_ratio=0.6):\n",
    "    \"\"\"Generate a noise image\"\"\"\n",
    "    noise_image = np.random.uniform(low=-20, high=20, \n",
    "                                    size=(1, height, width, 3)).astype(np.float32)\n",
    "    \n",
    "    return noise_image * noise_ratio + content_image * (1 - noise_ratio)\n",
    "\n",
    "def download_model(url, file_name, expected_bytes):\n",
    "    \"\"\"\n",
    "    Download the pre-trained VGG-19 model if it is not already downloaded.\n",
    "    \"\"\"\n",
    "    if os.path.exists(file_name):\n",
    "        print('VGG-19 pre-trained model is ready.')\n",
    "        return\n",
    "    print('Downloading the pre-trained VGG-19 model ...')\n",
    "    file_name, _ = urllib.request.urlretrieve(url, file_name)\n",
    "    file_stat = os.stat(file_name)\n",
    "    if file_stat.st_size == expected_bytes:\n",
    "        print('Successfully downloaded VGG-19 pre-trained model', file_name)\n",
    "    else:\n",
    "        raise Exception('File: ' + file_name + ' might be corrupted. Try downloading with browser.')\n",
    "        \n",
    "def resize_image(img_path, width, height, save=False):\n",
    "    \"\"\"Resize image (and save it), and return image by expanding dims\"\"\"\n",
    "    image = Image.open(img_path)\n",
    "    \n",
    "    # PIL is column major so swap the places of width and height\n",
    "    image = ImageOps.fit(image, size=(width, height), method=Image.ANTIALIAS)  # Returns a sized and cropped version of the image\n",
    "    \n",
    "    # Save resized image\n",
    "    if save:\n",
    "        img_dirs = img_path.split('/')\n",
    "        img_dirs[-1] = 'resized_' + img_dirs[-1]  # ./img.jpg -> ./resized_img.jpg\n",
    "        out_path = '/'.join(img_dirs)\n",
    "        if not os.path.exists(out_path):\n",
    "            image.save(out_path)\n",
    "    \n",
    "    # Return resized image after expanding dims\n",
    "    image = np.asarray(image, dtype=np.float32)  # Convert input (PIL format) to array\n",
    "    return np.expand_dims(image, axis=0)\n",
    "\n",
    "def plot_images(content_img, style_img, generated_img):\n",
    "    # Create figure with sub-plots\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(10, 10))\n",
    "    \n",
    "    # Adjust spacing\n",
    "    fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "    \n",
    "    # Use interpolation to smooth pixels\n",
    "    smooth = True\n",
    "    \n",
    "    if smooth:\n",
    "        interpolation = 'sinc'\n",
    "    else:\n",
    "        interpolation = 'nearest'\n",
    "        \n",
    "    # Plot content image\n",
    "    # NOTE: Pixel vales are divided by 255 to normalize to [0.0, 1.0] range \n",
    "    ax = axes.flat[0]\n",
    "    ax.imshow(content_img / 255.0, interpolation=interpolation)\n",
    "    ax.set_xlabel('Content')\n",
    "    \n",
    "    # Plot generated image\n",
    "    ax = axes.flat[1]\n",
    "    ax.imshow(generated_img / 255.0, interpolation=interpolation)\n",
    "    ax.set_xlabel('Generated')\n",
    "    \n",
    "    # Plot style image\n",
    "    ax = axes.flat[2]\n",
    "    ax.imshow(style_img / 255.0, interpolation=interpolation)\n",
    "    ax.set_xlabel('Style')\n",
    "    \n",
    "    # Remove ticks from all the plots\n",
    "    for ax in axes.flat:\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG-19 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG-19 parameters file\n",
    "VGG_DOWNLOAD_LINK = 'http://www.vlfeat.org/matconvnet/models/imagenet-vgg-verydeep-19.mat'\n",
    "VGG_FILENAME = 'imagenet-vgg-verydeep-19.mat'\n",
    "EXPECTED_BYTES = 534904783\n",
    "\n",
    "class VGG:\n",
    "    def __init__(self, input_img):\n",
    "        download_model(VGG_DOWNLOAD_LINK, VGG_FILENAME, EXPECTED_BYTES)\n",
    "        self.input_img = input_img\n",
    "        self.vgg_layers = scipy.io.loadmat(VGG_FILENAME)['layers']\n",
    "        # VGG-19 trained with mean centered images with mean = [123.68, 116.779, 103.939] along RGB dimensions\n",
    "        self.mean_pixels = np.array([123.68, 116.779, 103.939]).reshape((1, 1, 1, 3))\n",
    "        \n",
    "    def _weights(self, layer_idx, expected_layer_name):\n",
    "        \"\"\"\n",
    "        Return the weights and biases at layer_idx from pre-trained VGG-19.\n",
    "        \"\"\"\n",
    "        W = self.vgg_layers[0][layer_idx][0][0][2][0][0]\n",
    "        b = self.vgg_layers[0][layer_idx][0][0][2][0][1]\n",
    "        layer_name = self.vgg_layers[0][layer_idx][0][0][0][0]\n",
    "        assert layer_name == expected_layer_name\n",
    "        return W, b.reshape(b.size)\n",
    "    \n",
    "    def conv_relu(self, prev_layer, layer_idx, layer_name):\n",
    "        \"\"\"\n",
    "        Create CONV layer with ReLU using the weights and biases extracted \n",
    "        from VGG-19 model at layer_idx.\n",
    "        \n",
    "        _weights returns numpy arrays, convert them to TF tensors.\n",
    "        \n",
    "        Args:\n",
    "          prev_layer: output tensor from the previous layer\n",
    "          layer_name: str, to name the layer. It is used to specify \n",
    "          variable scope.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(layer_name, reuse=tf.AUTO_REUSE) as scope:\n",
    "            W, b = self._weights(layer_idx, layer_name)\n",
    "            W_tensor = tf.convert_to_tensor(W, np.float32, name='weight')\n",
    "            b_tensor = tf.convert_to_tensor(b, np.float32, name='bias')\n",
    "            conv = tf.nn.conv2d(prev_layer, \n",
    "                                filter=W_tensor, \n",
    "                                strides=[1, 1, 1, 1], \n",
    "                                padding='SAME')\n",
    "            out = tf.nn.relu(conv + b_tensor)\n",
    "        setattr(self, layer_name, out)  # set layer_name attribute as output tensor\n",
    "    \n",
    "    def avg_pool(self, prev_layer, layer_name):\n",
    "        \"\"\"\n",
    "        Create the average pooling layer. According to paper (arXiv: \n",
    "        https://arxiv.org/pdf/1508.06576.pdf) replacing max-pooling with\n",
    "        average pooling improves gradient flow.\n",
    "        \n",
    "        Args:\n",
    "          prev_layer: output tensor from the previous layer\n",
    "          layer_name: str, to name the layer. It is used to specify \n",
    "          variable scope.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(layer_name, reuse=tf.AUTO_REUSE) as scope:\n",
    "            out = tf.nn.avg_pool(prev_layer, \n",
    "                                 ksize=[1, 2, 2, 1], \n",
    "                                 strides=[1, 2, 2, 1], \n",
    "                                 padding='SAME')\n",
    "            \n",
    "        setattr(self, layer_name, out)  # set layer_name attribute as output tensor\n",
    "        \n",
    "    def load(self):\n",
    "        self.conv_relu(self.input_img, 0, 'conv1_1')\n",
    "        self.conv_relu(self.conv1_1, 2, 'conv1_2')\n",
    "        self.avg_pool(self.conv1_2, 'avgpool1')\n",
    "        self.conv_relu(self.avgpool1, 5, 'conv2_1')\n",
    "        self.conv_relu(self.conv2_1, 7, 'conv2_2')\n",
    "        self.avg_pool(self.conv2_2, 'avgpool2')\n",
    "        self.conv_relu(self.avgpool2, 10, 'conv3_1')\n",
    "        self.conv_relu(self.conv3_1, 12, 'conv3_2')\n",
    "        self.conv_relu(self.conv3_2, 14, 'conv3_3')\n",
    "        self.conv_relu(self.conv3_3, 16, 'conv3_4')\n",
    "        self.avg_pool(self.conv3_4, 'avgpool3')\n",
    "        self.conv_relu(self.avgpool3, 19, 'conv4_1')\n",
    "        self.conv_relu(self.conv4_1, 21, 'conv4_2')\n",
    "        self.conv_relu(self.conv4_2, 23, 'conv4_3')\n",
    "        self.conv_relu(self.conv4_3, 25, 'conv4_4')\n",
    "        self.avg_pool(self.conv4_4, 'avgpool4')\n",
    "        self.conv_relu(self.avgpool4, 28, 'conv5_1')\n",
    "        self.conv_relu(self.conv5_1, 30, 'conv5_2')\n",
    "        self.conv_relu(self.conv5_2, 32, 'conv5_3')\n",
    "        self.conv_relu(self.conv5_3, 34, 'conv5_4')\n",
    "        self.avg_pool(self.conv5_4, 'avgpool5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style Transfer\n",
    "\n",
    "[A Neural Algorithm of Artistic Style (Gatys et al., 2016) \n",
    "](https://arxiv.org/pdf/1508.06576.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleTransfer:\n",
    "    \n",
    "    def __init__(self, content_img, style_img, img_width, img_height):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            content_img: str, path to content image.\n",
    "            style_img: str, path to style image.\n",
    "            img_width: int, width for output image.\n",
    "            img_height: ing, height for output image.\n",
    "            \n",
    "        NOTE: Input content image and input style image will be resized \n",
    "        to match img_width and img_height\n",
    "        \"\"\"\n",
    "        self.img_width = img_width\n",
    "        self.img_height = img_height\n",
    "        self.content_img = resize_image(content_img, self.img_width, self.img_height)\n",
    "        self.style_img = resize_image(style_img, self.img_width, self.img_height)\n",
    "        self.initial_img = generate_noise_image(self.content_img, self.img_width, self.img_height)\n",
    "        \n",
    "        # Create global step and hyper-parameters for the model\n",
    "        self.global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "        self.content_layer = 'conv4_2'  # Paper\n",
    "        self.style_layers = ['conv1_1', 'conv2_1', 'conv3_1', 'conv4_1', 'conv5_1']  # Paper\n",
    "        self.content_w = 0.01  # alpha/beta = 0.001 or 0.0001 but 1/20 or 1/50 works fine too.\n",
    "        self.style_w = 1.0 \n",
    "        self.style_layer_w = [0.5, 1.0, 1.5, 3.0, 4.0]  # More emphasis on deep layers\n",
    "        self.lr = 2.0\n",
    "        \n",
    "    def create_input(self):\n",
    "        \"\"\"\n",
    "        All the 3 inputs (content image, style image, generated image) have the \n",
    "        same dimensions and are inputs to same computation to extract the same set\n",
    "        of features. To avoid assembling same sub-graphs multiple times, use one \n",
    "        variable for the 3 inputs.\n",
    "        \n",
    "        NOTE: image height corresponds to number of rows.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope('input', reuse=tf.AUTO_REUSE) as scope:\n",
    "            self.input_img = tf.get_variable(name='input_img', \n",
    "                                             shape=[1, self.img_height, self.img_width, 3], \n",
    "                                             dtype=tf.float32, \n",
    "                                             initializer=tf.zeros_initializer())\n",
    "            \n",
    "    def load_vgg(self):\n",
    "        \"\"\"\n",
    "        Load the saved model parameters of VGG-19. Use input_img as input to \n",
    "        compute the output at each layer of VGG.\n",
    "        \n",
    "        During training VGG-19, the images were mean centered with mean pixels\n",
    "        to be [123.68, 116.779, 103.939] along RGB dimensions, so this should \n",
    "        be subtracted from images.\n",
    "        \"\"\"\n",
    "        self.vgg = VGG(self.input_img)\n",
    "        self.vgg.load()\n",
    "        self.content_img -= self.vgg.mean_pixels  # subtract mean_pixels from content\n",
    "        self.style_img -= self.vgg.mean_pixels  # subtract mean_pixels from style\n",
    "        \n",
    "    def _content_loss(self, P, F):\n",
    "        \"\"\"\n",
    "        Calculate the loss between the feature representation of the \n",
    "        content image and the generated image.\n",
    "        \n",
    "        Args:\n",
    "          P: Content representation of the content image\n",
    "          F: Content representation of the generated image\n",
    "        \"\"\"\n",
    "        # NOTE: P is content image content and will remain constant during training\n",
    "        # however F be assigned initial_img and F will vary as training goes on \n",
    "        coefficient = tf.reciprocal(tf.cast(4 * tf.reduce_prod(P.shape), dtype=tf.float32))\n",
    "        self.content_loss = coefficient * tf.reduce_sum(tf.square(P - F))\n",
    "    \n",
    "    def _gram_matrix(self, F, N, M):\n",
    "        \"\"\"\n",
    "        Create and return the gram matrix for tensor F.\n",
    "        \"\"\"\n",
    "        F_reshape = tf.reshape(F, shape=[N, M])\n",
    "        return tf.matmul(F_reshape, tf.transpose(F_reshape))\n",
    "    \n",
    "    def _layer_style_loss(self, a, g):\n",
    "        \"\"\"\n",
    "        Calculate the style loss at a certain layer.\n",
    "        \n",
    "        Args:\n",
    "          a: feature representation of the style image at layer.\n",
    "          g: feature representation of the generate image at layer.\n",
    "        \"\"\"\n",
    "        M = tf.reduce_prod(a.shape[:3])\n",
    "        N = a.shape[-1]\n",
    "        a_gram = self._gram_matrix(a, N, M)\n",
    "        g_gram = self._gram_matrix(g, N, M)\n",
    "        coefficient = tf.reciprocal(tf.cast(4 * tf.square(N) * tf.square(M), dtype=tf.float32))\n",
    "        return coefficient * tf.reduce_sum(tf.square(g_gram - a_gram))\n",
    "    \n",
    "    def _style_loss(self, A):\n",
    "        \"\"\"\n",
    "        Calculate the total style loss as a weighted sum of style losses at \n",
    "        all style layers.\n",
    "        \"\"\"\n",
    "        # NOTE: A is a list of different layer's style features computed from style image and \n",
    "        # will remain constant during training however `getattr` will compute style features\n",
    "        # from initial_img and style features will vary as training goes on\n",
    "        wE = [w * self._layer_style_loss(a, getattr(self.vgg, l)) for w, a, l in \n",
    "              zip(self.style_layer_w, A, self.style_layers)]\n",
    "        self.style_loss = tf.reduce_sum(wE)\n",
    "        \n",
    "    def loss(self):\n",
    "        \"\"\"\n",
    "        Calculate loss: alpha * content_loss + beta * style_loss\n",
    "        \"\"\"\n",
    "        with tf.variable_scope('loss', reuse=tf.AUTO_REUSE) as scope:\n",
    "            with tf.Session() as sess:\n",
    "                # Assign content image to input variable\n",
    "                sess.run(self.input_img.assign(self.content_img))\n",
    "                generate_img_content = getattr(self.vgg, self.content_layer)  # Tensor\n",
    "                content_img_content = sess.run(generate_img_content)\n",
    "                print('content_img_content', np.sum(content_img_content))\n",
    "            self._content_loss(content_img_content, generate_img_content)\n",
    "            \n",
    "            with tf.Session() as sess:\n",
    "                # Assign style image to input variable\n",
    "                sess.run(self.input_img.assign(self.style_img))\n",
    "                style_layers_features = sess.run([getattr(self.vgg, layer) \n",
    "                                                  for layer in self.style_layers])\n",
    "                \n",
    "                print('style_layers_features', np.sum([np.sum(f) for f in style_layers_features]))\n",
    "                \n",
    "            self._style_loss(style_layers_features)\n",
    "                \n",
    "        # alpha * content loss + beta * style loss   \n",
    "        self.total_loss = self.content_w * self.content_loss + self.style_w * self.style_loss\n",
    "        \n",
    "    def optimize(self):\n",
    "        \"\"\"Gradient Descent Optimizer\"\"\"\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.lr).minimize(self.total_loss, global_step=self.global_step)\n",
    "        \n",
    "    def summarize(self):\n",
    "        with tf.name_scope(\"summaries\"):\n",
    "            tf.summary.scalar(\"content_loss\", self.content_loss)\n",
    "            tf.summary.scalar(\"style_loss\", self.style_loss)\n",
    "            tf.summary.scalar(\"combined_loss\", self.total_loss)\n",
    "            # Merge multiple summaries \n",
    "            self.summary_op = tf.summary.merge_all()\n",
    "            \n",
    "    def setup(self):\n",
    "        make_dir('checkpoints')\n",
    "        make_dir('output')\n",
    "        \n",
    "    def build(self):\n",
    "        self.setup()\n",
    "        self.create_input()\n",
    "        self.load_vgg()\n",
    "        self.loss()\n",
    "        self.optimize()\n",
    "        self.summarize()\n",
    "        \n",
    "    def train(self, n_iters):\n",
    "        skip_step = 1\n",
    "        with tf.Session() as sess:\n",
    "            # Initialize variables\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            # Writer to write variables\n",
    "            writer = tf.summary.FileWriter('./graphs/style', sess.graph)\n",
    "            \n",
    "            # Saver to save checkpoints\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "            # Get checkpoint state\n",
    "            ckpt = tf.train.get_checkpoint_state(os.path.dirname('./checkpoints/style'))\n",
    "            \n",
    "            # If checkpoint exists then restore from checkpoint\n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "                saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            \n",
    "            # Generate noise image and assign it to input_img\n",
    "            sess.run(self.input_img.assign(self.initial_img))\n",
    "            \n",
    "            # Starting point for training loop\n",
    "            initial_step = self.global_step.eval()\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            for i in range(initial_step, n_iters):\n",
    "                if i >= 5 and i < 20:\n",
    "                    skip_step = 10\n",
    "                elif i >= 20:\n",
    "                    skip_step = 20\n",
    "                    \n",
    "                _ = sess.run(self.optimizer)  # Optimize and update\n",
    "                \n",
    "                if (i + 1) % skip_step == 0:\n",
    "                    # Get generated image, loss, and summary\n",
    "                    generated_img, total_loss, summary = sess.run([self.input_img, \n",
    "                                                                   self.total_loss, \n",
    "                                                                   self.summary_op])\n",
    "                    \n",
    "                    # Add back the mean pixes that were subtracted earlier\n",
    "                    generated_img += self.vgg.mean_pixels\n",
    "                    \n",
    "                    # Plot content, style and generated images\n",
    "                    # plot_images(self.content_img, self.style_img, generated_img)\n",
    "                    \n",
    "                    writer.add_summary(summary, global_step=i)\n",
    "                    print('Step {}\\n \\tSum: {:5.1f}'.format(i+1, np.sum(generated_img)))\n",
    "                    print('\\tLoss: {:5.1f}'.format(total_loss))\n",
    "                    print('\\tDuration: {} seconds'.format(time.time() - start_time))\n",
    "                    start_time = time.time()  # Reset start time\n",
    "                    \n",
    "                    # Save generated image\n",
    "                    filename = './output/st_{}.jpg'.format(i)\n",
    "                    save_image(filename, generated_img)\n",
    "                    \n",
    "                    if (i + 1) % 20 == 0:\n",
    "                        # Save the variables into a checkpoint\n",
    "                        saver.save(sess, 'checkpoints/style', i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG-19 pre-trained model is ready.\n",
      "content_img_content 63156830.0\n",
      "style_layers_features 428224540.0\n",
      "Step 1\n",
      " \tSum:   nan\n",
      "\tLoss:   nan\n",
      "\tDuration: 25.613046884536743 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ankoor/.virtualenvs/py3/lib/python3.6/site-packages/ipykernel_launcher.py:12: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-51deb2fb4dbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStyleTransfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/priel-morgan.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./data/flowers.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_height\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-c645994fab12>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, n_iters)\u001b[0m\n\u001b[1;32m    209\u001b[0m                     \u001b[0;31m# Save generated image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./output/st_{}.jpg'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m                     \u001b[0msave_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerated_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m20\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-c344ede455ed>\u001b[0m in \u001b[0;36msave_image\u001b[0;34m(path, image)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_min\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'uint8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mimsave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_noise_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py3/lib/python3.6/site-packages/numpy/lib/utils.py\u001b[0m in \u001b[0;36mnewfunc\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;34m\"\"\"`arrayrange` is deprecated, use `arange` instead!\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mnewfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_set_function_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py3/lib/python3.6/site-packages/scipy/misc/pilutil.py\u001b[0m in \u001b[0;36mimsave\u001b[0;34m(name, arr, format)\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoimage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannel_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py3/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   1948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1949\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1950\u001b[0;31m             \u001b[0msave_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1951\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1952\u001b[0m             \u001b[0;31m# do what we can to clean up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py3/lib/python3.6/site-packages/PIL/JpegImagePlugin.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(im, fp, filename)\u001b[0m\n\u001b[1;32m    760\u001b[0m                   len(extra) + 1)\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m     \u001b[0mImageFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"jpeg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py3/lib/python3.6/site-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    511\u001b[0m                 \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_to_pyfd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 513\u001b[0;31m                 \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    514\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoder error %d when writing image file\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "# Transfer Style\n",
    "model = StyleTransfer('./data/priel-morgan.jpg', './data/flowers.jpg', img_width=224, img_height=224)\n",
    "model.build()\n",
    "model.train(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG-19 pre-trained model is ready.\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'layers', 'meta'])\n",
      "Index: [4] -> Layer: \"pool1\" W shape: ()\n",
      "Index: [9] -> Layer: \"pool2\" W shape: ()\n",
      "Index: [18] -> Layer: \"pool3\" W shape: ()\n",
      "Index: [27] -> Layer: \"pool4\" W shape: ()\n",
      "Index: [36] -> Layer: \"pool5\" W shape: ()\n",
      "IndexError\n"
     ]
    }
   ],
   "source": [
    "# Print Conv layer index and layer names\n",
    "download_model(VGG_DOWNLOAD_LINK, VGG_FILENAME, EXPECTED_BYTES)\n",
    "vgg = scipy.io.loadmat(VGG_FILENAME)\n",
    "print(vgg.keys())\n",
    "\n",
    "for i in range(vgg['layers'].shape[1]):\n",
    "    try:\n",
    "        name = vgg['layers'][0][i][0][0][0][0]\n",
    "        w = vgg['layers'][0][i][0][0][2][0][0]\n",
    "        w_shape = np.array(w).shape\n",
    "        if 'pool' in name:\n",
    "            print('Index: [{}] -> Layer: \"{}\" W shape: {}'.format(i, name, w_shape))\n",
    "        else:\n",
    "            continue\n",
    "    except IndexError:\n",
    "        print('IndexError')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': 3}\n",
      "{'x': 3, 'x_sq': 9}\n"
     ]
    }
   ],
   "source": [
    "class A:\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "        \n",
    "    def x_square(self):\n",
    "        out = self.x ** 2\n",
    "        setattr(self, 'x_sq', out)\n",
    "        \n",
    "    def load(self):\n",
    "        self.x_square()\n",
    "        \n",
    "a = A(3)\n",
    "print(vars(a))\n",
    "\n",
    "# After setting attribute\n",
    "a.load()\n",
    "print(vars(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33907663 0.76559658 0.58313098]\n",
      "[0.33907663 0.76559658 0.58313098]\n"
     ]
    }
   ],
   "source": [
    "a = np.random.uniform(size=(3, 3, 3, 4))\n",
    "print(a[:, 1, 1, 1])\n",
    "\n",
    "# at = tf.convert_to_tensor(a, tf.float32)\n",
    "at = tf.constant(a)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(at[:, 1, 1, 1]))\n",
    "\n",
    "with tf.variable_scope(name_or_scope='a', reuse=tf.AUTO_REUSE) as scope:\n",
    "    w = tf.get_variable(name='w', initializer=at, trainable=False)\n",
    "    \n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     print(sess.run(w[:, 1, 1, 1]))\n",
    "#     print(sess.run(tf.equal(w, at)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.44097298, 0.82561802, 0.06120844],\n",
       "        [0.94815193, 0.28613082, 0.29290534]],\n",
       "\n",
       "       [[0.97938947, 0.0907233 , 0.32692558],\n",
       "        [0.05042714, 0.6130169 , 0.37729897]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.uniform(size=(2, 2, 3))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[[2, 2], [2, 2]], [[2, 2], [2, 2]]])\n",
    "a = np.expand_dims(a, axis=0)\n",
    "b = np.array([[[5, 5], [5, 5]], [[5, 5], [5, 5]]])\n",
    "b = np.expand_dims(b, axis=0)\n",
    "np.sum(np.square(a - b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 224, 224, 3)\n",
      "VGG-19 pre-trained model is ready.\n"
     ]
    }
   ],
   "source": [
    "# Load image\n",
    "img = resize_image('./data/flowers.jpg', height=224, width=224, save=False)\n",
    "print(img.shape)\n",
    "\n",
    "with tf.variable_scope('input', reuse=tf.AUTO_REUSE) as scope:\n",
    "    input_img = tf.get_variable(name='image', \n",
    "                                shape=[1, 224, 224, 3], \n",
    "                                dtype=tf.float32, \n",
    "                                initializer=tf.zeros_initializer())\n",
    "    \n",
    "vgg = VGG(input_img)\n",
    "vgg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "401408\n",
      "(1, 28, 28, 512)\n",
      "(1, 224, 224, 64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.294784"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_feat = getattr(vgg, 'conv4_2')\n",
    "g_feat = c_feat * 0.5\n",
    "den = tf.cast(4 * tf.reduce_prod(g_feat.shape), dtype=tf.float32)\n",
    "l = tf.reduce_sum(tf.square(g_feat - c_feat)) / den\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    cf = sess.run(c_feat)\n",
    "    s = tf.reduce_prod(c_feat.shape).eval()\n",
    "    print(s)\n",
    "    e = sess.run(l)\n",
    "    \n",
    "print(cf.shape)\n",
    "\n",
    "gf = np.random.uniform(size=(1, 224, 224, 64))\n",
    "print(gf.shape)\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[[2, 2, 2], [2, 2, 2]], [[2, 2, 2], [2, 2, 2]]])\n",
    "a = np.expand_dims(a, axis=0)\n",
    "b = np.array([[[5, 5, 5], [5, 5, 5]], [[5, 5, 5], [5, 5, 5]]])\n",
    "b = np.expand_dims(b, axis=0)\n",
    "a.shape, b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 4), (3, 4))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar = np.reshape(a, newshape=(3, -1))\n",
    "br = np.reshape(b, newshape=(3, -1))\n",
    "ar.shape, br.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[40, 40, 40],\n",
       "       [40, 40, 40],\n",
       "       [40, 40, 40]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gram matrix\n",
    "ar.dot(br.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(1), Dimension(28), Dimension(28), Dimension(512)])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "M = tf.reduce_prod(c_feat.shape[:3])\n",
    "N = c_feat.shape[-1]\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(M))\n",
    "    print(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr = tf.reshape(c_feat, shape=[N, M])\n",
    "g = tf.matmul(cr, tf.transpose(cr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How `getattr` works when there are 3 inputs: Fix 2 inputs, and vary 3rd during training)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ST:\n",
    "    def __init__(self, content_img, img_width, img_height):\n",
    "        self.img_width = img_width\n",
    "        self.img_height = img_height\n",
    "        self.content_img = resize_image(content_img, self.img_width, self.img_height)\n",
    "        self.initial_img = generate_noise_image(self.content_img, self.img_width, self.img_height)\n",
    "        self.global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "        self.content_layer = 'conv4_2'\n",
    "    \n",
    "    def check_tensor(self, tensor):\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            out = sess.run(tensor)\n",
    "        \n",
    "    def create_input(self):\n",
    "        with tf.variable_scope('input', reuse=tf.AUTO_REUSE) as scope:\n",
    "            self.input_img = tf.get_variable(name='input_img', \n",
    "                                             shape=[1, self.img_height, self.img_width, 3], \n",
    "                                             dtype=tf.float32, \n",
    "                                             initializer=tf.zeros_initializer())\n",
    "            \n",
    "    def load_vgg(self):\n",
    "        self.vgg = VGG(self.input_img)\n",
    "        self.vgg.load()\n",
    "              \n",
    "    def _content_loss(self, P, F):\n",
    "        coefficient = tf.reciprocal(tf.cast(4 * tf.reduce_prod(P.shape), dtype=tf.float32))\n",
    "        self.content_loss = coefficient * tf.reduce_sum(tf.square(P - F))\n",
    "        \n",
    "    def loss(self):\n",
    "        with tf.variable_scope('loss', reuse=tf.AUTO_REUSE) as scope:\n",
    "            with tf.Session() as sess:\n",
    "                # Assign content image to input variable\n",
    "                sess.run(self.input_img.assign(self.content_img))\n",
    "                generate_img_content = getattr(self.vgg, self.content_layer)  # Tensor\n",
    "                content_img_content = sess.run(generate_img_content)\n",
    "                print('P: ', np.sum(content_img_content))\n",
    "                print('F: ', np.sum(self.check_tensor(generate_img_content)))\n",
    "            self._content_loss(content_img_content, generate_img_content)\n",
    "                \n",
    "        self.total_loss = self.content_loss \n",
    "        \n",
    "    def optimize(self):\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(self.total_loss, global_step=self.global_step)\n",
    "\n",
    "    def build(self):\n",
    "        self.create_input()\n",
    "        self.load_vgg()\n",
    "        self.loss()\n",
    "        self.optimize()\n",
    "        \n",
    "    def train(self, n_iters):\n",
    "        skip_step = 1\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            sess.run(self.input_img.assign(self.initial_img))\n",
    "            initial_step = self.global_step.eval()            \n",
    "            for i in range(initial_step, n_iters):   \n",
    "                _ = sess.run(self.optimizer)  # Optimize and update\n",
    "                generated_img, total_loss = sess.run([self.input_img, \n",
    "                                                      self.total_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG-19 pre-trained model is ready.\n",
      "P:  75623784.0\n",
      "F:  None\n"
     ]
    }
   ],
   "source": [
    "model = ST('./data/flowers.jpg', 224, 224)\n",
    "model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
