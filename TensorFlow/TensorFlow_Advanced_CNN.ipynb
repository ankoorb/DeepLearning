{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  **Leaky/Parametric ReLU**\n",
    "-  **Batch Normalization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/envs/tensorflow/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# MNIST data\n",
    "import tensorflow.examples.tutorials.mnist.input_data as data\n",
    "mnist = data.read_data_sets('MNIST_data/', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a placeholders for input images and input image labels\n",
    "n_input = mnist.train.images.shape[1]\n",
    "n_output = 10\n",
    "x = tf.placeholder(tf.float32, [None, n_input]) # First dim = None for mini-batch\n",
    "y = tf.placeholder(tf.float32, [None, n_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a placeholder to denote training phase\n",
    "train_phase = tf.placeholder(tf.bool, name='train_phase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CNN requires 4D tensor: [N, H, W, C]\n",
    "# Shape of x is 2D: [batch, height * width]\n",
    "\n",
    "side = int(np.sqrt(mnist.train.images.shape[1]))\n",
    "\n",
    "# Tensor shape \"SPECIAL\" value: -1 (CHECK: help(tf.reshape))\n",
    "x_tensor = tf.reshape(x, [-1, side, side, 1]) # -1 makes total size constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ReLU and  PReLU  \n",
    "-  [Python Implementation](http://gforge.se/2015/06/benchmarking-relu-and-prelu/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Leaky ReLU: Allows a small non-zero graident when the unit is not active\n",
    "def leaky_relu(x, leak=0.2, name='lrelu'):\n",
    "    \"\"\"\n",
    "    Leaky ReLU (NOTE: import tensorflow)\n",
    "    Arguments:\n",
    "        x: Tensor\n",
    "        leak: Leakage parameter (float)\n",
    "        name: Variable scope\n",
    "    Returns:\n",
    "        x: Tensor output of non-linearlyt\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "        f1 = 0.5 * (1 + leak)\n",
    "        f2 = 0.5 * (1 - leak)\n",
    "        out = f1 * x + f2 * abs(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Batch Normalization\n",
    "- Reduce internal covariate shift by  normalizing updates for each batch using batch mean and variance\n",
    "- [Batch Normalization in TensorFlow](https://stackoverflow.com/questions/33949786/how-could-i-%20use-batch-normalization-in-tensorflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Batch Normalization (Source: StackOverflow Batch Normalization in TensorFlow)\n",
    "def batch_norm(x, phase_train, scope='bn', affine=True):\n",
    "    \"\"\"\n",
    "    Convolutional layer batch normalization (NOTE: import tensorflow)\n",
    "    Arguments:\n",
    "        x: 4D Tensor (B x H x W x C)\n",
    "        phase_train: Boolean, TF Variable (true indates training phase)\n",
    "        scope: String, Variable scope\n",
    "        affine: Boolean, Affine trasnform output or not\n",
    "    Output:\n",
    "        Batch normalized output\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope):\n",
    "        \n",
    "        # Get Tensor shape\n",
    "        shape = x.get_shape().as_list()\n",
    "        \n",
    "        # Batch Norm: Beta \n",
    "        # [NOTE: Create a constant tensor]\n",
    "        beta = tf.Variable(tf.constant(0.0, shape=[shape[-1]]),\n",
    "                           name='beta', trainable=True)\n",
    "        \n",
    "        # Batch Norm: Gamma \n",
    "        # [NOTE: Create a constant tensor]\n",
    "        gamma = tf.Variable(tf.constant(1.0, shape=[shape[-1]]),\n",
    "                           name='gamma', trainable=affine)\n",
    "        \n",
    "        # Batch: Mean and Variance \n",
    "        # [NOTE: tf.nn.moments calculates tensor mean and var. USE: help(tf.nn.moments)]\n",
    "        batch_mean, batch_var = tf.nn.moments(x, axes=[0, 1, 2], name='moments')\n",
    "        \n",
    "        # Exponential Moving Average\n",
    "        EMA = tf.train.ExponentialMovingAverage(decay=0.9)\n",
    "        ema_mean, ema_var = EMA.average(batch_mean), EMA.average(batch_var)\n",
    "        # NOTE: help(tf.train.ExponentialMovingAverage)\n",
    "        \n",
    "        def mean_var_with_update():\n",
    "            ema_apply_op = EMA.apply([batch_mean, batch_var])\n",
    "            with tf.control_dependencies([ema_apply_op]):\n",
    "                return tf.identity(batch_mean), tf.identity(batch_var)\n",
    "        \n",
    "        mean, var = tf.cond(phase_train, \n",
    "                            mean_var_with_update,\n",
    "                            lambda: (EMA.average(batch_mean), EMA.average(batch_var)))\n",
    "        \n",
    "        norm = tf.nn.batch_norm_with_global_normalization(x, mean, var, beta, gamma, 1e-3, affine)\n",
    "    return norm    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fully Connected network\n",
    "def fc(x, n_units, scope=None, stddev=0.02, activation=lambda x: x):\n",
    "    shape = x.get_shape().as_list()\n",
    "    with tf.variable_scope(scope or 'Linear'):\n",
    "        mtx = tf.get_variable('Matrix', [shape[1], n_units], tf.float32,\n",
    "                              tf.random_normal_initializer(stddev=stddev))\n",
    "        return activation(tf.matmul(x, mtx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convolution 2D\n",
    "def conv2d(x, n_filters, filter_w=5, filter_h=5, stride_w=2, stride_h=2, \n",
    "           stddev=0.02, activation=None, bias=True, padding='SAME', name='Conv2D'):\n",
    "    \"\"\"\n",
    "    2D Convolution\n",
    "    Arguments:\n",
    "        x: Tensor\n",
    "        n_filters: Number of filters to apply\n",
    "        filter_w: Filter width\n",
    "        filter_h: Filter height\n",
    "        stride_w: Stride in cols\n",
    "        stride_h: Stride in rows\n",
    "        stddev: Std. Deviation of initialization\n",
    "        activation: Non-linearity function\n",
    "        padding: 'SAME' or 'VALID'\n",
    "        name: Variable scope to use\n",
    "    output:\n",
    "        x: Tensor (Convolved)\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "        W = tf.get_variable('W', \n",
    "                            [filter_w, filter_h, x.get_shape()[-1], n_filters],\n",
    "                            initializer=tf.truncated_normal_initializer(stddev=stddev))\n",
    "        conv = tf.nn.conv2d(x, W, strides=[1, stride_w, stride_h, 1], padding=padding)\n",
    "        if bias:\n",
    "            b = tf.get_variable('b', [n_filters], \n",
    "                               initializer=tf.truncated_normal_initializer(stddev=stddev))\n",
    "            conv = tf.nn.bias_add(conv, b)\n",
    "        if activation:\n",
    "            conv = activation(conv)\n",
    "        return conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN\n",
    "\n",
    "#### Convolution layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convolution layer-1\n",
    "conv_1 = conv2d(x_tensor, n_filters=16, name='conv_1')\n",
    "bn_1 = batch_norm(conv_1, phase_train=train_phase, scope='bn_1')\n",
    "a_conv_1 = leaky_relu(bn_1, name='lrelu_1')\n",
    "\n",
    "# Convolution layer-2\n",
    "conv_2 = conv2d(a_conv_1, n_filters=8, name='conv_2')\n",
    "bn_2 = batch_norm(conv_2, phase_train=train_phase, scope='bn_2')\n",
    "a_conv_2 = leaky_relu(bn_2, name='lrelu_2')\n",
    "\n",
    "# Convolution layer-3\n",
    "conv_3 = conv2d(a_conv_2, n_filters=4, name='conv_3')\n",
    "bn_3 = batch_norm(conv_3, phase_train=train_phase, scope='bn_3')\n",
    "a_conv_3 = leaky_relu(bn_3, name='lrelu_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolution Layer-1\n",
      "(?, 14, 14, 16)\n",
      "(?, 14, 14, 16)\n",
      "(?, 14, 14, 16)\n",
      "Convolution Layer-2\n",
      "(?, 7, 7, 8)\n",
      "(?, 7, 7, 8)\n",
      "(?, 7, 7, 8)\n",
      "Convolution Layer-3\n",
      "(?, 4, 4, 4)\n",
      "(?, 4, 4, 4)\n",
      "(?, 4, 4, 4)\n"
     ]
    }
   ],
   "source": [
    "# Convolution layer: Dimension check\n",
    "print 'Convolution Layer-1'\n",
    "print conv_1.get_shape()\n",
    "print bn_1.get_shape()\n",
    "print a_conv_1.get_shape()\n",
    "print 'Convolution Layer-2'\n",
    "print conv_2.get_shape()\n",
    "print bn_2.get_shape()\n",
    "print a_conv_2.get_shape()\n",
    "print 'Convolution Layer-3'\n",
    "print conv_3.get_shape()\n",
    "print bn_3.get_shape()\n",
    "print a_conv_3.get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fully Connected layer\n",
    "- NOTE: Print dimensions  of  last  convolution  layer to  reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reshape activation output of third convolution layer to connect to a fully connected layer\n",
    "a_conv_3_flat = tf.reshape(a_conv_3, [-1, 4 * 4 * 4]) # Convolution Layer-3 Shape: (?, 4, 4, 4)\n",
    "fc_4 = fc(a_conv_3_flat, n_output)\n",
    "y_pred = tf.nn.softmax(fc_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fully Connected Layer-4\n",
      "(?, 64)\n",
      "(?, 10)\n",
      "(?, 10)\n"
     ]
    }
   ],
   "source": [
    "# Fully Connected layer: Dimension check\n",
    "print 'Fully Connected Layer-4'\n",
    "print a_conv_3_flat.get_shape()\n",
    "print fc_4.get_shape()\n",
    "print y_pred.get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loss\n",
    "cross_entropy = -tf.reduce_sum(y * tf.log(y_pred))\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "y_hat = tf.argmax(y_pred, dimension=1)\n",
    "y_true = tf.argmax(y, dimension=1)\n",
    "correct_pred = tf.equal(y_hat, y_true)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a session to use the graph\n",
    "sess = tf.Session()\n",
    "\n",
    "# Initialize all variables\n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-0 Validation Accuracy: 0.959599971771\n",
      "Epoch-1 Validation Accuracy: 0.977199971676\n",
      "Epoch-2 Validation Accuracy: 0.973800003529\n",
      "Epoch-3 Validation Accuracy: 0.978399991989\n",
      "Epoch-4 Validation Accuracy: 0.979799985886\n",
      "Test Accuracy:  0.9825\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "batch_size = 100\n",
    "n_epochs = 5\n",
    "batches = mnist.train.num_examples//batch_size\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for batch in range(batches):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size=batch_size)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y, train_phase: True})\n",
    "    acc = sess.run(accuracy, feed_dict={x: mnist.validation.images,\n",
    "                                        y: mnist.validation.labels,\n",
    "                                        train_phase: False})\n",
    "    print 'Epoch-{} Validation Accuracy: {}'.format(epoch, acc)\n",
    "    \n",
    "print 'Test Accuracy: ', sess.run(accuracy, feed_dict={x: mnist.test.images,\n",
    "                                                       y: mnist.test.labels,\n",
    "                                                       train_phase: False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function variable_scope in module tensorflow.python.ops.variable_scope:\n",
      "\n",
      "variable_scope(*args, **kwds)\n",
      "    Returns a context manager for defining ops that creates variables (layers).\n",
      "    \n",
      "    This context manager validates that the (optional) `values` are from\n",
      "    the same graph, ensures that graph is the default graph, and pushes a\n",
      "    name scope and a variable scope.\n",
      "    \n",
      "    If `name_or_scope` is not None, it is used as is. If `scope` is None, then\n",
      "    `default_name` is used.  In that case, if the same name has been previously\n",
      "    used in the same scope, it will made unique be appending `_N` to it.\n",
      "    \n",
      "    Variable scope allows to create new variables and to share already created\n",
      "    ones while providing checks to not create or share by accident. For details,\n",
      "    see the [Variable Scope How To](../../how_tos/variable_scope/index.md),\n",
      "    here we present only a few basic examples.\n",
      "    \n",
      "    Simple example of how to create a new variable:\n",
      "    \n",
      "    ```python\n",
      "    with tf.variable_scope(\"foo\"):\n",
      "        with tf.variable_scope(\"bar\"):\n",
      "            v = tf.get_variable(\"v\", [1])\n",
      "            assert v.name == \"foo/bar/v:0\"\n",
      "    ```\n",
      "    \n",
      "    Basic example of sharing a variable:\n",
      "    \n",
      "    ```python\n",
      "    with tf.variable_scope(\"foo\"):\n",
      "        v = tf.get_variable(\"v\", [1])\n",
      "    with tf.variable_scope(\"foo\", reuse=True):\n",
      "        v1 = tf.get_variable(\"v\", [1])\n",
      "    assert v1 == v\n",
      "    ```\n",
      "    \n",
      "    Sharing a variable by capturing a scope and setting reuse:\n",
      "    \n",
      "    ```python\n",
      "    with tf.variable_scope(\"foo\") as scope:\n",
      "        v = tf.get_variable(\"v\", [1])\n",
      "        scope.reuse_variables()\n",
      "        v1 = tf.get_variable(\"v\", [1])\n",
      "    assert v1 == v\n",
      "    ```\n",
      "    \n",
      "    To prevent accidental sharing of variables, we raise an exception when\n",
      "    getting an existing variable in a non-reusing scope.\n",
      "    \n",
      "    ```python\n",
      "    with tf.variable_scope(\"foo\"):\n",
      "        v = tf.get_variable(\"v\", [1])\n",
      "        v1 = tf.get_variable(\"v\", [1])\n",
      "        #  Raises ValueError(\"... v already exists ...\").\n",
      "    ```\n",
      "    \n",
      "    Similarly, we raise an exception when trying to get a variable that\n",
      "    does not exist in reuse mode.\n",
      "    \n",
      "    ```python\n",
      "    with tf.variable_scope(\"foo\", reuse=True):\n",
      "        v = tf.get_variable(\"v\", [1])\n",
      "        #  Raises ValueError(\"... v does not exists ...\").\n",
      "    ```\n",
      "    \n",
      "    Note that the `reuse` flag is inherited: if we open a reusing scope,\n",
      "    then all its sub-scopes become reusing as well.\n",
      "    \n",
      "    Args:\n",
      "      name_or_scope: `string` or `VariableScope`: the scope to open.\n",
      "      default_name: The default name to use if the `name_or_scope` argument is\n",
      "        `None`, this name will be uniquified. If name_or_scope is provided it\n",
      "        won't be used and therefore it is not required and can be None.\n",
      "      values: The list of `Tensor` arguments that are passed to the op function.\n",
      "      initializer: default initializer for variables within this scope.\n",
      "      regularizer: default regularizer for variables within this scope.\n",
      "      caching_device: default caching device for variables within this scope.\n",
      "      partitioner: default partitioner for variables within this scope.\n",
      "      custom_getter: default custom getter for variables within this scope.\n",
      "      reuse: `True` or `None`; if `True`, we go into reuse mode for this scope as\n",
      "        well as all sub-scopes; if `None`, we just inherit the parent scope reuse.\n",
      "      dtype: type of variables created in this scope (defaults to the type\n",
      "        in the passed scope, or inherited from parent scope).\n",
      "    \n",
      "    Returns:\n",
      "      A scope that can be to captured and reused.\n",
      "    \n",
      "    Raises:\n",
      "      ValueError: when trying to reuse within a create scope, or create within\n",
      "        a reuse scope, or if reuse is not `None` or `True`.\n",
      "      TypeError: when the types of some arguments are not appropriate.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.variable_scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
