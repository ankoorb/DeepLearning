{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Residual Learning for Image Recognition\n",
    "\n",
    "**Authors:** K. He, X. Zhang, S. Ren, J. Sun  \n",
    "**Link:** https://arxiv.org/pdf/1512.03385.pdf  \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "- Presented a residual learning framework that makes it easier to train deep networks\n",
    "- Accuracy degradation problem occurs when deep networks start converging: with the network depth increasing the accuracy gets saturated and then degrades rapidly.\n",
    "\n",
    "![](images/resnet0.png)\n",
    "\n",
    "- Training accuracy degradation indicates that not all systems are similarly easy to optimize.\n",
    "- Experiments on ImageNet to show:\n",
    "    - Extremely deep residual networks are easy to optimize. Deep  simple stack layer networks exhibit higher training error when the depth increases\n",
    "    - Deep residual networks gains accuracy from increased depth\n",
    "    - 152 layer residual network ensemble achieved *3.57* % top-5 error on ImageNet test set.\n",
    "- Experiments on CIFAR-10: Explored models with over 1000 layers\n",
    "- Consider $\\mathcal{H}(\\bf x)$ as an underlying mapping to be fit by a few stacked layers, where $\\bf x$ is the input to the first of these layers. Hypothesis that multiple non-linear layers can approximate complicated function is equivalent to hypothesis that non-linear layers can approximate the residual functions $\\mathcal{H}(\\bf x) - \\bf x$. Instead of using stacked layers to approximate $\\mathcal{H}(\\bf x)$ the stacked layers are used to approximate a residual function $\\mathcal{F}(\\bf x):=\\mathcal{H}(\\bf x) - \\bf x$. The original function becomes $\\mathcal{F}(\\bf x)+\\bf x$\n",
    "\n",
    "- If the added layers can be constructed as identity mappings a deeper model should not have training error greater than a shallower similar network. The accuracy degradation problem suggests that the solvers might have difficulties in approximating identity mappings by multiple non-linear layers. With the residual learning reformulations, *if the identity mappings are optimal, the solvers may drive the weights of the multiple non-linear layers toward zero to approach identity mappings*.\n",
    "- Desired underlying mapping $\\mathcal{H}(\\bf x)$ can be realized by feedforward network with *shortcut connections* (connection that skip one or more layers). \n",
    "- In ResNet shortcut connections simply perfrom *identity* mapping (it is a function that always returns the same value that was used as its argument). Identity shortcut connections does not add extra parameter or computational complexity.\n",
    "- **Identity Mapping by Shortcuts**\n",
    "    - Building block: $\\bf y = \\mathcal{F}(\\bf{x}, \\{W_i\\}) + \\bf x$ where $\\bf x$ and $\\bf y$ are the input and output vectors fo the layers. Function $\\mathcal{F}(\\bf{x}, \\{W_i\\})$ represents the residual mapping to be learned. \n",
    "    \n",
    "    ![](images/resnet1.png)\n",
    "    \n",
    "    - Above example: $\\mathcal{F} = W_2\\sigma(W_1 \\bf x)$, where $\\sigma$ denotes non-linearity ReLU.\n",
    "    - Operation $\\mathcal{F} + \\bf x$ is performed by a shortcut connection and element-wise addition. Then ReLU non-linearity is applied: $\\sigma(\\mathcal{F} + \\bf x)$. Shortcut connection does not introduce extra parameter or computation complexity. \n",
    "    - Experiments involve function $\\mathcal{F}$ that has 2 or more layers. If $\\mathcal{F}$ has only a single layer then $\\bf y = W_1 \\bf x + \\bf x$ (a linear layer) and it does not offer any advantages\n",
    "- Training:\n",
    "    - Image resized with its shorter side randomly sampled in [256, 480] for scale augmentation. A 224 x 224 crop is then randomly sampled from an image with the per-pixel mean subtracted.\n",
    "    - Used batch normalization after each convolution and before activation\n",
    "    - SGD batch size = 256, learning rate = 0.1 and it is divided by 10 when the error plateaus\n",
    "    - Weight decay = 0.0001 and momentum = 0.9\n",
    "    - Convolution layers have 3 x 3 filters. Downsampling is done by using a stride of 2. \n",
    "    - Network ends with a global average pooling layer and  a fully connected softmax layer.\n",
    "- Deeper Bottleneck Architecture: Modified *building block* as a *bottleneck* design. For each residual function $\\mathcal{F}$ 3 layer stack (1 x 1, 3 x 3, 1 x 1) was used. 1 x 1 layers were used for reducing and restoring dimensions. Identity shortcuts were replaced with projection\n",
    "\n",
    "---\n",
    "\n",
    "![](images/ResNet.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
