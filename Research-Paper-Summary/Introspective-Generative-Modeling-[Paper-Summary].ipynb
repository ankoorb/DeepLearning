{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introspective Generative Modeling: Decide Discriminatively\n",
    "\n",
    "**Authors:** J. Lazarow, J. Lin, Z. Tu  \n",
    "**Link:** https://arxiv.org/pdf/1704.07820.pdf\n",
    "\n",
    "---\n",
    "\n",
    "- Developed Introspective Generative Modeling (IGM) that attains a generator using progressively learned deep convolutional neural networks. Generator is able to self-evaluate the difference between its generated samples and the given training data (i.e. Generator is also a discriminator).\n",
    "\n",
    "- Unsupervised learning (much harder task because of learning complexity and assumptions) models are often *generative* and supervised classifiers are often *discriminative*\n",
    "\n",
    "- Introspective Generative Modeling (IGM) is simultaneously a generator and a discriminator. Modeling consists of two stages during training:\n",
    "\n",
    "    (1) A `pseudo-negative sampling stage` (synthesis) for self-generation. The figure below shows one round of sampling by extracting patches from training images thus allowing synthesis of much larger images.\n",
    "    \n",
    "    ![](images/igm-synth.png)\n",
    "    \n",
    "    (2) A `CNN classifier learning stage` (classification) for self evaluation and model updating.\n",
    "        \n",
    "    ![](images/igm.png)\n",
    "    \n",
    "    - In the figure above the top sub-figure shows the input training samples shown in <span style=\"color:red\">red</span>\n",
    "    - In the figure above the bottom sub-figure shows the `pseudo-negative` samples drawn by the learned final model (shown in <span style=\"color:blue\">blue</span>\n",
    "    - In the figure above the left panel displays `pseudo-negative` samples drawn at each time stamp $t$\n",
    "    - In the figure above the right panel shows `classification by the CNN` on training samples and  `pseudo-negative` samples at each time stamp $t$\n",
    "    \n",
    "---\n",
    "\n",
    "- Algorithm\n",
    "\n",
    "![](images/igm-alg.png)\n",
    "\n",
    "- Some properties about IGM\n",
    "    - Existing CNN classifiers can be directly made into generators (if trained properly)\n",
    "    - Able to train on images of a size and generate an image of larger size while maintaining the coherence for the entire image.\n",
    "- General pipeline of IGM is similar to Generative Modeling via Discriminative approach method (GDL) with boosting algorithm replaced by a CNN in IGM (demonstrated significant improvement in modeling and computational power)\n",
    "- GDL learns a generative model through a sequence of discriminative classifiers (boosting) using repeadedly self-generated samples, called *pseudo-negatives*\n",
    "\n",
    "- Differences between GDL and IGM\n",
    "    - CNN in IGM results in a significant boost to feature learning\n",
    "    - GDL: Markov Chain Monte Carlo based sampling process (computational bottleneck. IGM: Backpropagation to synthesis/sampling process\n",
    "- Differences between GAN and IGM\n",
    "    - IGM maintains a single model that is simultaneously a generator and a discriminator. GAN uses two CNN's, a generator and a discriminator.\n",
    "    - GAN's are hard to train. IGM carries out a straightforward use of backpropagation in both the sampling and the classifier training stage, making the learning process direct.\n",
    "    - GAN generator is a mapping from features to images. IGM directly models the underlying statistics of an image with an efficient sampling/inference process, which makes IGM flexible.\n",
    "    - GAN performs a forward pass to reconstruct an image. In IGM image synthesis is carried out using backpropagation so it is slower (but feasible)\n",
    "    - IGM has larger model complexity (a cascade of ~ 60 to 200 CNN classifiers are included) compared to GAN.\n",
    "    \n",
    "---\n",
    "\n",
    "NOTE: Check paper for derivation and experiment details\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
