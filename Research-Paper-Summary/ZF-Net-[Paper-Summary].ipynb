{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing and Understanding Convolutional Networks\n",
    "\n",
    "**Authors: ** M. Zeiler and R. Fergus  \n",
    "**Link: ** https://arxiv.org/pdf/1311.2901.pdf  \n",
    "**YouTube:** [Matthew Zeiler Presentation](https://www.youtube.com/watch?v=ghEmQSxT6tw)\n",
    "\n",
    "---\n",
    "\n",
    "- Introduced visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Visualization technique also allows to observe the evolution of features during training. \n",
    "- Normally only first layer projections to pixel space are possible\n",
    "- Understanding the operation of a convnet requires interpreting the feature activity in intermediate layers. Developed a new way to map feature activity back to input pixel space, showing what input pattern caused a given activation in the feature map.\n",
    "- [<span style=\"color:teal\">Visualization</span>](https://www.youtube.com/watch?v=AgkfIQ4IGaM) technique uses a multi-layered deconvolutional network to project the feature activations back to the input pixel space. Deconvolutional network is similar to convolution network in reverse. \n",
    "    - Convolution: Mapping pixels to features\n",
    "    - Deconvolution: Mapping features to pixels\n",
    "- A deconvnet (with path back to image pixels) is attached to each layer of CNN. An input is fed into CNN and features are computed throughout the layers. To examine a given CNN activation, set all other activations in the layer to zero and pass the feature maps as input to attached deconvnet layer. Then (1) unpool, (2) rectify and (3) filter to reconstruct the activity in the layer beneath that gave rise to the chosen activation\n",
    "    - Unpooling: Max pooling operation is non-invertible, however an approximate inverse can be obtained by recording the locations of the maxima within each pooling region in a set of switch variables. \n",
    "    - Rectification: ReLU non-linearities rectify the feature maps (makes sure feature maps are always positive). Valid feature reconstructions at each layer are obtained by passing reconstructed signal through a ReLU\n",
    "    - Filtering: Deconvnet uses transposed versions of learned filters applied to rectified maps\n",
    "![](images/zfnet0.png)\n",
    "\n",
    "---\n",
    "\n",
    "- Architecture: Similar to AlexNet (sparse connection replaced with dense connections)\n",
    "![](images/zfnet1.png)\n",
    "\n",
    "- Training: \n",
    "    - ImageNet 2012 training set (1.3 Million images)\n",
    "    - SGD with batch size = 128 examples, momentum = 0.9, learning rate = 0.01\n",
    "    - Weights initialized to 0.01 and biases set to 0\n",
    "    - Observed that few first layer filters were dominating. Solution: Renormalize each filter in the convolution layers whose RMS value exceeds a fixed radius of 0.1 to fixed radius of 0.1\n",
    "- AlexNet problems:\n",
    "    - First layer filters are a mix of extremely high and low frequency information, with little coverage of the mid frequencies.\n",
    "    - Second layer visualization shows aliasing distortion/artifacts caused by large stride 4 used in 1st layer convolution. Aliasing is an effect that causes different signals to become indistinguishable (or aliases of one another) when sampled.\n",
    "    - Solution:\n",
    "        - Reduced 1st layer filter size from 11 x 11 to 7 x 7\n",
    "        - Reduced stride of convolutions from 4 to 2\n",
    "        ![](images/zfnet2.png)\n",
    "        - (b) and (d): AlexNet and (c) and (e): ZF Net\n",
    "    - `Smaller filter and smaller stride retains more information`\n",
    "- Occulison sensitivity: Probability of correct class drops significantly when the object is occluded\n",
    "- Correspondence analysis: Used Hamming distance\n",
    "- Feature generalization: Complex invariances learned in convolution layers. Tested model performance on Caltech and other data sets by fixing first 7 layers and training final softmax layer. Found the model was able to generalize well.\n",
    "![](images/zfnet3.png) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
