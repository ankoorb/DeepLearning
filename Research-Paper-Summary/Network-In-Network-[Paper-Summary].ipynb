{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network In Network\n",
    "\n",
    "**Authors:** M. Lin, Q. Chen, S. Yan  \n",
    "**Link:** https://arxiv.org/pdf/1312.4400.pdf  \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "- Proposed a novel deep network structure called `Network In Network` (NIN).  \n",
    "- Convolution layers take inner product of the linear filter and the underlying receptive field followed by a non-linear activation function at every local function of the input to produce feature maps.\n",
    "- Convolution filter in CNN is a generalized linear model (GLM) for the underlying data patch. GLM can achieve a good extent of abstraction (the feature is invariant to the variants of the same concept) when the samples of latent concepts are linearly separable. However, the data for the same concept often live on a non-linear manifold, therefore the representations that capture these concepts are generally highly non-linear function of the input.\n",
    "    - Idea: Replace GLM with non-linear function approximator\n",
    "- In NIN the GLM is replaced with a \"micro network\" structure which is a general non-linear function approximator. Authors chose Multi-layer Perceptron (MLP) as instantiation of the micro network.\n",
    "![](images/nin0.png)\n",
    "- MLPConv layer maps the input local patch to feature map with an MLP by sliding it over the input. ReLU non-linearity is used as activation function in the MLP.\n",
    "    - In CNN using linear rectifier, the feature map can be calculated as $f_{i, j, k} = max(w_{k}^Tx_{i, j}, 0)$, where $(i, j)$ is the pixel index in the feature map, $x_{i,j}$ stands for the input patch centered at location $(i, j)$, and $k$ is used to index the channels in of the feature map.\n",
    "    - MLPConv layer performs calculation as follows ($n$ is number of layers in the multilayer perceptron): \n",
    "    $$f_{i, j, k}^1 = max({w_{k_1}^{1}}^Tx_{i, j} + b_{k_1}, 0)$$\n",
    "    $$\\vdots$$\n",
    "    $$f_{i, j, k}^n = max({w_{k_n}^{n}}^Tf_{i, j}^{n-1} + b_{k_n}, 0)$$  \n",
    "- NIN structure consists of stacked multiple MLPConv layers. Instead of using fully connected layer (prone to overfitting) for classification, NIN uses *global average pooling* layer (acts as a regularizer) which feeds into softmax layer.\n",
    "![](images/nin1.png)\n",
    "- **Global Average Pooling:** Generate one feature map for each corresponding category of the classification task in the last MLPConv layer and then take average of each feature map and feed into softmax layer. Advantages:\n",
    "    - No parameter to optimize\n",
    "    - More robust to spatial translation of the input\n",
    "- Experiments: Used these four datasets: CIFAR-10, CIFAR-100, SVHN, MNIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
